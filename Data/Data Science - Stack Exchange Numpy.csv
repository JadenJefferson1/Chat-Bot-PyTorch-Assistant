QuestionBody,AnswerBody,Tags
"I am having difficulties understanding .moveaxis in Numpy.
I first create an array by using a=np.arange(24).reshape(2,3,4). The system will first fill up axis 2 with 0 1 2 3, then move along axis 1 to the next row. When the first 'page' is done, the system move along axis 0. The following is obtained.
array([[[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11]],

       [[12, 13, 14, 15],
        [16, 17, 18, 19],
        [20, 21, 22, 23]]])

If I input b = np.swapaxes(a,0,2); b now, the system should fill up axis 0 first, then axis 1 and finally axis 2. The following is obtained.
array([[[ 0, 12],
        [ 4, 16],
        [ 8, 20]],

       [[ 1, 13],
        [ 5, 17],
        [ 9, 21]],

       [[ 2, 14],
        [ 6, 18],
        [10, 22]],

       [[ 3, 15],
        [ 7, 19],
        [11, 23]]])

This is understandable as axis 1 is preserved, so we can still see columns like 0 4 8 and 1 5 9 after swapping the axes.
But I don't really understand how .moveaxis works. If I input b = np.moveaxis(a,0,2); b, the following is obtained.
array([[[ 0, 12],
        [ 1, 13],
        [ 2, 14],
        [ 3, 15]],

       [[ 4, 16],
        [ 5, 17],
        [ 6, 18],
        [ 7, 19]],

       [[ 8, 20],
        [ 9, 21],
        [10, 22],
        [11, 23]]])

I know that the .moveaxis function is meant to 'move axis 0 to a new position while the other axes remain the same order', but what is the meaning of that? I understand that the result should be an array with shape (3, 4, 2), but why would the system go down the first column in the first place then move on to the second page?
","You asked for help on understanding the mechanism behind .moveaxis.
The documentation explains it twice,
imprecisely in English and
very precisely in code when you click on [source].
There's not a lot going on there.
We just pick a new ordering of the existing axes,
specified by the caller.
    order = [n for n in range(a.ndim) if n not in source]

    for dest, src in sorted(zip(destination, source)):
        order.insert(dest, src)

And then the final assignment essentially computes np.transpose(order),
so that's the method you want to study in order to better understand
this function.
Recall that
.rollaxis()
is being deprecated, with
.moveaxis()
offering a superset of its functionality.
Studying the subset might prove helpful to you.
",<python><numpy>
"I am trying to fit some data inside an algorithm, but i am getting this error:
ValueError: Found input variables with inconsistent numbers of samples: [0, 6]
How i can solve this?
Here is my code bellow:
#Import all libs
from connect_db import connect_db
import pandas as pd
from pathlib import Path
import matplotlib.pyplot as plt
import matplotlib as mpl
import datetime
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

#Database access informations
DB_CONFIG = {
    'host': 'localhost',
    'database': 'test_db',
    'user': 'root',
    'user_pass': 'root'
}

#create a date parser function 
def dateparser(s):
    return datetime.datetime.strptime(s, '%Y-%m-%d %H:%M:%S')

#Initialize the classifier of the algorithm
clf = RandomForestClassifier(random_state=0)

#Initialize the connection script
connect_db = connect_db(host=DB_CONFIG['host'],
                        database=DB_CONFIG['database'],
                        user=DB_CONFIG['user'],
                        password=DB_CONFIG['user_pass'])

#Create a connection
cnx = connect_db.connect()

#Create a cursor to manipulate the database
cursor = cnx.cursor()

#Create engine for converting the csv file to a sql table.
engine = connect_db.create_engine()

mpl.rcParams['agg.path.chunksize'] = 10000

#Open the csv file and pass it to a sql table
with open(Path(&quot;databases/weather_data.csv&quot;)) as file:
    #Read the file, using a function to parse the date to correct format.
    csv_data = pd.read_csv(file, parse_dates=['Date_Time'], date_parser=dateparser)

    #Convert the file to a sql database and upload the data.
    sql_data = csv_data.to_sql(name=f&quot;{DB_CONFIG['database']}&quot;,
                               con=engine,
                               if_exists='replace',
                               chunksize=100)
    #Read the sql table
    sql_data = pd.read_sql_table('test_db', engine)

#Rearanged the columns order, for better malleability 
sql_data = sql_data[['Location','Date_Time','Temperature_C','Humidity_pct','Wind_Speed_kmh', 'Precipitation_mm']]

#Slice the data.
y = np.array(sql_data.iloc[5])
x = np.array(sql_data.loc['Temperature_C':'Wind_Speed_kmh'])

#Split the data into train data and test data
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33)

#Fit the data inside the algorithm
clf.fit(x_train, y_train)

#Test the data
y_pred = clf.predict(x_test)

#Plot and show the data for visualization purposes
plt.scatter(x_test, y_test, color='red')
plt.plot(x_test, y_pred, color='blue', linewidth=2)
plt.show()
```

","I believe you are misusing .loc[] and .iloc[]. iloc[5] gets you the sixth row, not the sixth column. For the sixth column you need to use .iloc[:, 5]. For loc I believe .loc[:, 'Temperature_C':'Wind_Speed_kmh'] would work, but not sure. Look into pandas' documentation
",<python><scikit-learn><numpy>
"def scale_dataset(dataframe, oversample=False):
  X = dataframe[dataframe.columns[:-1]].values
  y = dataframe[dataframe.columns[-1]].values

  scaler = StandardScaler()
  X = scaler.fit_transform(X)

  if oversample:
    ros = RandomOverSampler()
    X, y = ros.fit_resample(X, y)

  data = np.hstack((X, np.reshape(y, (-1, 1))))

  return data, X, y

train, X_train, y_train = scale_dataset(train, oversample=True)

","Looks like that train in the input of the function is not a pd.DataFrame, but an np.ndarray
",<pandas><numpy>
"Why is the largest eigenvalue used for eigenvector calculation?
What is the significance of using a largest absolute value for the whitening of a picture?
","The largest eigenvalue of a matrix is significant because it determines the dominant behavior of the transformation and is often associated with stability and convergence properties.
Using the largest absolute value eigenvalue for the whitening of a picture is significant because it helps in decorrelating the features of the image. By using the largest absolute value eigenvalue, the whitening process ensures that the transformed image data has uncorrelated components, which can be beneficial for feature extraction and pattern recognition. It also helps in reducing the redundancy in the image data, making it more suitable for subsequent analysis and processing.
",<clustering><numpy><scipy><matrix>
"Problem Statement
The goal is to have the K-Means customer code run for clusters and not use scikit-learn libraries. Learning exercise. This K-means has the standard predict, fix, centroids, cluster means functions.
This is a data science coding programming question on creating a custom K-Means for machine learning so it is appropriate to ask here in the data science forum since resources will be keen on K-Means algorithms.
My Attempt to Fix the Problem
The TypeError unhashable appears to suggests that slicing on self.Centroids[:, k] is not working properly. I have already used a numpy arrays (np.asarray) to properly shape the array.
In the fix() function, first self attribute is casted to a numpy array before mutating the contents. The function name is has_fit(self, n_iterations) and the self.Centroids gets the self.getCentroids for the centroids in a numpy array.
Shapes
disparity_matrix.shape is (2700, 1)
Error
TypeError                                 Traceback (most recent call last)
Cell In[218], line 5
      3 disparity_matrix = np.asarray(disparity_matrix)
      4 stars_model = CustomKMeans(disparity_matrix, n_clusters=2)
----&gt; 5 start_model_fit = stars_model.has_fit(n_iterations=300)
      7 # Get cluster labels
      8 stars = np.asarray(stars)

Cell In[217], line 37, in CustomKMeans.has_fit(self, n_iterations)
     34 _euclidian = np.array([]).reshape(self.m, 0)
     35 for k in range(self.K):
     36     # Reshape the centroid column vector to be a 2D array
---&gt; 37     centroid_column = self.Centroids[:, k].reshape(-1, 1)
     38     _distance = np.sum((self.X - centroid_column.T) ** 2, axis=1)
     39     _euclidian = np.c_[_euclidian, _distance]

TypeError: unhashable type: 'slice'

Base Invocation Calls / Sequences
My invoke python calls for my object:
disparity_matrix = np.asarray(disparity_matrix)
stars_model = CustomKMeans(disparity_matrix, n_clusters=2)
start_model_fit = stars_model.has_fit(n_iterations=300)

My Python K-Means Code
My class code is listed:
class CustomKMeans:
    def __init__(self, X, n_clusters=8, max_iter=300, tol=1e-4):
        self.X=np.asarray(X)
        self.Cluster={}
        self.Centroids=np.array([]).reshape(self.X.shape[1],0)
        self.K=n_clusters
        self.m=self.X.shape[0]
        
    def getCentroids(self,X,K):
        i = random.randint(0,X.shape[0])
        _centroid = np.array([X[i]])
        for z in range(1,K):
            _array = np.array([]) 
            for item in X:
                _array = np.append(_array, np.min(np.sum((item-_centroid)**2)))
            _probability = _array/np.sum(_array)
            _cummulative_probability = np.cumsum(_probability)
            _random = random.random()
            _jmp = 0
            for k, l in enumerate(_cummulative_probability):
                if _random &lt; l:
                    _jmp = k
                    break
            _centroid = np.append(_centroid,[X[i]],axis=0)
        return np.asarray(_centroid.T)

    def has_fit(self, n_iterations):
        self.Centroids = self.getCentroids(self.X, self.K)

        for n in range(n_iterations):
            _euclidian = np.array([]).reshape(self.m, 0)
            for k in range(self.K):
                # Reshape the centroid column vector to be a 2D array
                centroid_column = self.Centroids[:, k].reshape(-1, 1)
                _distance = np.sum((self.X - centroid_column.T) ** 2, axis=1)
                _euclidian = np.c_[_euclidian, _distance]
            _centroid_adj = np.argmin(_euclidian, axis=1) + 1

            # Adjust the centroids
            _centroids = {}
            for n in range(self.K):
                _centroids[n + 1] = np.array([]).reshape(self.X.shape[1], 0)

            for o in range(self.m):
                # Reshape self.X[o] to match the shape of _centroids[_centroid_adj[o]]
                reshaped_X_o = self.X[o].reshape(-1, 1)  # Use -1 to infer the size along the first axis
                _centroids[_centroid_adj[o]] = np.c_[_centroids[_centroid_adj[o]], reshaped_X_o]

            for p in range(self.K):
                _centroids[p + 1] = _centroids[p + 1].T

            for q in range(self.K):
                self.Centroids[:, q] = np.mean(_centroids[q + 1], axis=0)

            self.Centroids = _centroids
        
    def has_cluster_means(self, X):
        cluster_means = []
        for i in range(self.n_clusters):
            points = X[self.labels == i]
            if len(points) &gt; 0:
                cluster_mean = np.mean(points, axis=0)
                cluster_means.append(cluster_mean)
        return cluster_means
    
    def has_predict(self):
        return self.Clusters,self.Centroids.T

","The self.Centroids attribute starts by being a numpy array, which you can you index by row and column. However, later on, you overwrite self.Centroids by _centroids (last line of code in has_fit), which is a dictionary that cannot be index in the same way giving you the error.
",<python><k-means><numpy><reshape>
"I would like to solve a non-linear system (which contains the goals of a football team in previous matches) using the Gauss-Netwon algorithm, in order to find the parameter (of frequency) to use as lambda in the simple Poisson Distribution to calculate the probability of scoring a given number of goals. The question is not about Poisson, but only about the Gauss-Newton algorithm. I would need the executable solution as Python code, because with just a few suggestions i wouldn't be able to solve the problem. Is there any library that uses the Gauss-Newton algorithm or do you have to write everything manually? Can you show me how to solve the nonlinear system with Gauss-Newton with Poisson please?
I have a nonlinear system with three equations in a single unknown. The nonlinear system was created using the analytical expression of Poisson CDF with the Distribution Function. I calculate the cumulative frequencies from the observations and use them as data. The system works properly, there are no problems. In the system are the goals of a football team in previous matches.
F(t) := P(X &lt;= t) ~ sum_i_frequency(observation_i &lt;=t) / total_observation =: f(t)

List_Goals: [1, 2, 2, 1, 2]
Matches played: 5

If the goals scored are &lt;= 0 events, then i will have 0/5 = 0;
If the goals scored are &lt;= 1 events, then I will have 2/5 = 0.4;
If the goals scored are &lt;= 2 events, then I will have 5/5 = 1;

f(0) = 0/5 = 0;
f(1) = 2/5= 0.4;
f(2) = 5/5= 1;

System: {f(0) = F(0)} therefore 0/5 = 0;
        {f(1) = F(1)} therefore 2/5 = 0.4;
        {f(2) = F(2)} therefore 5/5 = 1;

The result of this system is: 0, 0.4, 1
The system runs successfully with Python code:
import numpy as np
Goals = [1, 2, 2, 1, 2] 
probability_mass_function = np.bincount(Goals)/len(Goals)

cummulative_mass_function = probability_mass_function.cumsum()

print(&quot;probability_mass_function: &quot;, probability_mass_function)
#result: ([0. , 0.4, 0.6])

print(&quot;cummulative_mass_function: &quot;, cummulative_mass_function)
#result: ([0. , 0.4, 1. ])

GAUSS-NEWTON
Now, i would like to solve the nonlinear system using the Gauss-Netwon algorithm, in order to find the parameter to use as lambda in the simple. Poisson distribution. So I would like to solve the nonlinear system using the following Gauss-Newton algorithm:

So what I would need is:
a) I identify the Jacobian matrix J, which in general is composed of the partial derivatives with respect to all the parameters. In this case I only have one parameter and the matrix is actually a column vector;
b) I calculate the transpose of J multiplied by J, which in this case is a scalar (formally a number, which however in this case is a function of the unknown parameter);
c) algorithmically speaking it looks good: the inverse of J^T*J is still a scalar, so I have to divide by that number (which however will contain the mu parameter);
d) I write the iterative method according to the formula and a stopping criterion;
So I get the parameter (of frequency)  to use in the Poisson distribution to calculate 0 goals, 1 goal, 2 goals, 3 goals, etc.
​
Naturally, these steps therefore, include functions and functional operators (the derivative). How can I use the Gauss-Network algorithm (with Python) on the non-linear system and get the final parameter I can use in Poisson distribution?
I would need the executable solution as Python code, because with just a few suggestions I wouldn't be able to solve the problem.
Thank you all!
","Python mathematical libraries numpy and scipy have routines for solving systems of linear equations: numpy.linalg.solve and scipy.linalg.solve.
These are based on Gaussian elimination, rather than invertsing matrix (which can be achieved, e.g., by numpy.linalg.inv.)
See also Solving equations and inverting matrices for the full list of available routines.
",<python><scikit-learn><data><numpy><scipy>
"I would like to solve a non-linear system (which contains the goals of a football team in previous matches) using the Gauss-Netwon algorithm, in order to find the parameter (of frequency) to use as lambda in the simple Poisson Distribution to calculate the probability of scoring a given number of goals. The question is not about Poisson, but only about the Gauss-Newton algorithm. I would need the executable solution as Python code, because with just a few suggestions i wouldn't be able to solve the problem. Is there any library that uses the Gauss-Newton algorithm or do you have to write everything manually? Can you show me how to solve the nonlinear system with Gauss-Newton with Poisson please?
I have a nonlinear system with three equations in a single unknown. The nonlinear system was created using the analytical expression of Poisson CDF with the Distribution Function. I calculate the cumulative frequencies from the observations and use them as data. The system works properly, there are no problems. In the system are the goals of a football team in previous matches.
F(t) := P(X &lt;= t) ~ sum_i_frequency(observation_i &lt;=t) / total_observation =: f(t)

List_Goals: [1, 2, 2, 1, 2]
Matches played: 5

If the goals scored are &lt;= 0 events, then i will have 0/5 = 0;
If the goals scored are &lt;= 1 events, then I will have 2/5 = 0.4;
If the goals scored are &lt;= 2 events, then I will have 5/5 = 1;

f(0) = 0/5 = 0;
f(1) = 2/5= 0.4;
f(2) = 5/5= 1;

System: {f(0) = F(0)} therefore 0/5 = 0;
        {f(1) = F(1)} therefore 2/5 = 0.4;
        {f(2) = F(2)} therefore 5/5 = 1;

The result of this system is: 0, 0.4, 1
The system runs successfully with Python code:
import numpy as np
Goals = [1, 2, 2, 1, 2] 
probability_mass_function = np.bincount(Goals)/len(Goals)

cummulative_mass_function = probability_mass_function.cumsum()

print(&quot;probability_mass_function: &quot;, probability_mass_function)
#result: ([0. , 0.4, 0.6])

print(&quot;cummulative_mass_function: &quot;, cummulative_mass_function)
#result: ([0. , 0.4, 1. ])

GAUSS-NEWTON
Now, i would like to solve the nonlinear system using the Gauss-Netwon algorithm, in order to find the parameter to use as lambda in the simple. Poisson distribution. So I would like to solve the nonlinear system using the following Gauss-Newton algorithm:

So what I would need is:
a) I identify the Jacobian matrix J, which in general is composed of the partial derivatives with respect to all the parameters. In this case I only have one parameter and the matrix is actually a column vector;
b) I calculate the transpose of J multiplied by J, which in this case is a scalar (formally a number, which however in this case is a function of the unknown parameter);
c) algorithmically speaking it looks good: the inverse of J^T*J is still a scalar, so I have to divide by that number (which however will contain the mu parameter);
d) I write the iterative method according to the formula and a stopping criterion;
So I get the parameter (of frequency)  to use in the Poisson distribution to calculate 0 goals, 1 goal, 2 goals, 3 goals, etc.
​
Naturally, these steps therefore, include functions and functional operators (the derivative). How can I use the Gauss-Network algorithm (with Python) on the non-linear system and get the final parameter I can use in Poisson distribution?
I would need the executable solution as Python code, because with just a few suggestions I wouldn't be able to solve the problem.
Thank you all!
","I hope this is what you are looking for
I am using scipy.optimize.least_squares method which implements Gauss-Newton method. (Doc - https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.least_squares.html)
An example code I tried is below(Edited after clarification from comment)
import numpy as np
from scipy.optimize import least_squares
from scipy.stats import poisson

# Define the Poisson distribution model
def poisson_model(params, x_data):
    # params: frequency parameter for the Poisson distribution
    return np.cumsum(poisson.pmf(x_data, params))

# Define the objective function
def objective_function(params, x_data, y_data):
    # x_data: the number of goals in previous matches
    # y_data: observed cumulative frequencies
    # Poisson distribution cumulative probabilities
    expected_cumulative_probabilities = poisson_model(params[0], x_data)
    # Compute the residuals
    residuals = expected_cumulative_probabilities - y_data
    return residuals

# Example data: number of goals in each match
goals_data = np.array([1, 2, 2, 1, 2])
max_goals = np.max(goals_data)
goal_counts = np.arange(0, max_goals + 1)
observed_cumulative_frequencies = np.cumsum(np.histogram(goals_data, bins=np.arange(-0.5, max_goals + 1.5))[0])

# Initial guess for the frequency parameter
initial_frequency = 1.0
initial_params = np.array([initial_frequency])

# Use least_squares to perform Gauss-Newton optimization
result = least_squares(objective_function, initial_params, args=(goal_counts, observed_cumulative_frequencies))

# The optimized frequency parameter is in result.x
optimized_frequency = result.x[0]

print(&quot;Optimized Frequency Parameter:&quot;, optimized_frequency)

In this code, observed_cumulative_frequencies represents the cumulative frequencies of goals. For example, if the observed cumulative frequencies for each goal count are [0, 1, 4, 5, 5, 5], it means that there are no occurrences of less than 0 goals, 1 occurrence of less than 1 goal, 3 occurrences of less than 2 goals, and so on.
",<python><scikit-learn><data><numpy><scipy>
"I have a neural network that produces output vectors from input vectors. These output vectors are different depending on what input vector it is being asked to predict for. I have a dictionary that maps these output vectors to human-readable values. Since the output vector is highly unlikely to be one of the vectors for which I have mappings, I am only trying to locate the closest vector.
However, no matter what I do, the output vector always maps to the same value in the dictionary. I have tried multiple different methods of comparing the two vectors, including calculating element-by-element, using KDTrees, and using vector magnitudes. Yet, every time, no matter the input to the network, the same &quot;closest vector&quot; is being found.
I'm stumped on this one. If all the outputs are different, you'd think they'd at least map to different vectors in the dictionary, even if they aren't the &quot;right&quot; ones. Any help is much appreciated.
    inputList = np.array(inputList)
outputList = np.array(outputList)

import tensorflow
from keras.models import Sequential
from keras.layers import Dense, Input
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
import numpy as np
import keras


# Create Keras model
model = Sequential()
model.add(Dense(1, input_dim=850))
model.add(Dense(25))
model.add(Dense(25))
model.add(Dense(25))
model.add(Dense(850))

# Gradient descent algorithm
adam = Adam(0.0001)

model.compile(loss='mse', optimizer=adam)
model.build((None, 850))
model.fit(inputList, outputList, epochs=1000, callbacks=tensorflow.keras.callbacks.EarlyStopping(monitor='loss', patience=3))
model.save(r'C:\Users\lasti\Downloads\keras3')
toSearch = np.fromstring(all[&quot;cat&quot;], sep=&quot; &quot;)
toSearch = np.array([toSearch])
prediction1 = model.predict(toSearch)

all = {x: np.fromstring(all[x], sep=&quot; &quot;) for x in all}

def find_closest(x, array2d):
    least_diff = 999
    least_diff_index = -1
    for num, elm in enumerate(array2d):
        diff = [abs(x[count]-elm[count]) for count in range(850)]
        diff = sum(diff)
        if diff &lt; least_diff:
            least_diff = diff
            least_diff_index = num
    return array2d[least_diff_index]
vals = np.fromiter(all.values(), dtype=object)
closest = find_closest(prediction1.T, vals)

for x, y in all.items():
   if(np.array_equal(y, closest)):
      print(x)

Note that inputList and outputList are just lists of numpy vectors. all maps from human-readable strings (the keys) to vectors (the values).
","When you want to predict a discrete element (one of N possible outcomes), it's better not to use MSE, but to use categorical cross-entropy. For that, instead of using the vectors as expected outputs, use the indexes of the element in all. You can check any example of multiclass classification in Keras to have an example of how to do it in code.
Also, you should note that stacking multiple Dense layers is equivalent to a single Dense layer. You should include non-linearities (e.g. ReLU) in the intermediate layers. Check this answer for details on this issue.
",<keras><numpy>
"I have checked the Skewness of my data before applying a Log transformation using the next code :
print(&quot;Skewness: %f&quot; % df['Wind Speed (km/h)'].skew())

and it gave me :
Skewness: 1.113493

So my data is highly Skewed. Then, I have applied a log transformation using the next code:
log_transformed = np.log(df['Wind Speed (km/h)'])

and it gave me the next result:
Skewness: nan

I could not understand why I got this value and what should I do
","Probably your column contains zeros or negative values.
Note that np.log(0) returns -inf and np.log(-1) returns nan, which would explain your computed skewness.
",<data-cleaning><numpy><transformation>
"Could someone please suggest me what would be the best way to remove such huge number of outlier data from the image. The regular clipping between data range in numpy array would simply reduce the data shape and the reconstruction to image is not possible going on that approach.
I tried clipping the data range to one standard deviation range, but all the data including outlier got clustered in the edge.
Also, it's desirable to have the mean and standard deviation preserved with original data array after removing the outlier but not absolutely necessary.
PS:
Things to keep in mind:

Remove the outlier in range greater around -5 in the given histogram profile, also preserving the bimodal distribution and array shape(for image reconstruction).
Clipping value in range brings the data at edge distorting the original result, which is unwanted.
I tried to create masked array in numpy, but that cannot be saved as original file in rasterio.



Thank you for your inputs!
","Just a comment: if you are removing the data that you highlighted in red the mean will necessarily change. The large number of zeros probably stems from creating the rotated image where the space around it was filled with zeros. Just fill the area with different values or use a RGBA representation and make it translucent. Cropping the image is another solution but then you loose parts of the image.
Out of curiosity: what is shown in the image? Are these blood vessels?
",<python><statistics><numpy><outlier>
"I'm debugging my python program.
I have np.cos(param_t_10[0]) returns 5.9168e-06 where param_t_10[0] == tensor(-1.5708)
If I print np.cos(-1.5708) I get -3.673205103346574e-06 which is the correct value.
Can anyone explain me what am I doing wrong?
","It is not a good idea to compare floating point numbers by ==. E.g., this may give different result depending on your number representation. Additionally, the conversion from a tensor to a float/numpy object can have side effects (you could print(param_t_10[0]) and print(float(param_t_10[0])) for a first check).
",<python><numpy>
"I'm trying to write code for a force test that will output the maximum force before structural failure occurs. I'm a bit of a novice to python, so the issue here might be something simple that I'm missing. Essentially, when I run the code, it will run indefinitely without outputting any value (so far, the longest I've let it run is about 10 minutes). I've been using a fake data frame with a vector length of about 80, so I think it must be an issue with the code rather than an issue of my vector being too long/requiring a long processing time.
Here's the code:
def find_new_max(data):

    n=5
    a=1

    groups=np.array_split(data['Force'], len(data['Force'])//n)

    slope=[group.max() - group.min() for group in groups[0:a]]

    while np.all(np.array(slope)) &gt; 0:
        a=a+1

    return(max(data['Force'][0:a*n]))


find_new_max(df)

Essentially, the goal is to have the code analyze slope by looking at groups consisting of five data points each, starting with just the first group, and then moving on to the consecutive group if a negative slope is not identified. Once it finds a negative slope, the loop should stop, and it should use the new &quot;a&quot; value to search the vector up to the group with the negative slope for a maximum value.
I'm not quite sure what's wrong here, as no errors are popping up. I've also tried to have it search only one group at a time like this:
def find_new_max(data):

    n=5
    b=0
    a=1

    groups=np.array_split(data['Force'], len(data['Force'])//n)

    slope=[group.max() - group.min() for group in groups[b:a]]

    while np.all(np.array(slope)) &gt; 0:
        b=b+1     
        a=a+1

    return(max(data['Force'][0:a*n]))


find_new_max(df)

But the same problem occurs.
Any help is appreciated, thank you!
","A couple of things:

Your array is not being split into groups of 5. Your array is split into 5 groups of length = (len(data['force'])/n).
See array_split documentation: https://numpy.org/doc/stable/reference/generated/numpy.array_split.html#numpy-array-split

your slope variable is not being updated within the while loop, causing it to run indefinitely.

The np.all method is returning a boolean, not an integer. How come you decided to use the .all() method?
Set the while loop to continue until slope is negative, not a boolean.


",<python><pandas><algorithms><numpy><data-analysis>
"I'm reading several keras tutorials and found that many examples are written like this:
keras.Input(shape=(None,), dtype=&quot;int64&quot;, name=&quot;english&quot;)
Why do they use shape=(None,) instead of just shape=(None)?
I know that here None means any value. But, why are they writing that extra comma?
","The Keras expects the shape argument to be tuple. And in python if you write shape =(None) and check its datatype with type(shape) you will get the output as int. However, if you write shape=(None,), it's considered as a tuple containing one element which is None.
So basically, the extra comma is used to define a tuple with one element.
",<python><keras><scikit-learn><numpy>
"In the below code why the numbers 6 and 8 not selected?
a = np.arange(12).reshape(3,4) \
b1 = np.array([False, True, True]) \
b2 = np.array([True, False, True, False])

a[b1,b2]

","A simple solution to your numpy selector is to use
a[b1][:, b2]

The actual reason it didn't show 6 and 8 was because the two arrays you selected didn't change any of their shapes but made pairs of (x, y) instead. a[b1] will result in the shape changing from (3, 4) to (2, 4). Let's call this result x. Now you want to slice along the other axis using b2. Doing x[:, b2] will slice it another time at the other axis, changing the shape from (2, 4) to (2, 2).
In other words, the selector that you used paired b1 and b2. The pairs that were in different lists which are [1, 2] and [0, 2] to [1, 0] and [2, 2]. If you put both lists together, it can also be understood as transposing the selecting matrix from
[[1 2]
 [0 2]]

into
[[1 0]
 [2 2]]

",<numpy><indexing>
"In the below code why the numbers 6 and 8 not selected?
a = np.arange(12).reshape(3,4) \
b1 = np.array([False, True, True]) \
b2 = np.array([True, False, True, False])

a[b1,b2]

","What happens here is (roughly) the following:

Numpy looks for the indices of the True- Entries in b1and b2.
This is similar to

idx1 = np.where(b1)    # [1,2]
idx2 = np.where(b2)    # [0,2]


Numpy iterates over all elements in idx1 and idx2 in parallel. For this, it is important, that both list have the same length, i.e. b1 and b2 have the same number of Truevalues (try it with b2 = np.array([True, False, True, True]) and your code will brake).

The following code demonstrates, what happens and prints the elements of a[b1,b2]:
for i in range(len(idx1)):
    u = idx1[i]
    v = idx2[i]
    print(a[u,v])

",<numpy><indexing>
"I'm working with a pandas dataframe that has some columns like these:
Col1     Col2    Col3    Col4
Yes      No      Perhaps Not sure
Perhaps  Yes             No
No               Yes    

I'm trying to count the values of each column and then adding them together to get the total counts of that values. 
however, It's not working with columns that don't have all the values like Col4 is missing Yes and perhaps
What is the best way to approach this problem?
","import numpy as np
import pandas as pd

# Create the DataFrame.
df = pd.DataFrame({
    'Col1': ['Yes', 'Perhaps', 'No'],
    'Col2': ['No', 'Yes', np.nan],
    'Col3': ['Perhaps', np.nan, 'Yes'],
    'Col4': ['Not sure', 'No', np.nan],
})

counts = (
    df
    .apply(pd.Series.value_counts)  # 1. Count the number of occurrences of each value in each column.
    .sum(axis=1)                    # 2. Sum the counts across columns.
    .astype(int)                    # 3. Convert to integer.
    .sort_values(ascending=False)   # 4. Sort the counts in descending order.
    .to_frame('counts')             # 5. Convert to a named DataFrame.
)

&gt; print(counts)

        counts
No           3
Yes          3
Perhaps      2
Not sure     1

Number 3–5 is optional formatting.
",<python><pandas><numpy>
"Im trying to brighten and dim an image using OpenCV with two approaches.
Approach 1: Used OpenCV's add and subtract functions to brigthen and dim the image.
Approach 2: Manually added/subtracted pixel values.
Both produced different results, with the OpenCV approach producing better results than the manual as you can see in the output image below. Why is this happening?.
Code:
img = cv2.imread('New_Zealand_Coast.jpg',cv2.IMREAD_COLOR)
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

# Matrix for storing pixel alter values
mat = np.ones(img.shape, dtype='uint8') * 70

# Approach 1
img_brighter = cv2.add(img, mat)
img_dimmer = cv2.subtract(img, mat)

# Approach 2
img_brighter_manual = img + mat
img_dimmer_manual = img - mat

# Plot Approach 1
plt.figure(figsize=(20,6))
plt.subplot(131)
plt.imshow(img)
plt.title('Original 1')
plt.subplot(132)
plt.imshow(img_brighter)
plt.title('Brighter')
plt.subplot(133)
plt.imshow(img_dimmer)
plt.title('Dimmer')
plt.suptitle('Approach 1 : With CV Add/Subtract Function')

# Plot Approach 2
plt.figure(figsize=(20,6))
plt.subplot(131)
plt.imshow(img)
plt.title('Original 1')
plt.subplot(132)
plt.imshow(img_brighter_manual)
plt.title('Brighter')

plt.subplot(133)
plt.imshow(img_dimmer_manual)
plt.title('Dimmer')
plt.suptitle('Approach 2 : With manual Add/Subtract')

Output:

Original Image Array:

Modified array from Approach 1:

Modified array from Approach 2:

As you can see, there is some difference in the elements of arrays produced by the two approaches.
Here is the original image used in the code, in case you want to test with it.

","The issue is caused by the fact that the resulting value of the manual addition is larger than the maximum value it can store. The data type of the array is uint8, which can hold a maximum value of 255, whereas the resulting value of the addition (for the one shown in your screenshot) is 188 + 70 = 258. This will cause an overflow where the value will wrap around and start from zero, resulting in a value of 258 - 256 (256 because the range of values it can store is 0-255) = 2. A way to solve this problem is by performing an extra check that checks if the resulting value is lower than the original value, and if it is, set the output value to 255:
# Approach 2
img_brighter_manual = img + mat
# extra check on output values
img_brighter_manual[img_brighter_manual &lt; img] = 255

The same approach can be applied when darkening the image (i.e. subtracting values), except for the other way around.
",<python><numpy><image-preprocessing><opencv><image>
"working on Kaggle's SpaceShip Titanic dataset and I was doing some analysis of the data.
Call tt the pd.DataFrame containing the data. I sum all the monetary variables and then try to filter by both having TotalExp==0 and CryoSleep.isna(), yet in the results I get both people with CryoSleep not nan and people with TotalExp not 0
monetary = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']
tt.loc[:,'TotalExp'] = tt.loc[:,monetary].sum(axis=1)
tt.loc[np.where((tt.TotalExp == 0.0) &amp; (tt.CryoSleep.isna()))][['CryoSleep','TotalExp']]

Example of current output:




index
CryoSleep
TotalExp




1405
NaN
977.0


1417
False
0.0


1454
NaN
3862.0


1531
NaN
0.0


1565
NaN
906.0




I have tried both filters separately and the behavior is the same in each case (bringing cases that should not).
The expected behavior is that only rows where CryoSleep is NaN and TotalExp = 0, both at the same time, are displayed.
What I am missing?
","To fix the code to select the rows where CryoSleep is NaN and TotalExp is 0, you can modify the last line of the code as follows:
I work on sample DataFrame
import pandas as pd
import numpy as np

# create a sample dataframe
tt = pd.DataFrame({
    'RoomService': [10, 0, 20, 0, 30],
    'FoodCourt': [20, 0, 30, 0, 40],
    'ShoppingMall': [30, 0, 40, 0, 50],
    'Spa': [40, 0, 50, 0, 60],
    'VRDeck': [50, 0, 60, 0, 70],
    'CryoSleep': [np.NaN, 1, 0, 1, np.NaN]
})

# sum the monetary columns and assign to TotalExp column
monetary = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']
tt.loc[:, 'TotalExp'] = tt.loc[:, monetary].sum(axis=1)

# select the rows where CryoSleep is NaN and TotalExp is 0
selected_data = tt.loc[(tt['CryoSleep'].isna()) &amp; (tt['TotalExp'] == 0), ['CryoSleep', 'TotalExp']]

# print the selected data
print(selected_data)


You don't need np.where to get the result but if you still want to use that function try following code;
selected_data = tt.loc[np.where((tt['CryoSleep'].isna()) &amp; (tt['TotalExp'] == 0)), ['CryoSleep', 'TotalExp']]
```

",<python><pandas><numpy>
"It is my first GRU model so pardon the stupidity. I am trying to learn by training a simple GRU network on variable length sequences. The sequences are numpy arrays of tensors. The length of numpy array varies from sample to sample. The model generator and fit code is below:
def declare_model(emb_size, gru_size, num_classes):
    inputs = keras.Input(shape=(None, emb_size))
    gru_out = keras.layers.Bidirectional(keras.layers.GRU(gru_size, return_sequences=False))(inputs)
    gru_out = keras.layers.Flatten()(gru_out)
    predictions = keras.layers.Dense(num_classes, activation='sigmoid')(gru_out)
    model = keras.Model(inputs=inputs, outputs=predictions)
    model.compile(optimizer=keras.optimizers.Adam(), loss='binary_crossentropy', metrics=['accuracy'])
    return model
m = declare_model(emb_size=200, gru_size=20, num_classes=2)
m.fit(dafr[&quot;Data&quot;], dafr[&quot;Label&quot;], epochs=100, batch_size=32, validation_split=0.2)

The type of an element of 'dafr[&quot;Data&quot;]' is &quot;numpy.ndarray&quot; type of each element of this element is &quot;torch.Tensor&quot; shape of each tensor is &quot;200 {torch. Size([200])}&quot; and dtype of tensor is float. Type of element of 'dafr[&quot;Label&quot;]' is 'numpy.int64'. While fitting I am getting error &quot;ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).&quot;. Why is this error occurring and how can I resolve it?
","The error is happening because Keras models are designed to work with numpy arrays, not with PyTorch tensors. To resolve this issue, you need to convert the PyTorch tensors to numpy arrays. You can do this by calling the numpy() method on each tensor. For example, you can modify the code this way:
import numpy as np

dafr[&quot;Data&quot;] = np.array([x.numpy() for x in dafr[&quot;Data&quot;]])
dafr[&quot;Label&quot;] = np.array(dafr[&quot;Label&quot;])

After converting the data, you should be able to fit the model without encountering the error.
",<tensorflow><machine-learning-model><numpy><gru>
"I have pandas data frame
data=df.loc[[0]] 
print(data)
0    3.5257,3.5257,3.5257000000000005,3.5257,3.5257...
Name: testdata, dtype: object

I need to convert it to numpy array and want to plot the figure
","You can use the .values attribute of the dataframe to convert it to a numpy array, and then use the numpy array to plot the figure. Here's an example:
import matplotlib.pyplot as plt

data = df.loc[[0]].values[0]
plt.plot(data)
plt.show()

Note that in the above code, df.loc[[0]].values[0] is used to extract the numpy array from the dataframe, as the output of df.loc[[0]] is still a dataframe.
You can also use the .to_numpy() method to convert the dataframe to numpy array.
data = df.to_numpy()
plt.plot(data)
plt.show()

Also make sure that you have the matplotlib library imported and the data is of numerical values, otherwise you will get an error while plotting.
",<pandas><numpy><matplotlib>
"My numpy array has the shape of (99,2) basically it has 2 columns one is the word and the other is a hot encoding vector size of 300 I want to delete the column of words and want to have only encoding vectors
in order to delete I have to use the simple line of code
arr = np.delete(x, 0, axis=1)

it deletes the all words but gives me the shape of
(99,1)
How Can I have the shape of (99,300)
","The trick I used is as below
First del that column by the following command
arr = np.delete(x, 0, axis=1)

Second Flatten the array ad make it a list
flt= arr.flatten()
flt =list(flt)

last make the new numpy array to restore the dimensions
new_arr = np.array(flt)

it gives me the desired shape
",<word-embeddings><numpy><python-3.x><one-hot-encoding>
"My NumPy array looks like this
  array([-5.65998629e-02, -1.21844731e-01,  2.44745120e-01,  1.73819885e-01,
         -1.99641913e-01, -9.42866057e-02, ..])]
 ['آؤ_بھگت'
  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., ..])                       ]
 ['آؤلی'
  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0..])                       ]

When I want to search some specific word I use the built in function
arr_index = np.where(x == 'شعلہ_مزاجی')
print(arr_index)

print(x[arr_index])

When I print it gives the index, but not the second value
How to get the second value in numpy array?
","Update :
I was not getting the actual index value and it will also not print if the second value does not have any encodings.
arr_index = np.where(x == 'یے')

print(len(arr_index))

value = (arr_index[0])
print(value)

result = str(value)[1:-1]

result = int(result)

value = x[(result)][1]

print(value)

This will give the embeddings
",<nlp><word-embeddings><numpy>
"I am running some test code and observed something strange. the numpy power function giving strange output post 2 to the power 62
p=np.array([1,2,3,50,60,62,63,70,80,99])
np.power(2,p)

The outout of above code is
array([                   2,                    4,                    8,
       1125899906842624,  1152921504606846976,  4611686018427387904,
   -9223372036854775808,                    0,                    0,
                      0])


Curious if someone can explain what is happening here

","Numpy uses C datatype and thus it has limited precision. I guess the highest precision integer in your system is 64-bit. You can refer the documentation of NumPy to get a more detailed answer on overflow errors and extended precision.
",<python><numpy><jupyter>
"
Questions like these are difficult to debug, but you'll hopefully find more help on datascience.stackexchange.com than here

I see the suggestion that post in Data science is also a good option
Using python AI mnist to recognize my picture, trained accuracy is 97.99%, but accuracy to my img is less than 20%
I'm hoping can use MNIST doing 0~9 number recognition, and trainning accuracy rate reach up to 97%  , I thought it will be fine to reconize my pic
but predict/recognize my 2 picture as number 7
predict/recognize my 3 picture as number 6
predict/recognize my 5 picture as number 2
here is the share pic link : https://i.sstatic.net/DRH6G.jpg

import keras
from keras.datasets import mnist
import matplotlib.pyplot as plt
import PIL
from PIL import Image
(train_images,train_labels),(test_images,test_labels) = mnist.load_data()
train_images.shape
len(train_labels)
train_labels
test_images.shape
len(test_labels)
test_labels


from keras import models
from keras import layers
network = models.Sequential()
network.add(layers.Dense(512,activation='relu',input_shape=(28*28,)))
network.add(layers.Dense(10,activation='softmax'))


network.compile(optimizer='rmsprop',
                loss='categorical_crossentropy',
                metrics=['accuracy'])


train_images = train_images.reshape((60000,28*28))
train_images = train_images.astype('float32')/255
test_images = test_images.reshape((10000,28*28))
test_images = test_images.astype('float32')/255

from keras.utils import to_categorical

train_labels = to_categorical(train_labels)
test_labels = to_categorical(test_labels)

network.fit(train_images,train_labels,epochs= 3 ,batch_size=128)


test_loss , test_acc = network.evaluate(test_images,test_labels)
print('test_acc:',test_acc)



network.save('m_lenet.h5')


#########

import numpy as np
from keras.models import load_model
import matplotlib.pyplot as plt
from PIL import Image

model = load_model('/content/m_lenet.h5')

picPath = '/content/02_a.png'
img = Image.open(picPath)


reIm = img.resize((28,28),Image.ANTIALIAS)

plt.imshow(reIm)
plt.savefig('/content/result.png')

im1 = np.array(reIm.convert(&quot;L&quot;))



im1 = im1.reshape((1,28*28))


im1 = im1.astype('float32')/255


#　predict = model.predict_classes(im1)


predict_x=model.predict(im1) 
classes_x=np.argmax(predict_x,axis=1)

print (&quot;---------------------------------&quot;)

print ('predict as：')
print (predict_x)

print (&quot;&quot;)
print (&quot;&quot;)

print ('predict number as：')
print (classes_x)
print (&quot;---------------------------------&quot;)
print (&quot;Original img : &quot;)



what should I do for this?

should I also import my img with ans for AI to trainning?
add more layers?

that all the idea I came up, if there is more, just let me know? If that the only two idea to slove, also tell me how to implement (ex:import my img with ans for AI to trainning)

The discussion I read through https://stackoverflow.com/questions/69625822/mnist-trained-network-tested-with-my-own-samples
","It means that your model is overfitting- when you see that the model performs well on the training data but does not perform well on the test data.
There are various steps to deal with this type of problems.

Perform Data Augmentation such as Flip, rotate, etc..
Cross-validation
Regularization

",<python><keras><pytorch><numpy><mnist>
"Trying to use sigmoid as an activation function for the last dense layer of a LSTN, I get this error
ValueError: `logits` and `labels` must have the same shape, received ((None, 60, 1) vs (None,)).

The code is this
scaler = StandardScaler()
X_train_s = scaler.fit_transform(X_train) #scaled_train 
X_test_s = scaler.transform(X_test) #scaled_test     

length = 60
n_features=89

generator = TimeseriesGenerator(X_train_s, Y_train['TARGET_ENTRY_LONG'], length=length, batch_size=1)
validation_generator = TimeseriesGenerator(X_test_s, Y_test['TARGET_ENTRY_LONG'], length=length, batch_size=1)


# define model
model = Sequential()
model.add(LSTM(90, activation='relu', input_shape=(length, n_features), return_sequences=True, dropout = 0.3))
model.add(LSTM(30,activation='relu',return_sequences=True, dropout = 0.3))
model.add(Dense(1, activation = 'sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy')

model.summary()

# fit model

model.fit(generator,epochs=3,
                    validation_data=validation_generator)
                   #callbacks=[early_stop])

If I replace the last layer declaration with the following one
model.add(Dense(1))

I get no errors, but probably also not the expected result. Any idea?
","Found the cause of the trouble after several attempts, it was in the layer before the last one: it shall have no &quot;return_sequences=True&quot; set, that is for all the layers before if the last one is a dense layer for binary classification using sigmoid as activation function. Therefore, this layer
model.add(LSTM(30,activation='relu',return_sequences=True, dropout = 0.3))

shall be written instead as following
model.add(LSTM(30,activation='relu', dropout = 0.3))

",<python><neural-network><lstm><numpy>
"I need to create random data using this lines
n_samples = 3000

X = np.concatenate((
    np.random.normal((-2, -2), size=(n_samples, 2)),
    np.random.normal((2, 2), size=(n_samples, 2))
))

but didn't get the difference between two lines of random here . I got that this way be used to concatenate two random numbers to create 2 clusters but why one of them using (-2,-2) and the other (2,2) and does 2 in this size because concatenate using to merge 2 groups of random data or not ?
","Providing multiple values to either the loc or scale arguments can be used to generate multiple random distributions at once with different parameters. In the code you provided the values for the loc argument are the same, meaning that you could also just use the value -2 instead of (-2, -2). You can see this when fixing the seed and generating new numbers
import numpy as np

np.random.seed(0)
print(np.random.normal((-2, -2), size=(5,2)))
# [[-0.23594765 -1.59984279]
#  [-1.02126202  0.2408932 ]
#  [-0.13244201 -2.97727788]
#  [-1.04991158 -2.15135721]
#  [-2.10321885 -1.5894015 ]]

np.random.seed(0)
print(np.random.normal(-2, size=(5,2)))
# [[-0.23594765 -1.59984279]
#  [-1.02126202  0.2408932 ]
#  [-0.13244201 -2.97727788]
#  [-1.04991158 -2.15135721]
#  [-2.10321885 -1.5894015 ]]

The different between the two lines is that one is generating random noise from a normal (Gaussian) distribution with a mean of -2 and the other from a mean of 2, see also the loc keyword in the documentation.
",<python><numpy><gaussian>
"Part of our thesis project is to create a Diabetes predictor web application, and I have something I like to clarify. Is it a common practice to save an ML model as a Joblib/Pickle file like this one? I've seen a lot of notebooks on Kaggle regarding Diabetes prediction, and I rarely see anyone saving the ML model as a Joblib/Pickle file. Thanks for the help!

","Scikit Learn Documentation: 9. Model Persistence
Using joblib is the recommended way in the documentation. However keep the limitations that are mentioned there in mind.
As for whether this is advisable: If you want to use your model in real-world application and it takes a long time to train, training the model from scratch every time will take you a long time. Training and saving it, then subsequently loading it when you need it is faster. If you want to share your trained model, problems with compatibility may arise. In that case it could be better to re-train the model on the new machine and save it there instead of sharing it and causing problems.
",<python><classification><pandas><random-forest><numpy>
"I have this code that is supposed to convert an image entry of a Torchvision dataset to a base64 string. To do that, it serializes the tensor from a Torchvision dataset to a string, modifies that string, parses the string as JSON, then as a numpy array, loads that numpy array to an image and finally this image to base64.
This solution uses superfluous parsing and serialization steps, and is therefore probably slower than a more direct solution. Also, it does somehow not work only on sample 9051, which is why I am looking for a more robust solution.
How to do this properly? Can I directly convert an entry from a Torchvision dataset to a PIL image?
def get_image_as_base64_string_from_torchvision_dataset_sample(
    sample: Tensor,
) -&gt; str:
    image_tensor: Tensor = sample[0]
    nested_lists_of_floats_string = \
        remove_suffix(
            string=remove_prefix(
                string=str(image_tensor),
                prefix='tensor(',
            ),
            suffix=')',
        )
    batch_index: int = 0
    image_as_nested_list_of_floats = \
        json.loads(nested_lists_of_floats_string)[batch_index]
    image_as_nested_numpy_array: numpy.ndarray = \
        numpy.asarray(image_as_nested_list_of_floats)
    image_as_nested_numpy_array *= 255
    image: Image.Image = Image.fromarray(image_as_nested_numpy_array)
    image = image.convert(&quot;L&quot;)
    image_base64: bytes = encode_image_to_base64_bytes(image=image)
    image_base64_string: str = image_base64.decode()

    return image_base64_string

","I found you can use the ToPILImage transform outside of a dataloader:
def get_image_as_base64_string_from_torchvision_dataset_sample(
    sample: Tensor,
) -&gt; str:
    features_index: int = 0
    features: Tensor = sample[features_index]
    features *= 256
    to_pil_image_transform = transforms.ToPILImage()
    image = to_pil_image_transform(features)
    image = image.convert(&quot;L&quot;)
    image_base64: bytes = encode_image_to_base64_bytes(image=image)
    image_base64_string: str = image_base64.decode()

    return image_base64_string

This already avoids the serialization, string modification steps and drops the numpy dependency. I just tried running with this, and it produces results in line with previous runs. So it seems to be functionally identical to me. The speedup for sampling 10000 images is roughly 50 %, so this was worthwhile to me. Also, this works on every dataset entry, so it is in fact more robust.
",<pytorch><numpy><image-preprocessing><torchvision>
"I'm completely new to data science and I have a problem that I need help in resolving. I have time series data (with 87 million rows currently, though that will grow) with x, y coordinates, a timestamp (using date_trunc('hour') to enable better comparisons), and the value, stored in a Postgresql table. The analyses I need to perform on this data (find the value with the closest timestamp for one or more x,y coordinates, average the values in different ways, etc.) do not perform in Postgres at the speed I need (ideally sub 5 second response time). So I'm investigating using a multidimensional Numpy array. My problem is multiple: first, that I have no idea if this is a good idea, but I'm willing to try it and find out. More importantly, and the specific issue I need help with, is how to convert the data from the 2D tabular Postgres format to a 3D numpy array.
The data looks similar to this:
x_id    y_id    approx_time           value
 4       26     2022-10-14 08:00:00    0.01
 4       26     2022-09-03 08:00:00    0.02
...

Any suggestions on how to convert this to an array that will allow me to perform the analyses I need to perform, and secondarily, any suggestions on better paths forward if an array won't get me where I need to be? Thanks in advance
","You could export the data from the database (for example to a csv file) and load that into memory using numpy (or pandas, which may be easier as it gives you extra functions and allows you to refer to the columns using the column names instead of indices). I am not sure how much memory you have available but 87 millions rows might be than what can fit into memory at once. In this case you could use chunking, or try to optimize the performance of your SQL query (which might be a good first step nevertheless depending on the exact logic you're looking for). Other python libraries that might be interesting to look at when working with large datasets are polars and dask.
",<time-series><numpy>
"My goal is to prove why normalized eigen values and eigen vectors have imaginary number.
According to this website: normalized eigen vector is just an eigen vector divided by the length of the vector.
I can prove it by comparing the eigen vector that I got manually with det(A - λ*I) = 0 with the normalized eigen vector from the numpy library.
However, I just don't get it how the numpy library can come up with an imaginary number for the returned eigen values and vectors.
Code
import numpy as np

A = np.array([[1,-1],
              [6, 4]])
eigvalues, eigvectors = np.linalg.eig(A)
display(eigvalues, eigvectors)

output
array([2.5+1.93649167j, 2.5-1.93649167j])
array([[-0.23145502+0.29880715j, -0.23145502-0.29880715j],
       [ 0.9258201 +0.j        ,  0.9258201 -0.j        ]])

Isn't the normalized eigen vector formula is x / np.sqrt(x1^2 + xn^2)
If x = [ -1 ], then normalized x = [ -1 / √3 ]
       [ 2. ]                      [  2 / √3 ]

Edit:
I tried to manually calculate the Code.
λ = (5 +- i√15) / 2

which roughly translate to 2.5 +- 1,93i. New question: why when I call eigenvalues.real it return [2.5, 2.5] instead of whatever is the calculation of 2.5 +- 1,93i is?
","So answering your new question so
$$ \lambda = 2.5 \pm i \sqrt{15}$$
have two eigenvalues encoded in the equation is
$  \lambda_1 = 2.5 + i \sqrt{15}$ and $  \lambda_2 = 2.5 - i \sqrt{15}$.
So every complex number consists of two parts real part and an imaginary part.
Notice that the real part for the same for $\lambda_1$ and $\lambda_2$ hence when you ask for the real part it returns [2.5, 2.5].
In your case, both eigenvalues are complex hence and the eigenvector which is a tuple $( x_1, x_2)$ where $x_i$ is complex as well, but you are right you divide by the magnitude which is a real number.
",<python><numpy>
"I have a 3 data column $(X, Y, Z)$ ranges from $(min, max)$. For example,
$X = (0, 5)$, $Y=(0, 3)$, $Z=(0, 2)$. By using them I need to create a numpy array in the form of
$[(0, 0, 0), (0, 0, 1), (0, 0, 2), (0, 1, 0), (0, 1, 1), (0, 1, 2), (0, 2, 0)...]$
So in total there will be $6 \times 4 \times 3 = 72$ data points.
Is there a simple command to do this ?
","You can use itertools.product to get a possible combinations of x, y, and z and then convert to resulting list to a numpy array:
import itertools
import numpy as np

x = range(0, 5 + 1)
y = range(0, 3 + 1)
z = range(0, 2 + 1)

np.array(list(itertools.product(x, y, z)))

",<python><data><numpy>
"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
data = pd.read_csv('housing.csv')
 
data.drop('ocean_proximity', axis=1, inplace = True)

data.head()

longitude   latitude    housing_median_age  total_rooms total_bedrooms  population  households  median_income   median_house_value
0   -122.23 37.88   41.0    880.0   129.0   322.0   126.0   8.3252  452600.0
1   -122.22 37.86   21.0    7099.0  1106.0  2401.0  1138.0  8.3014  358500.0
2   -122.24 37.85   52.0    1467.0  190.0   496.0   177.0   7.2574  352100.0
3   -122.25 37.85   52.0    1274.0  235.0   558.0   219.0   5.6431  341300.0
4   -122.25 37.85   52.0    1627.0  280.0   565.0   259.0   3.8462  342200.0

X = data.iloc[:, 6:-1].values
y= data.iloc[:, -1].values

from sklearn.model_selection import train_test_split
X_train, y_train, X_test, y_test = train_test_split (X,y, test_size = 0.2, random_state = 0)

print(X_train)

[[ 65.       4.2386]
 [447.       4.3898]
 [368.       3.9333]
 ...
 [393.       3.1977]
 [468.       5.6315]
 [298.       1.3882]]

print(y_train)

[[371.       4.1518]
 [429.       5.7796]
 [534.       4.3487]
 ...
 [326.       3.2027]
 [374.       6.1436]
 [406.       3.3326]]

from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(np.array(X_train).reshape(-1, 1), y_train)
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-345-0edbf6e4cc5c&gt; in &lt;module&gt;
----&gt; 1 regressor.fit(np.array(X_train).reshape(-1, 1), y_train)

~\anaconda3\lib\site-packages\sklearn\linear_model\_base.py in fit(self, X, y, sample_weight)
    516         accept_sparse = False if self.positive else ['csr', 'csc', 'coo']
    517 
--&gt; 518         X, y = self._validate_data(X, y, accept_sparse=accept_sparse,
    519                                    y_numeric=True, multi_output=True)
    520 

~\anaconda3\lib\site-packages\sklearn\base.py in _validate_data(self, X, y, reset, validate_separately, **check_params)
    431                 y = check_array(y, **check_y_params)
    432             else:
--&gt; 433                 X, y = check_X_y(X, y, **check_params)
    434             out = X, y
    435 

~\anaconda3\lib\site-packages\sklearn\utils\validation.py in inner_f(*args, **kwargs)
     61             extra_args = len(args) - len(all_args)
     62             if extra_args &lt;= 0:
---&gt; 63                 return f(*args, **kwargs)
     64 
     65             # extra_args &gt; 0

~\anaconda3\lib\site-packages\sklearn\utils\validation.py in check_X_y(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)
    829         y = y.astype(np.float64)
    830 
--&gt; 831     check_consistent_length(X, y)
    832 
    833     return X, y

~\anaconda3\lib\site-packages\sklearn\utils\validation.py in check_consistent_length(*arrays)
    260     uniques = np.unique(lengths)
    261     if len(uniques) &gt; 1:
--&gt; 262         raise ValueError(&quot;Found input variables with inconsistent numbers of&quot;
    263                          &quot; samples: %r&quot; % [int(l) for l in lengths])
    264 

ValueError: Found input variables with inconsistent numbers of samples: [33024, 4128]
```

","You are misusing the returned tuple from train_test_split
It returns first the two X matrices and then the two y matrices.
like so:
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.33, random_state=42)

",<python><pandas><linear-regression><numpy>
"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
data = pd.read_csv('housing.csv')
 
data.drop('ocean_proximity', axis=1, inplace = True)

data.head()

longitude   latitude    housing_median_age  total_rooms total_bedrooms  population  households  median_income   median_house_value
0   -122.23 37.88   41.0    880.0   129.0   322.0   126.0   8.3252  452600.0
1   -122.22 37.86   21.0    7099.0  1106.0  2401.0  1138.0  8.3014  358500.0
2   -122.24 37.85   52.0    1467.0  190.0   496.0   177.0   7.2574  352100.0
3   -122.25 37.85   52.0    1274.0  235.0   558.0   219.0   5.6431  341300.0
4   -122.25 37.85   52.0    1627.0  280.0   565.0   259.0   3.8462  342200.0

X = data.iloc[:, 6:-1].values
y= data.iloc[:, -1].values

from sklearn.model_selection import train_test_split
X_train, y_train, X_test, y_test = train_test_split (X,y, test_size = 0.2, random_state = 0)

print(X_train)

[[ 65.       4.2386]
 [447.       4.3898]
 [368.       3.9333]
 ...
 [393.       3.1977]
 [468.       5.6315]
 [298.       1.3882]]

print(y_train)

[[371.       4.1518]
 [429.       5.7796]
 [534.       4.3487]
 ...
 [326.       3.2027]
 [374.       6.1436]
 [406.       3.3326]]

from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(np.array(X_train).reshape(-1, 1), y_train)
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-345-0edbf6e4cc5c&gt; in &lt;module&gt;
----&gt; 1 regressor.fit(np.array(X_train).reshape(-1, 1), y_train)

~\anaconda3\lib\site-packages\sklearn\linear_model\_base.py in fit(self, X, y, sample_weight)
    516         accept_sparse = False if self.positive else ['csr', 'csc', 'coo']
    517 
--&gt; 518         X, y = self._validate_data(X, y, accept_sparse=accept_sparse,
    519                                    y_numeric=True, multi_output=True)
    520 

~\anaconda3\lib\site-packages\sklearn\base.py in _validate_data(self, X, y, reset, validate_separately, **check_params)
    431                 y = check_array(y, **check_y_params)
    432             else:
--&gt; 433                 X, y = check_X_y(X, y, **check_params)
    434             out = X, y
    435 

~\anaconda3\lib\site-packages\sklearn\utils\validation.py in inner_f(*args, **kwargs)
     61             extra_args = len(args) - len(all_args)
     62             if extra_args &lt;= 0:
---&gt; 63                 return f(*args, **kwargs)
     64 
     65             # extra_args &gt; 0

~\anaconda3\lib\site-packages\sklearn\utils\validation.py in check_X_y(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)
    829         y = y.astype(np.float64)
    830 
--&gt; 831     check_consistent_length(X, y)
    832 
    833     return X, y

~\anaconda3\lib\site-packages\sklearn\utils\validation.py in check_consistent_length(*arrays)
    260     uniques = np.unique(lengths)
    261     if len(uniques) &gt; 1:
--&gt; 262         raise ValueError(&quot;Found input variables with inconsistent numbers of&quot;
    263                          &quot; samples: %r&quot; % [int(l) for l in lengths])
    264 

ValueError: Found input variables with inconsistent numbers of samples: [33024, 4128]
```

","There's something weird about the dimensions of X_train and y_train. They automatically have the same number of rows after train_test_split, but for some reason you do reshape(-1,1) on X_train. This changes the number of rows for X_train, so of course it doesn't have the same number of rows as y_train, hence the error.
Normally you shouldn't have to reshape the features, it's normal to have several features by instance.
",<python><pandas><linear-regression><numpy>
"I've created and normalized my colored image dataset of 3716 sample and size 493*491 as x_train, its type is list
I'm tring to convert it into numpy array as follows
from matplotlib import image
import numpy as np
import cv2

def prepro_resize(input_img):
  oimg=image.imread(input_img)
  return cv2.resize(oimg, (IMG_HEIGHT, IMG_WIDTH),interpolation = cv2.INTER_AREA)

x_train_ = [(prepro_resize(x_train[i])).astype('float32')/255.0 for i in range(len(x_train))]

x_train_ = np.array(x_train_) #L1
#print(x_train_.shape)

but i get the following error when L1 runs
MemoryError: Unable to allocate 10.1 GiB for an array with shape (3716, 493, 491, 3) and data type float32
","You could try the following:
1.) Convert to greyscale images instead of RGB if your application does not need RGB. Colored images consume relatively more memory than greyscale ones.
2.) Resize the images to a lower resolution than the current one
Cheers!
",<python><deep-learning><tensorflow><numpy><opencv>
"I am calculating the volatility (standard deviation) of returns of a portfolio of assets using the variance-covariance approach. Correlation coefficients and asset volatilities have been estimated from historical returns.
Now what I'd like to do is compute the average correlation coefficient, that is the common correlation coefficient between all asset pairs that gives me the same overall portfolio volatility.
I could of course take an iterative approach, but was wondering if there was something simpler / out of the box approach in numpy or pandas?
I've tried googling, but can't find anything.
Thanks
","Sklearn can calculated weighted correlation
https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html
",<python><pandas><correlation><numpy><pearsons-correlation-coefficient>
"I'm trying to compute an inner product between tensors in numpy.
I have a vector $x$ of shape (n,) and a tensor $y$ of shape d*(n,) with d &gt; 1 and would like to compute $\langle y, x^{\otimes d} \rangle$. That is, I want to compute the sum
$$\langle y, x^{\otimes d} \rangle= \sum_{i_1,\dots,i_d\in\{1,\dots,n\}}y[i_1, \dots, i_d]x[i_1]\dots x[i_d].$$
A working implementation I have uses a function to first compute $x^{\otimes d}$ and then uses np.tensordot:
def d_fold_tensor_product(x, d) -&gt; np.ndarray:
    &quot;&quot;&quot;
    Compute d-fold tensor product of a vector.
    &quot;&quot;&quot;
    assert d &gt; 1, &quot;Tensor order must be bigger than 1.&quot;

    xd = np.tensordot(x, x, axes=0)
    for _ in range(d-2):
        xd = np.tensordot(xd, x, axes=0)

    return xd

n = 10
d = 4
x = np.random.random(n)
y = np.random.random(d * (n,))
result = np.tensordot(y, d_fold_tensor_product(x, d), axes=d)

Is there a more efficient and pythonic way? Perhaps without having to compute $x^{\otimes d}$.
","Maybe not much more efficient, but shorter:
r=y
for i in range(d):
    r=r @ x

",<python><numpy>
"Why fourier transform extrapolation goes to extreme on edges but not in the middle, how to fix it
with python
&quot;&quot;&quot; Code to create the Fuorier trasfrom  &quot;&quot;&quot;
data_FT = dataset_ex_df[['Date', 'GS']]
close_fft = np.fft.fft(np.asarray(data_FT['GS'].tolist()))
fft_df = pd.DataFrame({'fft':close_fft})
fft_df['absolute'] = fft_df['fft'].apply(lambda x: np.abs(x))
fft_df['angle'] = fft_df['fft'].apply(lambda x: np.angle(x))
plt.figure(figsize=(14, 7), dpi=100)
fft_list = np.asarray(fft_df['fft'].tolist())
for num_ in [3, 6, 9, 100]:
    fft_list_m10= np.copy(fft_list); fft_list_m10[num_:-num_]=0
    plt.plot(np.fft.ifft(fft_list_m10), label='Fourier transform with {} components'.format(num_))
plt.plot(data_FT['GS'],  label='Real')
plt.xlabel('Days')
plt.ylabel('USD')
plt.title('Figure 3: Goldman Sachs (close) stock prices &amp; Fourier transforms')
plt.legend()
plt.show()


","This happens because the FFT assumes your signal is periodic. This  is why you see the reconstruction increase on the left and decrease on the right.
You can avoid this by artificially making your function periodic before taking the fft.  Mirroring your signal is one way to do this, eg:
# make a signal 
N = 64
x = np.linspace(0, 1.5 * np.pi, N)
signal = 0.5*x + np.sin( 2*x ) + 0.25*np.random.randn( N )

# mirror it (make it periodic)
signal_mirror = np.append( signal, np.flip(signal)) 

# same thing you're doing
fft = np.fft.fft(signal_mirror)
num = 9
fft[num:-num] = 0
signal_mirror_recon = np.fft.ifft(fft)

# crop it to the length we care about
result = signal_mirror_recon[0:64]


",<python><time-series><numpy>
"I have written the following code for encoding categorical features of the dataframe( named 't') -
from sklearn.compose import ColumnTransformer

categorical_columns = ['warehouse_ID', 'Product_Type','month', 'is_weekend', 'is_warehouse_closed']

transformer = ColumnTransformer(transformers= [('le',preprocessing.LabelEncoder()  ,categorical_columns)],remainder= 'passthrough')

Train_transform = transformer.fit_transform(t) 

But it is showing this error -
TypeError: fit_transform() takes 2 positional arguments but 3 were given
","You don't even necessarily need the LabelEncoder()
One simple approach using a minimal example is:
from sklearn.compose import ColumnTransformer
from sklearn import preprocessing
import pandas as pd

categorical_columns = ['warehouse_ID', 'Product_Type','month', 'is_weekend', 'is_warehouse_closed']

t = pd.DataFrame([['CategoryString'] * len(categorical_columns)], columns=categorical_columns) # create dummy t; to be removed
display(t)

t = pd.DataFrame({col: t[col].astype('category').cat.codes for col in categorical_columns}, index=t.index)

display(t)

More approaches, incl. how to use LabelEncoder() for multiple columns, can already be found here
",<machine-learning><scikit-learn><pandas><numpy>
"I have four numpy arrays X_train, y_train, X_test, y_test.
I want to know how do I export and import them all together.  I tried exporting them using
numpy.save(&quot;data.npy&quot;,list, [X_train, y_train, X_test, y_test])

I then tried importing using
np_data = numpy.load(&quot;data.npy&quot;, allow_pickle=True)

But I can't access the data, and get the following errors:

np_data says array(&lt;class 'list'&gt;, dtype=object) 
np_data[0] says IndexError: too many indices for array: array is 0-dimensional, but 1 were indexed 

and also I don't understand why allow_pickle is necessary ?
","You can simply use numpy.savez to save multiple numpy arrays to a single file. This would look as follows in your case:
numpy.savez(&quot;data.npy&quot;, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test)

",<data><numpy>
"I want to generate a random numpy ndarray of 0 and 1. I want that the number of occurrences of 1 in every specific rowto range between 2 and 5.
I tried:x = np.random.randint(0,2, size=(10, 10))yet i cannot control the number of ones.
I tried the np. random.choice() but we only can control the probability of the number. for instance, 1 will be 0.2 of the array. Yet, I want this probability to vary and be in a specific range.
I also tried this code for every row in of my ndarray.
one_count = np.random.randint(2, 5))
zero_count = colnumber - one_count
my_array = [0] * zero_count + [1] * one_count
np.random.shuffle(my_array)
Can you please help me in finding a  better solution?
","It ultimately depends on the nature of the underlying data for your simulation to represent that. For instance, if you have some type of censored Poisson process what I will show won't make sense as is.
But, one way is to generate all possible permutations that meet your criteria (here there end up being 627 possible permutations that meet {10 choose 2} + {10 choose 3} ... + {10 choose 5}). And then you can sample at random from that greater choice set.
import itertools as it
import numpy as np
np.random.seed(10)

# Lets create the whole sets of possible permutation lists
res = []
zr = np.zeros(10)
for i in range(2,6):
    for p in it.combinations(range(10),i):
        on = zr.copy()
        on[list(p)] = 1
        res.append(on.copy())

resnp = np.stack(res,axis=0)

# Now lets sample 1000 from this list
total_perms = resnp.shape[0]
samp = np.random.choice(total_perms,1000)
res_samp = resnp[samp]

# Check to make sure this is OK
np.unique(res_samp.sum(axis=1),return_counts=True)

If you had observed data, you could generate probabilities from that observed data and feed into the p probability argument for np.random.choice.
In this scenario, there are more permutations for the 10 choose 5 than there are for the 10 choose 2, if you want those types of scenarios to happen with equal probability, you would set the sum of the probabilities for the 10 choose 2 scenarios to be equal to that of the 10 choose 5.
",<python><statistics><numpy><probability><programming>
"I am looking for a way to calculate the string distance between two Pandas dataframe columns in a vectorized way. I tried distance and textdistance libraries but they require to use df.apply which is incredibly slow. Do you know any way to have a string distance using only column operations ?
Thanks
","I found here that performance across string distance libraries varies greatly : https://github.com/life4/textdistance#benchmarks
The python-Levenshtein library is lightning fast compared to the others so I will use this one. If it's not sufficient I will use parallelism as suggested by @Peter
",<pandas><preprocessing><numpy><distance>
"I am looking for a way to calculate the string distance between two Pandas dataframe columns in a vectorized way. I tried distance and textdistance libraries but they require to use df.apply which is incredibly slow. Do you know any way to have a string distance using only column operations ?
Thanks
","I have a similar problem and tried parallel computing using joblib. In terms of performance this approach seems okay. However, it appears that joblib &quot;blocks&quot; RAM memory when repeated very often. So I'm open for alternatives (or suggestions how to terminate the parallel job properly).
from joblib import Parallel, delayed
import distance
import pandas as pd
# https://joblib.readthedocs.io/en/latest/generated/joblib.Parallel.html

# Define some distance measure
def calc_dist(myrow):
    return distance.levenshtein(myrow[0], myrow[1])

# Some fake data
df = pd.DataFrame({
     &quot;text1&quot;:[&quot;some text&quot;,&quot;foo&quot;,&quot;bar&quot;,&quot;new text&quot;,&quot;more words&quot;], 
     &quot;text2&quot;:[&quot;text&quot;,&quot;hello&quot;,&quot;bar&quot;,&quot;bar&quot;,&quot;move words&quot;]})

# Columns to lists / zip them 
l1=df['text1'].tolist()
l2=df['text2'].tolist()
nlist = list(zip(l1,l2))

# Calculate distances
dist_vec = Parallel(n_jobs=2)(delayed(calc_dist)(i) for i in nlist)

print(dist_vec)
&gt; [5, 4, 0, 8, 1]

",<pandas><preprocessing><numpy><distance>
"I have a large data set in .txt form, and was trying to split it into three for testing, validation, and training by transforming the data into a .npy form then load it and use it on my model.
Is such a thing even possible? And if so, what should I do to make it happen?
Please excuse if my question is not logical or unreasonable, I am just trying to learn stuff.
the data should look something like this, keep in mind it is a huge amount of data

the first 2 lines as requested:
0.60927    -0.35816    -0.10597    0.095495           0    0.063161   -0.064612           0           0   -0.053995  -0.0051178           0           0           0           0           0     0.96824     0.32552     0.44348    0.011149           0     0.13371   0.0014431           0           0    0.074547  -0.0050623           0           0           0           0           0
0.65083    0.090205     0.05407    0.046624           0     0.14346   -0.086913           0           0    -0.01106   0.0072273           0           0           0           0           0    0.057936      0.5131     0.18093   -0.089536           0    0.023838   0.0032454           0           0     0.10316 -0.00034063           0           0           0           0           0
","You can read the txt like this. Note that I added header=None because I assume your file does NOT contain a header line, but please remove it if it does have it.
import pandas as pd
data = pd.read_csv('file_path.txt', sep=' ', header=None)

Then convert it to the numpy array format
data_npy = data.values

(Optional) Shuffle it.
import numpy as np
np.random.seed(1)
np.random.shuffle(data_npy)

Split it into three sets, in a ratio of 6:2:2
num_rows = data_npy.shape[0]
data_npy1 = data_npy[:int(num_rows*.6)]
data_npy2 = data_npy[int(num_rows*.6): int(num_rows*.8)]
data_npy3 = data_npy[int(num_rows*.8):]

And lastly save them.
np.save('file_path1.npy', data_npy1)
np.save('file_path2.npy', data_npy2)
np.save('file_path3.npy', data_npy3)

UPDATE
Check number of entities per line
num_lines = 10

with open('file_path.txt', 'r') as f:
    for i in range(num_lines):
        num_entities = f.readline().split(' ')
        print(f'Line {i} has {num_entities} entities')

",<python><deep-learning><dataset><numpy>
"I have a large data set in .txt form, and was trying to split it into three for testing, validation, and training by transforming the data into a .npy form then load it and use it on my model.
Is such a thing even possible? And if so, what should I do to make it happen?
Please excuse if my question is not logical or unreasonable, I am just trying to learn stuff.
the data should look something like this, keep in mind it is a huge amount of data

the first 2 lines as requested:
0.60927    -0.35816    -0.10597    0.095495           0    0.063161   -0.064612           0           0   -0.053995  -0.0051178           0           0           0           0           0     0.96824     0.32552     0.44348    0.011149           0     0.13371   0.0014431           0           0    0.074547  -0.0050623           0           0           0           0           0
0.65083    0.090205     0.05407    0.046624           0     0.14346   -0.086913           0           0    -0.01106   0.0072273           0           0           0           0           0    0.057936      0.5131     0.18093   -0.089536           0    0.023838   0.0032454           0           0     0.10316 -0.00034063           0           0           0           0           0
","import numpy as np
file_path = 'data.txt'
data = np.loadtxt(file_path)
",<python><deep-learning><dataset><numpy>
"from tensorflow import keras
from keras.models import load_model
from PIL import Image, ImageOps
import numpy as np

model = './ C:/Parth/cs11/converted_keras/saved_model.h5'


data = np.ndarray(shape=(1, 224, 224, 3), dtype=np.float32)

image = Image.open('C:/cs11/pic.jpg')

size = (224, 224)
image = ImageOps.fit(image, size, Image.ANTIALIAS)

image_array = np.asarray(image)

normalized_image_array = (image_array.astype(np.float32) / 127.0) - 1

data[0] = normalized_image_array

prediction = model.predict(data)
print(prediction)

And the error i am receiving is
  File &quot;cs11.py&quot;, line 22, in &lt;module&gt;
    prediction = model.predict('data')
AttributeError: 'str' object has no attribute 'predict

I  am not able to understand why it is happening so even if data is an np.ndarray. Could someone please tell me a way around this? Or an alternative method for it?
","The error tells you, you are trying to use the predict method on the model variable, but model is a string instead of a tensorflow/keras model which does not have this method. You therefore need to use load_model and pass it the location of the model file, which should return you a tensorflow model using which you can then use predict.
",<machine-learning><python><keras><tensorflow><numpy>
"I'm trying to split my x and y into train and test data for my ML model but it's giving me this error: ValueError: Found input variables with inconsistent numbers of samples: [6, 366]. My numpy array x looks like this:
array([[2, 3, 4, ..., 31, 1, 2],
       [1, 1, 1, ..., 12, 1, 1],
       [2021, 2021, 2021, ..., 2021, 2022, 2022],
       [53, 53, 1, ..., 52, 52, 52],
       [1, 1, 1, ..., 4, 1, 1],
       [1, 1, 1, ..., 1, 0, 1]], dtype=object)

x.shape: (6, 366)
My y numpy array:
array([ 774.534973,  975.50769 , ... ,  3824.19873 ])

With shape: (366,)
","You should probably transpose your x array since the first dimension should correspond to the number of samples in your dataset, currently the first dimension represents the number of features instead of the number of samples. The following should work:
import numpy as np
from sklearn.model_selection import train_test_split
# generate random data with same shape as your data
X, y = np.random.randn(6, 366), np.random.randn(366)
print(X.shape, y.shape)
# (6, 366) (366,)

# transpose features array to make sure first dimension corresponds to number of samples
X = X.T
print(X.shape, y.shape)
# (366, 6) (366,)
X_train, X_test, y_train, y_test = train_test_split(X, y)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)
# (274, 6) (92, 6) (274,) (92,)

",<machine-learning><scikit-learn><data-science-model><numpy>
"I'm trying to do cross-validation with One Class Classification - I'm using the PyOD lib - but I don't know if I'm doing it right. The precision is too low and I'm also not able to bring up the mean and standard deviation of F1
from sklearn.model_selection import KFold 
from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score
from sklearn.metrics import precision_recall_fscore_support as score


k = 10
kf = KFold(n_splits=k, random_state=None)

acc_score = []
pr_score = []
rc_score = []
model = clf
 
for train_index , test_index in kf.split(X, Y):
    x_train , x_test = X.iloc[train_index,:],X.iloc[test_index,:]
    y_train , y_test = Y[train_index] , Y[test_index]
     
    model.fit(x_train,y_train)
    pred_values = model.predict(x_test)
  
       
    acc = accuracy_score(pred_values , y_test)
    acc_score.append(acc)
    avg_acc_score = sum(acc_score)/k
    pr = precision_score(pred_values, y_test)
    pr_score.append(pr)
    avg_pr_score=sum(pr_score)/k
    rc = recall_score(pred_values, y_test)
    rc_score.append(rc)
    avg_rc_score=sum(rc_score)/k
    
print('accuracy of each fold - {}'.format(acc_score))
print('Avg accuracy : {}'.format(avg_acc_score))
print('Precision: {}'.format(avg_pr_score))
print('Recall: {}'.format(avg_rc_score))

","The problem seems to be that you are dividing by 10 (k) at each iteration, I can think to try to calculate the average, this is incorrect and probably it is what is causing you to see a very low metric value. It would be simpler and correct, to only store the values for the metric in each iteration at the validation set and finally just calculate the average and std of the list in which you stored the values.
Try:
import numpy as np
from sklearn.model_selection import KFold 
from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score
from sklearn.metrics import precision_recall_fscore_support as score


k = 10
kf = KFold(n_splits=k, random_state=None)

acc_score = []
pr_score = []
rc_score = []
model = clf
 
for train_index , test_index in kf.split(X, Y):
    x_train , x_test = X.iloc[train_index,:],X.iloc[test_index,:]
    y_train , y_test = Y[train_index] , Y[test_index]
     
    model.fit(x_train,y_train)
    pred_values = model.predict(x_test)
  
       
    acc = accuracy_score(pred_values , y_test)
    acc_score.append(acc)
    pr = precision_score(pred_values, y_test)
    pr_score.append(pr)
    rc = recall_score(pred_values, y_test)
    
print(f'accuracy of each fold - {acc_score}')
print(f'Avg accuracy :{np.mean(acc_score)} ± {np.std(acc_score)} std')
print(f'Precision:{np.mean(pr_score)} ± {np.std(pr_score)} std')
print(f'Recall:{np.mean(rc_score)} ± {np.std(rc_score)} std')

",<machine-learning><python><scikit-learn><data><numpy>
"I have a Tensorflow model weight file that I am using to make the prediction on test images. These test images are in NumPy array format and the shapes of the images are (720, 1280, 3).
I am getting the following error while making the prediction-
ValueError: Input 0 is incompatible with layer model: expected shape=(None, 416, 416, 3), found shape=(1, 720, 1280, 3)

When I tried to change the shape like below-
image_np.shape=(416,416,3)

It is giving me the following error-
ValueError: cannot reshape array of size 2764800 into shape (416,416,3)

I am using Tensorflow 2.x with Python 3.7.
Please help me to resolve this issue.
","The issue is that the model expects images of 416 by 416 pixels, whereas you are using larger images. Simply using reshape doesn't work since the overall number of pixels is still to high for a 416x416 image (720 * 1280 &gt; 416 * 416). Therefore you have to resize your image first to 416x416 before passing it to your model. You can either directly resize to 416x416 (which would give an image with a different aspect ratio) or resize first but retain the aspect ratio and then pad the image to get to 416x416. You could use the resize function from the cv2 library to resize an image.
",<tensorflow><numpy><python-3.x>
"I have a Tensorflow model weight file that I am using to make the prediction on test images. These test images are in NumPy array format and the shapes of the images are (720, 1280, 3).
I am getting the following error while making the prediction-
ValueError: Input 0 is incompatible with layer model: expected shape=(None, 416, 416, 3), found shape=(1, 720, 1280, 3)

When I tried to change the shape like below-
image_np.shape=(416,416,3)

It is giving me the following error-
ValueError: cannot reshape array of size 2764800 into shape (416,416,3)

I am using Tensorflow 2.x with Python 3.7.
Please help me to resolve this issue.
","you can use use &quot;change mode&quot; to change the channel into channel first or channel last format.
from tensorflow.keras.preprocessing.image import  img_to_array
y = np.zeros((len(ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.float32)
for n, id_ in tqdm(enumerate(ids), total=len(ids)):
    # Load images
    img = load_img(path + '/images/' + id_, color_mode=&quot;rgb&quot;)
    img = img_to_array(img,data_format='channels_first')

",<tensorflow><numpy><python-3.x>
"import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

observations = 1000
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs = np.random.uniform(-10, 10, (observations,1))
inputs = np.column_stack((xs,zs))
noise = np.random.uniform(-1, 1, (observations,1))
targets = 2*xs - 3*zs + 5 + noise

targets = targets.reshape(observations,)
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.plot(xs, zs, targets)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

ax.view_init(azim=100)

plt.show()

targets = targets.reshape(observations,1)

I'm receiving the error for the above block of code:
ValueError: operands could not be broadcast together with remapped shapes [original-&gt;remapped]: (1000,)  and requested shape (1000,1)

---&gt; ax.plot(xs, zs, targets)

","This is more of a programming question than a data science question and would therefore be better suited for the stackoverflow stackexchange page. However, the code you provided works perfectly fine for me, it gives me the following plot:

",<machine-learning><data-science-model><numpy>
"import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

observations = 1000
xs = np.random.uniform(low=-10, high=10, size=(observations,1))
zs = np.random.uniform(-10, 10, (observations,1))
inputs = np.column_stack((xs,zs))
noise = np.random.uniform(-1, 1, (observations,1))
targets = 2*xs - 3*zs + 5 + noise

targets = targets.reshape(observations,)
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.plot(xs, zs, targets)

# Set labels
ax.set_xlabel('xs')
ax.set_ylabel('zs')
ax.set_zlabel('Targets')

ax.view_init(azim=100)

plt.show()

targets = targets.reshape(observations,1)

I'm receiving the error for the above block of code:
ValueError: operands could not be broadcast together with remapped shapes [original-&gt;remapped]: (1000,)  and requested shape (1000,1)

---&gt; ax.plot(xs, zs, targets)

","Reshape the xs and zs variables as well as matplotlib no longer supports scalar input in (1,1,N) format.
Reshape xs and zs input variables too along with target variable.
Add this 2 lines below the target reshaping variable it will work:
xs = xs.reshape(observations,)
zs = zs.reshape(observations,)
Note: Once done with plotting, remember to reshape all variables back to original scalar array form for optimization algorithm to work.
",<machine-learning><data-science-model><numpy>
"xtrain is a numpy array
from sklearn.linear_model import LogisticRegression

outer_kfold = KFold(n_splits=5, random_state=27, shuffle=True)
final_scores = list()
for train, test in outer_kfold.split(xtrain):
    x_train, x_test = xtrain[train], xtrain[test]
    y_train, y_test = ytrain[train], ytest[test]
    
    model=LogisticRegression()
    model.fit(x_train, y_train)
    
    preds = model.predict(x_test)
    
    final_scores.append(accuracy_score(y_test, preds))
    print(&quot;Score:&quot;, final_scores[-1])
    print(&quot;\nAverage Score:&quot;, np.average(final_scores))
        
    model=LogisticRegression()
    model.fit(x_train, y_train)
        
    preds = model.predict(x_test)
        
    final_scores.append(accuracy_score(y_test, preds))
    print(&quot;Score:&quot;, final_scores[-1])
    print(&quot;\nAverage Score:&quot;, np.average(final_scores))




 Error
    ---------------------------------------------------------------------------
    KeyError                                  Traceback (most recent call last)
    /var/folders/vd/lb0gkn7j2t34s4ljbgdb7g5r0000gn/T/ipykernel_1376/2946208749.py in &lt;module&gt;
          5 for train, test in outer_kfold.split(xtrain):
          6     x_train, x_test = xtrain[train], xtrain[test]
    ----&gt; 7     y_train, y_test = ytrain[train], ytest[test]
          8 
          9     model=LogisticRegression()
    
    /opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/pandas/core/series.py in __getitem__(self, key)
        964             return self._get_values(key)
        965 
    --&gt; 966         return self._get_with(key)
        967 
        968     def _get_with(self, key):
    
    /opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/pandas/core/series.py in _get_with(self, key)
        999             #  (i.e. self.iloc) or label-based (i.e. self.loc)
       1000             if not self.index._should_fallback_to_positional():
    -&gt; 1001                 return self.loc[key]
       1002             else:
       1003                 return self.iloc[key]
    
    /opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/pandas/core/indexing.py in __getitem__(self, key)
        929 
        930             maybe_callable = com.apply_if_callable(key, self.obj)
    --&gt; 931             return self._getitem_axis(maybe_callable, axis=axis)
        932 
        933     def _is_scalar_access(self, key: tuple):
    
    /opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/pandas/core/indexing.py in _getitem_axis(self, key, axis)
       1151                     raise ValueError(&quot;Cannot index with multidimensional key&quot;)
       1152 
    -&gt; 1153                 return self._getitem_iterable(key, axis=axis)
       1154 
       1155             # nested tuple slicing
    
    /opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/pandas/core/indexing.py in _getitem_iterable(self, key, axis)
       1091 
       1092         # A collection of keys
    -&gt; 1093         keyarr, indexer = self._get_listlike_indexer(key, axis)
       1094         return self.obj._reindex_with_indexers(
       1095             {axis: [keyarr, indexer]}, copy=True, allow_dups=True
    
    /opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/pandas/core/indexing.py in _get_listlike_indexer(self, key, axis)
       1312             keyarr, indexer, new_indexer = ax._reindex_non_unique(keyarr)
       1313 
    -&gt; 1314         self._validate_read_indexer(keyarr, indexer, axis)
       1315 
       1316         if needs_i8_conversion(ax.dtype) or isinstance(
    
    /opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/pandas/core/indexing.py in _validate_read_indexer(self, key, indexer, axis)
       1375 
       1376             not_found = list(ensure_index(key)[missing_mask.nonzero()[0]].unique())
    -&gt; 1377             raise KeyError(f&quot;{not_found} not in index&quot;)
       1378 
       1379 
    
    KeyError: '[7, 39, 44, 45, 54, 58, 74, 79, 82, 90, 94, 98, 99, 103, 108, 115, 116, 119, 130, 134, 147, 157, 159, 161, 177, 186, 188, 192, 201, 203, 217, 218, 219, 220, 229, 231, 234, 239, 248, 250, 254, 273, 276, 285, 288, 291, 299, 301, 305, 307, 308, 313, 314, 321, 324, 325, 331, 342, 343, 345, 347, 354, 368, 372, 386, 394, 407, 413, 433, 438, 442, 447, 448, 462, 470, 481, 487, 496, 503, 507, 513, 517, 536, 538, 545, 554, 558, 559, 569, 573, 574, 587, 589, 592, 596, 599, 602, 605, 607, 608, 623, 627, 634, 642, 644, 650, 654, 662, 664, 666, 675, 687, 691, 705, 712, 714, 716, 717, 718, 724, 729, 755, 758, 761, 781, 783, 793, 802, 813, 822, 823, 847, 858, 859, 863, 867, 871, 874, 895, 900, 911, 925, 941, 946, 951, 955, 958, 961, 963, 968, 975, 982, 995, 1002, 1005, 1028, 1038, 1041, 1049, 1050, 1051, 1060, 1063, 1070, 1071, 1073, 1075, 1089, 1117, 1122, 1139, 1144, 1145, 1153, 1155, 1156, 1159, 1160, 1162, 1167, 1169, 1172, 1177, 1178, 1183, 1190, 1197, 1198, 1213, 1217, 1221, 1224, 1225, 1227, 1233, 1253, 1256, 1262, 1264, 1266, 1271, 1282, 1287, 1288, 1289, 1297, 1299, 1305, 1307, 1308, 1322, 1328, 1331, 1332, 1342, 1347, 1356, 1373, 1380, 1392, 1406, 1409, 1416, 1422, 1447, 1454, 1458, 1463, 1494, 1508, 1513, 1519, 1520, 1525, 1532, 1534, 1536, 1540, 1544, 1550, 1554, 1564, 1578, 1604, 1614, 1620, 1624, 1628, 1652, 1657, 1664, 1676, 1682, 1683, 1685, 1686, 1692, 1703, 1711, 1717, 1720, 1729, 1732, 1735, 1736, 1740, 1743, 1745, 1748, 1750, 1753, 1758, 1768, 1773, 1785, 1790, 1793, 1814, 1817, 1827, 1828, 1829, 1851, 1857, 1873, 1885, 1892, 1909, 1917, 1924, 1929, 1933, 1939, 1945, 1947, 1956, 1958, 1961, 1971, 1988, 1989, 1990, 2017, 2019, 2025, 2031, 2040, 2045, 2054, 2064, 2065, 2089, 2090, 2097, 2098, 2119, 2120, 2134, 2136, 2140, 2157, 2161, 2164, 2168, 2174, 2181, 2196, 2209, 2229, 2252, 2255, 2262, 2271, 2285, 2296, 2325, 2328, 2331, 2333, 2345, 2349, 2351, 2353, 2355, 2362, 2363, 2364, 2377, 2378, 2385, 2398, 2402, 2403, 2407, 2408, 2411, 2437, 2438, 2445, 2450, 2456, 2458, 2481, 2487, 2488, 2489, 2491, 2495, 2505, 2512, 2514, 2532, 2537, 2539, 2550, 2567, 2570, 2573, 2586, 2588, 2593, 2597, 2605, 2606, 2608, 2615, 2619, 2624, 2637, 2639, 2640, 2645, 2652, 2653, 2656, 2657, 2666, 2677, 2684, 2688, 2694, 2696, 2698, 2701, 2702, 2704, 2706, 2711, 2722, 2723, 2735, 2736, 2755, 2773, 2774, 2776, 2787, 2800, 2807, 2812, 2815, 2820, 2827, 2831, 2837, 2842, 2856, 2858, 2861, 2864, 2866, 2868, 2878, 2883, 2887, 2889, 2897, 2899, 2900, 2901, 2909, 2917, 2918, 2919, 2921, 2927, 2929, 2932, 2939, 2954, 2959, 2981, 2989, 2999, 3001, 3005, 3014, 3016, 3023, 3032, 3039, 3053, 3069, 3072, 3079, 3080, 3081, 3092, 3095, 3099, 3100, 3108, 3111, 3112, 3126, 3134, 3140, 3144, 3153, 3157, 3165, 3167, 3191, 3196, 3198, 3207, 3210, 3211, 3224, 3234, 3242, 3248, 3265, 3272, 3283, 3285, 3287, 3291, 3293, 3304, 3329, 3338, 3339, 3369, 3370, 3371, 3376, 3382, 3384, 3391, 3397, 3419, 3422, 3423, 3426, 3427, 3431, 3435, 3455, 3458, 3461, 3463, 3472, 3473, 3477, 3484, 3485, 3489, 3491, 3492, 3498, 3500, 3502, 3504, 3505, 3511, 3516, 3522, 3531, 3532, 3554, 3563, 3565, 3571, 3585, 3588, 3593, 3595, 3611, 3619, 3628, 3636, 3644, 3645, 3658, 3662, 3663, 3665, 3669, 3675, 3680, 3689, 3690, 3692, 3696, 3715, 3716, 3729, 3737, 3738, 3741, 3755, 3761, 3762, 3767, 3769, 3771, 3777, 3784, 3789, 3792, 3801, 3802, 3803, 3807, 3808, 3811, 3812, 3816, 3819, 3823, 3829, 3830, 3832, 3834, 3835, 3849, 3862, 3865, 3866, 3872, 3878, 3891, 3897, 3901, 3903, 3906, 3916, 3920, 3925, 3928, 3935, 3938, 3943, 3945, 3954, 3963, 3979, 3985, 3986, 3988, 3993, 4008, 4023, 4029, 4040, 4045, 4051, 4058, 4060, 4067, 4071, 4076, 4078, 4084, 4086, 4089, 4098, 4106, 4109, 4113, 4117, 4124, 4133, 4140, 4145, 4152, 4154, 4158, 4165, 4175, 4184, 4192, 4194, 4195, 4203, 4205, 4207, 4224, 4227, 4228, 4230, 4232, 4236, 4253, 4258, 4261, 4268, 4269, 4272, 4276, 4288, 4290, 4295, 4296, 4301, 4305, 4306, 4322, 4326, 4331, 4332, 4364, 4367, 4369, 4370, 4381, 4382, 4396, 4399, 4406, 4410, 4422, 4423, 4424, 4425, 4429, 4437, 4444, 4445, 4446, 4460, 4462, 4464, 4479, 4481, 4482, 4484, 4486, 4487, 4492, 4495, 4497, 4503, 4510, 4514, 4520, 4530, 4544, 4546, 4556, 4557, 4558, 4563, 4569, 4571, 4575, 4576, 4583, 4586, 4589, 4591, 4594, 4599, 4613, 4621, 4627, 4629, 4636, 4646, 4649, 4652, 4656, 4661, 4673, 4678, 4679, 4685, 4688, 4695, 4698, 4705, 4706, 4708, 4714, 4727, 4728, 4732, 4736, 4737, 4741, 4744, 4748, 4757, 4760, 4797, 4844, 4846, 4848, 4859, 4870, 4874, 4888, 4897, 4907, 4914, 4925, 4926, 4928, 4932, 4939, 4951, 4953, 4954, 4958, 4964, 4966, 4976, 4977, 4980, 4983, 4984, 4992, 5005, 5013, 5014, 5021, 5041, 5042, 5051, 5053, 5063, 5065, 5066, 5069, 5073, 5078, 5080, 5083, 5084, 5089, 5090, 5112, 5114, 5116, 5118, 5137, 5142, 5160, 5163, 5173, 5180, 5189, 5196, 5198, 5206, 5207, 5210, 5211, 5216, 5221, 5224, 5232, 5234, 5240, 5241, 5243, 5244, 5247, 5260, 5261, 5265, 5281, 5282, 5289, 5290, 5301, 5309, 5311, 5316, 5319, 5323, 5328, 5329, 5345, 5347, 5355, 5364, 5365, 5368, 5375, 5376, 5388, 5389, 5402, 5407, 5408, 5414, 5418, 5430, 5433, 5438, 5443, 5451, 5455, 5460, 5461, 5476, 5477, 5478, 5483, 5487, 5488, 5492, 5493, 5495, 5509, 5513, 5522, 5531, 5535, 5549, 5552, 5555, 5557, 5558, 5564, 5570, 5583, 5590, 5592, 5599, 5603, 5604, 5615, 5629, 5638, 5640, 5641, 5644, 5648, 5651, 5668, 5669, 5676, 5680, 5682, 5684, 5689, 5699, 5727, 5729, 5735, 5736, 5738, 5748, 5756, 5757, 5761, 5764, 5765, 5774, 5792, 5793, 5796, 5799, 5803, 5817, 5822, 5824, 5832, 5833, 5838, 5872, 5877, 5878, 5887, 5888, 5899, 5910, 5915, 5916, 5921, 5944, 5946, 5950, 5959, 5987, 5989, 5991, 5992, 5993, 5996, 6005, 6012, 6013, 6021, 6027, 6034, 6037, 6042, 6046, 6049, 6054, 6055, 6058, 6061, 6063, 6064, 6070, 6075, 6077, 6083, 6087, 6091, 6102, 6117, 6126, 6129, 6137, 6147, 6149, 6160, 6168, 6170, 6181, 6184, 6189, 6190, 6191, 6192, 6200, 6211, 6213, 6215, 6219, 6228, 6233, 6238, 6240, 6252, 6257, 6260, 6262, 6274, 6275, 6279, 6280, 6302, 6303, 6305, 6315, 6335, 6364, 6366, 6368, 6377, 6384, 6385, 6386, 6392, 6397, 6404, 6418, 6426, 6442, 6446, 6450, 6453] not in index'

","Replace $7^{th}$ row with:
y_train, y_test = ytrain[train], ytrain[test]

",<scikit-learn><pandas><numpy>
"I have used np.log(data) and then applied data.diff() to transform my data in timeseries model. I have the predictions. How do I convert it back to normal scale?
Here is an example for your reference:
--------------------------------------------------------------------
| sales     | np.log(sales) | (np.log(sales)).diff() | predictions |
--------------------------------------------------------------------
|166.594019 | 5.115560      | -0.045918              | -0.045918   |
--------------------------------------------------------------------

Note: I have provided only one example which from index 2 as the first value after data.diff() will be null. And hence the prediction at index 1 is 0.
","As far as I understand the difference is $\log(s_t)-\log(s_{t-1})$, right?
I'm not sure that doing the diff on the log value is the best option but this means:
$$\log(s_t)-\log(s_{t-1})=\log\left(\frac{s_t}{s_{t-1}}\right)$$
You could use exp to go back to the regular ratio:
$$\exp\left(\log\left(\frac{s_t}{s_{t-1}}\right)\right)=\frac{s_t}{s_{t-1}}$$
",<python><time-series><numpy><arima><transformation>
"Lets assume I generate a random set of target labels for a binary classification with N elements and a certain frequency of the positive class (1), e.g. 10%:
targets =     [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, ...]

I now want to generate fake classification predictions such that the overall predictions roughly satisfy desired precision and recall values e.g. precision = 0.8 and recall = 0.2. I don't need exact values but like to get close such as 0.78 and 0.25.
predictions = [ 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, ...]

Is there a way to do this efficiently in python/numpy, ideally so that I can produce multiple sets of predictions by repeating the generation process with different random seeds?
","Sure, you can just calculate how many instances are needed for each classification status based on the constraints. For example:

gold positive = 10%, i.e. $TP+FN=0.1 \times N$
recall = 0.2, i.e. $TP/(TP+FN)=0.2$, so $FN=4\times TP$

From these two equations we get $TP=0.02 \times N$ and $FN=0.08 \times N$

precision = 0.8, i.e. $TP/(TP+FP)=0.8$ so $FP=0.25\times TP = 0.005 \times N$
Finally  $TP+FP+FN+TN=N$ so $TN=N-(TP+FP+FN)=0.895 \times N$

Let's say $N=1000$, we want to have 20 TP, 80 FN, 5 FP and 895 TN.
You should be able to write a code which does these calculations and then generates a list of predictions which match these constraints.
Note that technically this is not a model (not even a fake one) because there are no features and no real classification happening.
",<python><numpy><model-evaluations>
"I want to impute a numerical feature using the median, but the median for that feature is 0 and mean = 106. Should I go ahead and impute or is there anything else I can do?
PS: I don't want to create a new binary variable to capture the missingness of the data. I only want to impute using mean or median.
Edit 1: I have a regression task at hand, predicting house price. The variable in question carries the info about Masonry veneer area in square feet. These stats are obtained after splitting the data into train and test. There are 6 nan values. Below are some relevant stats:

count   1162.000000
mean     106.538726
std      185.924370
min     0.000000
25%        0.000000
50%        0.000000
75%      164.000000
max     1600.000000


Edit 2: The reason I chose median is because after imputing, it doesn't distort the histogram and both the curves (before imputing and after imputing) are the same.
","Another option is to impute values based on sampling from the empirical distribution. That would not &quot;distort the histogram&quot;.
",<python><numpy><data-imputation>
"I want to impute a numerical feature using the median, but the median for that feature is 0 and mean = 106. Should I go ahead and impute or is there anything else I can do?
PS: I don't want to create a new binary variable to capture the missingness of the data. I only want to impute using mean or median.
Edit 1: I have a regression task at hand, predicting house price. The variable in question carries the info about Masonry veneer area in square feet. These stats are obtained after splitting the data into train and test. There are 6 nan values. Below are some relevant stats:

count   1162.000000
mean     106.538726
std      185.924370
min     0.000000
25%        0.000000
50%        0.000000
75%      164.000000
max     1600.000000


Edit 2: The reason I chose median is because after imputing, it doesn't distort the histogram and both the curves (before imputing and after imputing) are the same.
","It's hard to say without seeing the distribution of the data, it would be useful if you added that, and described what the variables actually represent.
If your data is really skewed, then even if a median of 0 sounds like the wrong thing to do, it is better than using the mean since doing so will place more emphasis on the outliers in your data.
In general though, any form of imputation will be adding biases to your dataset, and if the number of missing samples is small then it may be worth ignoring those samples in your analysis.
When you impute missing values with the mean, median or mode you are assuming that the thing you're imputing has no correlation with anything else in the dataset, which is not always true.
Consider this example:
x1 = [1,2,3,4]

x2 = [1,4,?,16]

y = [3, 8, 15, 24]

For this toy example, $y = 2x_1 + x_2$. We also know that $x_2 = x_1^2$. Now suppose we wanted to do regression on X = [x1, x2] to determine the coefficients that give us the best y_hat.
If we imputed x2 with the mean or the mode, i.e. 7 or 4 we'd be adding a bias into our analysis, since that doesn't consider any interaction from x1!
Better methods would seek to learn how to predict the missing data from the other features.
I've added some links for further reading:

https://towardsdatascience.com/a-comprehensive-guide-to-data-imputation-e82eadc22609
https://en.wikipedia.org/wiki/Imputation_(statistics)
http://num.pyro.ai/en/stable/tutorials/bayesian_imputation.html
https://stats.idre.ucla.edu/wp-content/uploads/2016/02/multipleimputation.pdf
https://scikit-learn.org/stable/modules/impute.html#impute

",<python><numpy><data-imputation>
"I am trying to do 10-fold cross-validation on an audio dataset. The audio is clipped into small segments, so we have multiple clips from the same file. To avoid overfitting, each audio is assigned to a particular fold. The file structure is similar to the UrbanSoundDataset. I am generating MFCC features for each fold and saving the features using the following code:
   np.savez(&quot;{0}/{1}_mfcc&quot;.format(save_dir, &quot;fold&quot;+str(fold_id)), features=X, 
             labels=y)

The feature set for each fold is fixed to rows x 40 mfcc x 174 dimensions. For example, the dimension for fold 1 is (534, 40, 174), and the dimension for fold 2 is (538, 40, 174). When I load the fold values, I want to stack the 9 folds together for training. For example,if I stack fold1 and fold2 in the above example, I should have (1072,40,174) length array at the end of the stacking process.
How can I do that using numpy?
","I think what you are looking for is numpy.concatenate.
From the documentation this allows you to:

Join a sequence of arrays along an existing axis.

It has the function signature:
numpy.concatenate((a1, a2, ...), axis=0, out=None, dtype=None, casting=&quot;same_kind&quot;)

See the documentation for more.
",<machine-learning><python><cnn><numpy>
"I am trying to do a Random Forest in a dataset with numerical and categorical variables in order to obtain a categorical result (two possible classes, column name &quot;predicción&quot;). I am using the scikit-learn library in Jupyter notebook.
I have done the train-test split like this:
 X_train, X_test, y_train, y_test = train_test_split(datos.drop(columns = 'predicción'), datos['predicción'],random_state = 123)

then I made two lists with the column names that hold numerical or categorical values:
cat_cols = X_train.select_dtypes(include=['object', 'category']).columns.to_list()
numeric_cols = X_train.select_dtypes(include=['float64', 'int']).columns.to_list()

cat = list(np.array(cat_cols).reshape(1,9))

I then do the encoding using ColumnTransformer:
niveles = ['0', '1', '2']
encoder = ColumnTransformer([('ordinal', OrdinalEncoder(categories=[niveles]), cat)],remainder='passthrough')

So far so good, no errors up to this point. The error rises when I use the fit_transform:
X_train = encoder.fit_transform(X_train)
X_test  = encoder.fit_transform(X_test)

I have not been able to find a solution to this problem or any alternative. I am fairly new to machine learning if that can be an excuse. Any bit of help is welcome!
","You are using the fit_transform method on both the training and test dataset which is incorrect. You should only use fit_transform on the training dataset (since the encoder should only fit/learn from data in the training dataset and not the test dataset as this would be new information when deploying your model) and then use transform to apply the encoder on the test dataset.
",<scikit-learn><dataset><pandas><numpy><encoding>
"I am tryint to trying to train a Bayesian NN and at some point I need to compute log-likelihoods for some data points, according to a multivariate diagonal gaussian distribution with parameters (mu, sigma).
I have 2 problems:

I don't know the size of the values in advance (note that I am guaranteed that 'values', 'mu' and 'rho') are the same size, but they could either be 1D or 2D, which forces me to have an ugly if statement. Ideally I would just iterate over the elements no matter the size of the tensor.

This is painfully slow. I don't see how I could vectorize the logpdf the numpy way, as passing in the values, mu and sigma directly to norm.logpdf seems to implicitely construct a covariance matrix (which is too big and makes the program crash).
 from scipy.stats import norm

 ...

 mu    = self.mu.detach().numpy()
 sigma = np.log(1 + np.exp(self.rho.detach().numpy()))
 vals  = values.detach().numpy()
 log_likelihood_val = 0
 if len(values.size()) == 2:
     for i in range(values.size()[0]):
         for j in range(values.size()[1]):
             log_likelihood_val += norm.logpdf(vals[i,j], loc=mu[i,j], scale=sigma[i,j])
 else:
     for i in range(values.size()[0]):
         log_likelihood_val += norm.logpdf(vals[i], loc=mu[i], scale=sigma[i])
 return torch.tensor(log_likelihood_val)



How  should I implement it instead?
","I just ended up defining my own logpdf function so that it is easily vectorized, which solved both problems at once:
def log_likelihood(self, values: torch.Tensor) -&gt; torch.Tensor:
    def logpdf(x, mu, sigma):
        return -(((x - mu)/sigma)**2)/2 - torch.log(np.sqrt(2*np.pi) * sigma)
        
    sigma = torch.log(1 + torch.exp(self.rho))
    log_likelihood_val = torch.sum(logpdf(values, self.mu, sigma))
    return log_likelihood_val

Hope this might help someone.
",<numpy><scipy>
"I am a newbie in Machine Learning, I trained a binary classifier for bank loan prediction through Logistic Regression.
I measured the accuracy of it with two methods: accuracy score and jaccard index.
Accuracy score returns a value of 0.91 whereas jaccard score returns a value of 33.
Why is jaccard sore showing such a low value.(Ik it is a really stupid question, but it would be really good if you could help me out)
","Jaccard score is a similarity coefficient (i.e., how similar are two sets).
Mathematical formula is:
J(A, B) = |A∩B| / |A∪B|
In your case of binary classification, F-1, Precision, Recall are all better metrics for evaluating model performance (as @Oxbowerce mentioned). An important thing to keep in mind when evaluating model performance for classification problems is the distribution of your data (balanced vs imbalanced). For highly imbalanced datasets, F-1/Precision/Recall scores would most accurately describe how your model is predicting positive classes, as compared to Accuracy Score (read: Accuracy Paradox)
",<classification><scikit-learn><pandas><logistic-regression><numpy>
"I am a newbie in Machine Learning, I trained a binary classifier for bank loan prediction through Logistic Regression.
I measured the accuracy of it with two methods: accuracy score and jaccard index.
Accuracy score returns a value of 0.91 whereas jaccard score returns a value of 33.
Why is jaccard sore showing such a low value.(Ik it is a really stupid question, but it would be really good if you could help me out)
","The Jaccard index or score is often used for bounding boxes or semantic segmentation in machine learning, i.e. in computer vision problems. Your problem is a classification problem using tabular data, and therefore this metric is not really applicable for this type of problem. Accuracy (and maybe even more so precision and recall) are more valuable metrics to assess your model's accuracy.
",<classification><scikit-learn><pandas><logistic-regression><numpy>
"https://towardsdatascience.com/speed-testing-pandas-vs-numpy-ffbf80070ee7
(You can open the link in incognito if its locked).
Numpy arrays are faster than DataFrame on normal mathematical operations.
Should I use np arrays to train my algorithm? Or go for DataFrame?
I understand DataFrame makes it easier to 'look' at the data.
But will np array help in training?
","For TensorFlow, you need numpy arrays, or tensors as input. Here is the documentation for it and there are bunch of options when it comes to arguments for the fit method and it has to be an array, tensor at the most basic level or some generator that returns an object of a similar type.
",<python><pandas><optimization><numpy><dataframe>
"I am trying an Retail ML project, but am stuck on the Error &quot;Found input variables with inconsistent numbers of samples: [982644, 911]&quot;. I tired many thing &amp; I know why this error occurs, but I can't figure out a solution for it. Can anybody please help me. I've been stuck on it for past 2 days.
Y_train = train1['Sales']
Y_val = test_val1['Sales']

X_train = train1.drop(['Sales', 'Date', 'Customers'], axis = 1).values
X_val = test_val1.drop(['Sales', 'Date', 'Customers'], axis = 1).values
X_train = X_train.reshape(X_train.shape[0:])

rr = Ridge(alpha=10)
rr.fit(X_train, Y_train)
Y_pred1 = rr.predict(X_val)

print('MSE',np.sqrt(mean_squared_error(Y_pred1,Y_val)))
print('MAE',mean_absolute_error(Y_pred1,Y_val))
print('train model score',rr.score(X_train, Y_train))
print('test model score',rr.score(X_val,Y_val))

I am getting the error on rr.fit(X_train, Y_train). I have performed Linear Regression with the same object variables, but cant seem to perform the Regularization of the Model.
","If you are just dropping the columns like X_train = train1.drop(['Sales', 'Date', 'Customers'], axis = 1) will give a dataframe, when you use .values at the end then the result type will be numpy array. So please remove the .values also the reshape line won't be required. Thanks
",<machine-learning><python><numpy><jupyter>
"I am trying an Retail ML project, but am stuck on the Error &quot;Found input variables with inconsistent numbers of samples: [982644, 911]&quot;. I tired many thing &amp; I know why this error occurs, but I can't figure out a solution for it. Can anybody please help me. I've been stuck on it for past 2 days.
Y_train = train1['Sales']
Y_val = test_val1['Sales']

X_train = train1.drop(['Sales', 'Date', 'Customers'], axis = 1).values
X_val = test_val1.drop(['Sales', 'Date', 'Customers'], axis = 1).values
X_train = X_train.reshape(X_train.shape[0:])

rr = Ridge(alpha=10)
rr.fit(X_train, Y_train)
Y_pred1 = rr.predict(X_val)

print('MSE',np.sqrt(mean_squared_error(Y_pred1,Y_val)))
print('MAE',mean_absolute_error(Y_pred1,Y_val))
print('train model score',rr.score(X_train, Y_train))
print('test model score',rr.score(X_val,Y_val))

I am getting the error on rr.fit(X_train, Y_train). I have performed Linear Regression with the same object variables, but cant seem to perform the Regularization of the Model.
","I have checked the colab example. There seems to be not enough memory to train a Ridge model with your humongous dataset whose shape is (982644, 1169). The notebook crashes when attempting to execute the said line,
rr.fit(X_train, Y_train)

So I tried decreasing the size of the dataset, and everything worked fine.
xt = X_train.loc[:200000].copy()
yt = Y_train.loc[xt.index].copy()

xv = X_val.copy()
yv = Y_val.copy()

print(xt.shape, yt.shape)
print(xv.shape, yv.shape)

rr = Ridge(alpha=10)
rr.fit(xt, yt)

Y_pred1 = rr.predict(xv)

print('MSE:',np.sqrt(mean_squared_error(Y_pred1,yv)))
print('MAE:',mean_absolute_error(Y_pred1,yv))
print('train model score:',rr.score(xt, yt))
print('test model score:',rr.score(xv,yv))

Output:
(200001, 1169) (200001,)
(34565, 1169) (34565,)
MSE: 1431.9094471008812
MAE: 1058.263279968715
train model score: 0.8604548907625048
test model score: 0.8423455437913853

NB: be sure to load all the datasets properly, when dataset files in the mounted drive are corrupt you may get unexpected errors when trying to execute the code.
",<machine-learning><python><numpy><jupyter>
"We have several &quot;in_arrays&quot; like
in_1=np.array([0.4,0.7,0.8,0.3])
in_2=np.array([0.9,0.8,0.6,0.4])

I need to create two outputs like
out_1=np.array([0,0,1,0])
out_2=np.array([1,1,0,0])

So, the given element of the output array is 1 if the value in the corresponding input array is greater than 0.5 AND the value in this position of this input array is greater than the values of other arrays in this position. What is the right way to do this?
Update. This is clearly a data science question. This is needed to run voting between several predicted segmentation masks by several different models to assign the most likely type of object on the picture.
","Try this,
def max_is_greater_than_half(*args):

    df      = pd.DataFrame(dict({'col_'+str(i+1): val for i, val in enumerate(args)}))
    max_val = df.apply(max, axis=1)
    df = df.apply(lambda x: (x &gt; 0.5) &amp; (max_val == x), axis=0).astype(int)

    return [np.array(df[col].values) for col in df.columns]


out1, out2 = max_is_greater_than_half(in_1, in_2)

The code allows adding as many 'in' arrays as you want, but do not forget assigning the result to the same number of output arrays.
",<numpy>
"I am new at this and am pretty sure this is a stupid question, but here it goes:
Where can I see the results of a model's prediction?
I did this course about deep learning, followed the tutorial, ran all the code, and trained this artificial neural network to perform customer churn prediction.
In the &quot;Predicting the Test set results&quot;. It prints the y_pred as below:
y_pred = ann.predict(X_test)
y_pred = (y_pred &gt; 0.5)
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

Output:
[[0 0]
 [1 1]
 [1 1]
 ...
 [0 0]
 [1 1]
 [1 0]]

I thought it was the y_pred data that would show the prediction for each customer, but it only has 756 results and the dataset has more than 3000 customers.
Where can I see the prediction for each customer in the dataset?
Here is the whole code, in case, you guys need to check it:
import numpy as np
import pandas as pd
import tensorflow as tf

tf.__version__


dataset = pd.read_csv('Churn_Modelling2.csv', sep=';')
X = dataset.iloc[:, 3:-1].values
y = dataset.iloc[:, -1].values
print(X)
print(y)

print(X)

print(y)



from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)



### Initializing the ANN
&quot;&quot;&quot;

ann = tf.keras.models.Sequential()

&quot;&quot;&quot;### Adding the input layer and the first hidden layer&quot;&quot;&quot;

ann.add(tf.keras.layers.Dense(units=6, activation='relu'))

&quot;&quot;&quot;### Adding the second hidden layer&quot;&quot;&quot;

ann.add(tf.keras.layers.Dense(units=6, activation='relu'))

&quot;&quot;&quot;### Adding the output layer&quot;&quot;&quot;

ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))

&quot;&quot;&quot;## Part 3 - Training the ANN

### Compiling the ANN
&quot;&quot;&quot;

ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

&quot;&quot;&quot;### Training the ANN on the Training set&quot;&quot;&quot;

ann.fit(X_train, y_train, batch_size = 32, epochs = 100)

&quot;&quot;&quot;## Part 4 - Making the predictions and evaluating the model

### Predicting the Test set results
&quot;&quot;&quot;

y_pred = ann.predict(X_test)
y_pred = (y_pred &gt; 0.5)
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

pred=np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1)
np.savetxt(&quot;predicitons.csv&quot;, pred, delimiter=&quot;,&quot;)

&quot;&quot;&quot;### Making the Confusion Matrix&quot;&quot;&quot;

from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

Thank you in advance.
","The y_pred vector should hold all predictions for your observations present in the X_test dataset, which should be 756 observations. If you want to use your model on the whole dataset you can simply use the .predict() method on your X_train dataset:
# predict on your training dataset
ann.predict(X_train)
# predict on your test dataset
ann.predict(X_test)

",<scikit-learn><pandas><predictive-modeling><numpy>
"I have numpy array as follows:
train_x = [[1,2,3,0,0], [2,5,0,0,0], [2,3,0,0,0], [0,0,0,0,0], [0,0,0,0,0,0]]

Now, I would like to transform it to as shown below:
new_train_x = [[0,0,0,0,0],[0,0,0,0,0,0],[0,0,1,2,3],[0,0,0,2,5],[0,0,0,2,3]]

I tried writing manually. The length of such list is huge. But it is time consuming.
I would like to know the efficient and short code for this (manually takes time).
","So assuming that the padding is the same length and the shape of the array is consistent you can do something below:
        import numpy as np
        train_x = np.array([[1,2,3,0,0], [2,5,0,0,0], [2,3,0,0,0], [0,0,0,0,0], [0,0,0,0,0,0]])
        stringX = ''
        for i in train_x:
            for s in i:
                stringX += str(s)
            subStrX = stringX[0:12]
        prePadStr ='00000000000000' + subStrX
        
        counter = 0
        internalCounter = 0
        newStr = ''
        newLst = np.empty(shape=([5,5]),dtype=str)
        while counter &lt; 25: 
            newStr = prePadStr[counter:counter+5]
            newStrLst = [char for char in newStr]
            newLst[internalCounter] = newStrLst
            counter+=5
            internalCounter+=1
        newLst

If you need something that should be able to infer the padding and shape, I can provide that as well. However when it comes to efficiency, that will likely isn't the case that this is the most efficient, but that may not matter depending on what you have to process.
",<numpy>
"I have an image I loaded with the image.load_img() function but when I try to reshape it I get this error:
ValueError: cannot reshape array of size 12288 into shape (64,64)

Here is my code:
test_image = image.load_img('xray_dataset_covid19/test/PNEUMONIA/streptococcus-pneumoniae-pneumonia-temporal-evolution-1-day2.jpg'
                            , target_size = (64, 64))
test_image = image.img_to_array(test_image)
test_image = np.expand_dims(test_image, axis = 0)
result = model.predict(test_image)
if result[0][0] == 1:
    img = Image.fromarray(test_image.reshape(64,64) ,'L')
    img.show() 

","$64\times 64 = 4096$. You're short about $8000$ pixels.
",<numpy><image-preprocessing>
"I have an image I loaded with the image.load_img() function but when I try to reshape it I get this error:
ValueError: cannot reshape array of size 12288 into shape (64,64)

Here is my code:
test_image = image.load_img('xray_dataset_covid19/test/PNEUMONIA/streptococcus-pneumoniae-pneumonia-temporal-evolution-1-day2.jpg'
                            , target_size = (64, 64))
test_image = image.img_to_array(test_image)
test_image = np.expand_dims(test_image, axis = 0)
result = model.predict(test_image)
if result[0][0] == 1:
    img = Image.fromarray(test_image.reshape(64,64) ,'L')
    img.show() 

","
when I print(test_image.shape) I get (1, 64, 64, 3)

What you probably wanted was:
if result[0][0] == 1:
    img = Image.fromarray(test_image.reshape(64,64,3))
    img.show()

I.e. specify the ,3, because you have RGB data, and drop the ,'L' because that means you have B/W data.
If you actually wanted greyscale or b/w change the last line to one of:
img.convert('L').show()
img.convert('1').show()

Stepping back a bit, you could have used test_image directly, and not needed to reshape it, except it was in a batch of size 1. A better way to deal with it, and not have to explicitly state the image dimensions, is:
if result[0][0] == 1:
    img = Image.fromarray(test_image.squeeze(0))
    img.show()

squeeze() removes any dimensions of size 1; squeeze(0) avoids surprises by being more specific: if the first dimension is of size 1 remove it, otherwise do nothing.
Yet another way to do it, that ties in with how you use result, is:
if result[0][0] == 1:
    img = Image.fromarray(test_image[0])
    img.show()

",<numpy><image-preprocessing>
"I was able to convert the 9.2e18 AD to a date, but I am confused about the exact date. Which date is 9.2e18 AD and and 9.2e18 BC?

Time span (absolute) - [9.2e18 BC, 9.2e18 AD] i.e +/- 9.2e18 years

NumPy documentation, section &quot;Datetime Units&quot; under &quot;Datetimes and Timedeltas&quot;




Code
Meaning
Time span (relative)
Time span (absolute)




Y
year
+/- 9.2e18 years
[9.2e18 BC, 9.2e18 AD]


M
month
+/- 7.6e17 years
[7.6e17 BC, 7.6e17 AD]


W
week
+/- 1.7e17 years
[1.7e17 BC, 1.7e17 AD]


D
day
+/- 2.5e16 years
[2.5e16 BC, 2.5e16 AD]





I have converted the 9.2e18 (epoch - I believe it represented in epochs) to a date. It gave me very big date which I did not expect. Are my assumptions accurate?
How many years was covered according to the date 1970-01-01 from 9.2e18 BC?
What are some examples using this timespan to judge my assumptions to get the date of 9.2e18 BC and 9.2e18 AD with units by NumPy?

","From the documentation you referred: &quot;The length of the span is the range of a 64-bit integer times the length of the date or unit.&quot;
64 bit integer has values from -2^63 to 2^63-1, which is the same as from -9.2e18 to 9.2e18. So, the time span column shows you which dates would you cover if use only the corresponding units. Note, i.e. that time span for years 12 times bigger then time span for months and 52 times bigger then timespan for weeks.
So, the date 9.2e18BC is literally 9.2 quintillions years before christ
UPD with clarification to comment
First of all, there are two different concepts - the date (like 10th august of 2021) and time duration (like two years). The later is referred as time delta in python. And you can't add/subtract years in numpy from date because different years have different amount of time -- like 365 or 366 days. However you can subtract basically any amount of days like that:
start_date = np.datetime64('0000-01-01')
days_to_substract = np.timedelta64(100, 'D')
print(start_date - days_to_substract) # initial date minus 100 days
&gt;&gt;&gt; -001-09-23

Note, that you can in fact manipulate with dates in vanilla python with datetime, but as mentioned in the other answer, the dates can not be less then 01-01-0001 for basic python without numpy
",<time-series><numpy><mathematics><epochs><time>
"I was able to convert the 9.2e18 AD to a date, but I am confused about the exact date. Which date is 9.2e18 AD and and 9.2e18 BC?

Time span (absolute) - [9.2e18 BC, 9.2e18 AD] i.e +/- 9.2e18 years

NumPy documentation, section &quot;Datetime Units&quot; under &quot;Datetimes and Timedeltas&quot;




Code
Meaning
Time span (relative)
Time span (absolute)




Y
year
+/- 9.2e18 years
[9.2e18 BC, 9.2e18 AD]


M
month
+/- 7.6e17 years
[7.6e17 BC, 7.6e17 AD]


W
week
+/- 1.7e17 years
[1.7e17 BC, 1.7e17 AD]


D
day
+/- 2.5e16 years
[2.5e16 BC, 2.5e16 AD]





I have converted the 9.2e18 (epoch - I believe it represented in epochs) to a date. It gave me very big date which I did not expect. Are my assumptions accurate?
How many years was covered according to the date 1970-01-01 from 9.2e18 BC?
What are some examples using this timespan to judge my assumptions to get the date of 9.2e18 BC and 9.2e18 AD with units by NumPy?

","1 (Y)ear = 3.154e+16 nanoseconds.
In scientific notation, &quot;E&quot; refers to a power of 10. | So (9.2E+18) is written as &quot;9.2 × 10^18&quot; in scientific notation. The decimal value of (9.2E+18) would equal 9200000000000000000.
In reference to (Y)ears, what does the value (9.2E+18) mean?

9.2 x 10^18 Milliseconds = 291,536,394 Years.
9.2 x 10^18 Nanoseconds = 291.53639 Years.

The DateTime value type represents dates and times with values ranging from 00:00:00 (midnight), January 1, 0001 Anno Domini (Common Era) through 11:59:59 P.M., December 31, 9999 A.D. (C.E.) in the Gregorian calendar.
",<time-series><numpy><mathematics><epochs><time>
"I'm stacked with executing the sub-word tokenization preprocessing to use transformer.
According to the tutorial on the article, I have executed the sample code.
However, one function was not defined properly and no hint to fix it on the article.
If you have any ideas to fix the code, could you help me?
Error
     22  return X, y
     23 
---&gt; 24 X_train, y_train = preprocess(train_samples)
     25 X_val, y_val = preprocess(val_samples)

     12 def preprocess(samples):
     13  tag_index = {tag: i for i, tag in enumerate(schema)}
---&gt; 14  tokenized_samples = list(tqdm(map(tokenize_sample, samples)))
     15  max_len = max(map(len, tokenized_samples))
     16  X = np.zeros((len(samples), max_len), dtype=np.int32)

TypeError: 'module' object is not callable

Code
This code is from the article to build a model for named entity recognition using transformer.
import numpy as np
import tqdm
 
def tokenize_sample(sample):
  seq = [
         (subtoken, tag)
         for token, tag in sample
         for subtoken in tokenizer(token)['input_ids'][1:-1]
         ]
  return [(3, 'O')] + seq + [(4, 'O')]

def preprocess(samples):
  tag_index = {tag: i for i, tag in enumerate(schema)}
  tokenized_samples = list(tqdm(map(tokenize_sample, samples)))
  max_len = max(map(len, tokenized_samples))
  X = np.zeros((len(samples), max_len), dtype=np.int32)
  y = np.zeros((len(samples), max_len), dtype=np.int32)
  for i, sentence in enumerate(tokenized_samples):
    for j, (subtoken_id, tag) in enumerate(sentence):
      X[i, j] = subtoken_id
      y[i,j] = tag_index[tag]
  return X, y

X_train, y_train = preprocess(train_samples)
X_val, y_val = preprocess(val_samples)

What I tried
I checked that the function, tokenize_sample is executable with the below code.
However, I'm not sure how to insert it to the original code.
for sample in samples:
  print(tokenize_sample(sample))

","This look like a tqdm problem. Both the module tqdm and the main function tqdm have the same name. This often create some problem as people will just:
import tqdm

When the right import is:
from tqdm import tqdm

See here: https://stackoverflow.com/questions/39323182/tqdm-module-object-is-not-callable
(The first step might be to just remove tqdm from the line you want to execute to test if this is the origin of the problem)
",<python><nlp><numpy><transformer><python-3.x>
"My code is as follows:
import joblib as jl

_data = pd.read_csv('ifile.csv')
contamination = input(&quot;:&quot;)
labelEncoder(_data)
model = IsolationForest(contamination=float(contamination), n_estimators=1000, verbose=1)
model.fit(_data)
jl.dump(model, 'file.joblib')

this trains the model and dumps it to joblib file. After that i use the joblib to test data further as follows:
_data = pd.read_csv(ifile.csv)
model = jl.load('file.joblib')
predictions = model.predict(_data)
_data[anomaly] = pd.Series(model.predict(_data))
predictions = np.where(predictions == 1, 0, 1) #Mapping 1-&gt;0 and -1-&gt;1
acc = accuracy_score(_data, predictions)

This however return the following error:
raise ValueError(&quot;Classification metrics can't handle a mix of {0}
ValueError: Classification metrics can't handle a mix of continuous-multioutput and binary targets

Can someone tell me what am i doing wrong? I have tried converting both &quot;_data&quot; and &quot;predictions&quot; to numpy arrays, but it still returns the same error.
Thank you
","
sklearn.metrics.accuracy_score(y_true, y_pred....)

accuracy_score needs y_test not the x_test . You have passed x_test
",<pandas><accuracy><numpy><python-3.x><isolation-forest>
"I am aiming to guage the difference in my model performance from using data with and without Sesonality removal. My approach to Seasonality removal is taking the log of the column data and then performing 1-lag Differencing. This results in the first value of the column being NaN, which is then removed after all columns are processed. Also, it may be important to note both datasets are standardized after using sklearn.preprocessing.StandardScaler()
Edit 1: I have found a SO post with a very similar issue, however there are no responses. This is the post here
def remove_seasonality(col, drop=False):
    # check if col is stationary, if stationary skip col.
    if stationary_test(col, test=True) is True:
        return col
    
    # if column not number, skip col
    if not np.issubdtype(col.dtype, np.number): 
        return col

    if (col &lt; 0).any(): # check if contains neg values
        # add constant, make values positive 
        col = col + col.min()
    
    log_values = np.log(col + 1) # add 1 to avoid log(0)
    
    diff = np.diff(log_values, prepend=np.nan)
    
    if drop: 
        diff = diff.dropna()
    
    return diff

I am using a Tensorflow model composed with Convolutional Layers and GRU's, I am using MAE to evaluate my model performance. The problem is that the log-transform and differencing results in a much smaller magnitude of numbers, so I cannot compare the difference in datasets effect on the MAE.
I'm not sure how to rescale my predictions to be able to compare them, I don't think I can perform inverse differencing on my predictions because that operation requires the first value before Differencing - which doesn't exist.
Alternatively, Is there an appropriate metric I could use to evaluate the difference in data preprocessing without performing any rescaling?
","To find the first value to invert the differencing process, I have used the last data point from the preceding dataset, e.g for my Validation predictions, I used the last value from the Training set. This last value is plugged into this function:
def invert_seasonality(seasonality_vals, last_cum_val):
    invert_vals = np.insert(seasonality_vals, 0, np.log(last_cum_val + 1)).cumsum()
    invert_vals = np.exp(invert_vals) - 1
    
    return invert_vals    

",<tensorflow><time-series><numpy><metric>
"I am a bit new to the field of data science and could really use some help.
I used a natural language dictionary to train and test an ml model using keras and tensorflow.
It detects the sentiments in a string and returns 0 or 1.
Now, I have a dataset containing about 200,000 rows with each row containing 1 or 2 paragraphs of text and I wrote a quick function that checks and returns the sentimental polarity of each row and I am currently using a single for loop to parse through the dataset, check the string from each row and append the polarity into an empty column. It works perfectly when I tried it on a smaller subset of 100 rows.
The above process is extremely slow and on my current setup and 200,000 rows will take maybe more than a week to process. I am using sagemaker notebooks with a c5.xlarge instance currently and cant afford to get better compute hardware.
Do you guys have any advice on how to deal with this? Any help will be much appreciated!
def predictions(texter):
  texter = tokenizer.texts_to_sequences(texter)
  texter = pad_sequences(texter, maxlen = 96, dtype = 'int32', value = 0)
  sentiment = model.predict(texter ,batch_size = 1, verbose = 0)[0]
  sentiment = [float(a)/sum(sentiment) for a in sentiment]
  if ((sentiment[0]*100)&gt;75):
    return '0'
  else:
    return '1'

for i in range (len(df['body'])):
  df['bodysentiment'][i] = predictions(df['body'][i])
```

","Here are a few suggestions:

Detect the lines of code that last longer than the other with the Python's time function.
Generally speaking, PC processors have 4 cores and you can benefit from each of them if you apply multiprocessing with pandas thanks to the multiprocessing library.
Pypy is a very good library to run code very fast, even faster than Cython (https://www.pypy.org/)
.iterrows() or .loc could perform better (for loops are too slow).
Google collab uses plenty of powerful servers that could process lot of data in minutes instead of days in many cases.

",<keras><tensorflow><pandas><predictive-modeling><numpy>
"The Problem
I have a pandas dataframe that contains series of people, the week number that a visit occurred, and their systolic and diastolic blood pressures.
ID   Weeks   Systolic     Diastolic
1    9       140          90
1    15      155          97
2    7       140          90
2    8       121          75 
2    9       161          93
3    2       160          92
3    20      139          87
3    21      140          95
3    22      145          96
4    5       155          90
4    3       150          97

What I want to do is group each patient by ID, mark when someone's blood pressure went above 140/90, and find out when a patient's blood pressure went above that value for a second time.
So for example, in the table above patient 3 has their blood pressure go above 140/90 at weeks 2, 21, and 22, so the second instance would be at week 21. The resulting dataframe would look like this then:
ID     Week of Second Spike
1      15
2      9
3      21
4      5

What I've tried
I can make an indicator variable that shows where in the dataframe the blood pressure is above those values:
df['High'] = np.where((df['Systolic'] &gt;= 140) &amp; (df['Diastolic'] &gt;= 90) , 1, 0)

But after that point I'm unsure of how to indicate which week is the second week of high blood pressure for each patient. I know I can also perform a groupby to group IDs together, but I'm stuck after that point.
","I found one solution thanks to this SO thread. What I ended up doing was the following:
First, I made the indicator variable described in the question:
df['High'] = np.where((df['Systolic'] &gt;= 140) &amp; (df['Diastolic'] &gt;= 90) , 1, 0)

Then I made a cumulative sum to count how many times this person has had a blood pressure spike by a specific week.
df['Prev_highBP'] = df.groupby(['ID'])['High'].cumsum().astype(int)

Then I filtered for people with high blood pressure, grouped by ID, then made a new variable to hold the second smallest week within that group.
grouped = df.loc[testPre['High']==1].groupby(['ID'])['Week']
df['second_spike'] = grouped.transform(lambda x: x.nsmallest(2).max())

Finally I fixed the cases where there was only one blood pressure spike using the cumulative sum variable created earlier.
df['second_spike'] = np.where((df['Prev_highBP'] != 2) , np.NaN, df['second_spike'])

I can get the dataframe I wanted then by just dropping the duplicates:
secondSpike = df.drop_duplicates(subset=['ID'], keep='last')

There's likely a cleaner or more efficient way to do this, but it seems to work for the moment.
",<python><pandas><numpy>
"I am an experienced programmer, but new to Python and data science. I am following Aurelien Gerone's book and I don't understand one thing.
I create SGDClassifier and calculate its precision_recall_curve(). Then I am trying to find the lowest threshold to satisfy precision equal to 90%:
precisions, recalls, thresholds = precision_recall_curve(y_train, y_scores)
threshold_90_precision = thresholds[np.argmax(precisions &gt;= 0.90)]

Why on earth I am searching for argmax if I need to find the minimum threshold value? If I try to use argmin I get the wrong value, with precision equal to 0.1.
As I understand this:

precisions &gt;= 0.90 creates an array with precision scores only above or equal to 0.90,
argmax returns an index, at which I find the highest value in the given array (so this should be as far from 90% as possible, but it's not!),
then I choose a threshold with returned index.

What am I missing?
","Okay, I solved this myself.
precisions &gt;= 0.90 doesn't create an array with precision scores only above 90%, but it transforms this array to the array of Booleans, where precisions below 90% are turned to False and the others are True.
argmax, if there are multiple identical, maximum values (and True is max here) returns the first index of this occurrence.
I sometimes hate this book, why he just doesn't use method like &quot;array.first_equals(True)&quot;?
",<python><classification><scikit-learn><numpy>
"I am trying to create a predictive model using linear regression with a dataset that has 157,673 entries.
The data (in a csv file) is in such format:
Timestamp,Signal_1,Signal_2,Signal_3,Signal_4,Signal_5
2021-04-13 11:03:13+02:00,3,3,3,12,12

My current code:
    filename = 'test.csv'
    df = pd.read_csv(filename , parse_dates=['Timestamp'], header=0)
    df['Timestamp'] = pd.to_numeric(pd.to_datetime(df['Timestamp']))
    u, v, w, x, y, z = df.values.T
    
    X = np.asarray([v, w, x, y, z])
    Y = np.asarray([u, u, u, u, u])
    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, shuffle= True)
    
    lineReg = LinearRegression()
    lineReg.fit(X_train, y_train)
    print('Score: ', lineReg.score(X_test, y_test))
    print('Weights: ', lineReg.coef_)

When printing out the shape of both X and Y it is (5, 157673) (after putting u 4 more times in the Y array, since it would otherwise give the error ValueError: Found input variables with inconsistent numbers of samples: [5, 1]).
However, now I am running into the error MemoryError: Unable to allocate 185. GiB for an array with shape (157673, 157673) and data type float64.
Why is that? There must be a mistake somewhere and if not, why is it suddenly in the shape of (157673, 157673) instead of (6, 157673) ?
","You are using scikit-learn's LinearRegression which optimizes with OLS (ordinary least squares). OLS requires a lot of memory.
You should switch to scikit-learn's SGDRegressor which uses stochastic gradient descent (SGD) to optimize. SGD uses far less memory than OLS.
",<machine-learning><python><scikit-learn><pandas><numpy>
"I am trying to create a predictive model using linear regression with a dataset that has 157,673 entries.
The data (in a csv file) is in such format:
Timestamp,Signal_1,Signal_2,Signal_3,Signal_4,Signal_5
2021-04-13 11:03:13+02:00,3,3,3,12,12

My current code:
    filename = 'test.csv'
    df = pd.read_csv(filename , parse_dates=['Timestamp'], header=0)
    df['Timestamp'] = pd.to_numeric(pd.to_datetime(df['Timestamp']))
    u, v, w, x, y, z = df.values.T
    
    X = np.asarray([v, w, x, y, z])
    Y = np.asarray([u, u, u, u, u])
    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, shuffle= True)
    
    lineReg = LinearRegression()
    lineReg.fit(X_train, y_train)
    print('Score: ', lineReg.score(X_test, y_test))
    print('Weights: ', lineReg.coef_)

When printing out the shape of both X and Y it is (5, 157673) (after putting u 4 more times in the Y array, since it would otherwise give the error ValueError: Found input variables with inconsistent numbers of samples: [5, 1]).
However, now I am running into the error MemoryError: Unable to allocate 185. GiB for an array with shape (157673, 157673) and data type float64.
Why is that? There must be a mistake somewhere and if not, why is it suddenly in the shape of (157673, 157673) instead of (6, 157673) ?
","quick fix would be to change the data format - I can't see how your data looks like so my suggestion stay theoretical without example

float64 is the most expensive one. using float32 or float16 - depending how much precision point you need. If they are just integer depending on the range you can use even int8 (if less than 255) - use dtypes parameters in the pd.read_csv function. or convert them later on using astype.

Timestamp - decides what precision of time is needed - if you are fine with days - don't store hours, minutes and others

alternatively usse DataTable https://github.com/h2oai/datatable - Pandas is not the most efficient library when it comes to work with big data. however, I think the first 2 points will fix your issue. Because (157673, 157673) is not too big but 185. GiB sounds too much for it !


",<machine-learning><python><scikit-learn><pandas><numpy>
"I am following the excellent series on SVD by Steve Brunton from the University of Washington, on YouTube, but I have trouble interpreting his 4th video on the subject.
If I understand correctly, he mentions that one can compute the economy SVD decomposition $X = \hat{U}\hat{\Sigma}V^T$ with the following :
$$X^TX= V\hat{\Sigma}\hat{U}^T\hat{U}\hat{\Sigma}V^T = V\hat{\Sigma}^2V^T \implies X^TXV = V\hat{\Sigma}^2 $$
$$XX^T= \hat{U}\hat{\Sigma}\hat{V}^T\hat{V}\hat{\Sigma}\hat{U}^T = \hat{U}\hat{\Sigma}^2\hat{U}^T \implies XX^T\hat{U} = \hat{U}\hat{\Sigma}^2 $$
Instead of using U,S,VT = svd(X) in Python, I want to try decomposing the image in its SVD, and reconstructing the original using only $r$ values with this technique. I am trying to apply this on a picture of The Starry Night, loaded in a grayscale numpy array X, which I do this way :
r = 10    
XT = X.transpose()
C_1 = XT @ X
C_2 = X @ XT
[Lambda_1, V_hat] = np.linalg.eig(C_1)
[Lambda_2, U_hat] = np.linalg.eig(C_2)
V_hat_T = V_hat.transpose()
X_tilde = U_hat[:,:r] @ np.diag(Lambda_1)[0:r,:r] @ V_hat_T[:r,:]

But I can't reproduce the image, whatever the value I attribute to $r$, whereas the product of U @ S @ VT , computed with the svd(X) function, perfectly reconstructs the original picture.
What am I doing wrong? It is certainly superfluous to mention that I am a beginner, and I probably made some big mistake.
","The problem is occurring for a few reasons:

You must ensure the correct ordering of eigenvalues/eigenvectors.
You should be taking the square root of the eigenvalues (Lambda_1) to get the singular values.
You need to take special care to ensure you are using the right signs for the eigenvectors as these won't necessarily be correct by default. This is because we assume that the singular values are the positive roots of the eigenvalues, but the two sets of eigenvectors we retrieve don't necessarily respect this assumption when joined in the singular value decomposition.

Here is the working example:
r = 10    
XT = X.transpose()
C_1 = XT @ X
C_2 = X @ XT
[Lambda_1, V_hat] = np.linalg.eig(C_1)
[Lambda_2, U_hat] = np.linalg.eig(C_2)

# The eigenvalues/eigenvectors returned from np.linalg.eig are not sorted so may differ between Lambda_1 and Lambda_2
# When performing economy SVD we also want to ensure the largest eigenvalues come first
i_1 = np.argsort(Lambda_1)[::-1]
i_2 = np.argsort(Lambda_2)[::-1]
V_hat = V_hat[:, i_1]
U_hat = U_hat[:, i_2]

# We must take the square root of eigenvalues to get the singular values
Lambda = np.sqrt(np.sort(Lambda_1)[::-1])

# X @ V_hat and U_hat @ np.diag(Lambda) should be equal. But the eigenvectors retrieved may not have the correct signs 
# This code checks if  X @ V_hat and U_hat @ np.diag(Lambda) have different signs and then updates V_hat wherever the sign is incorrect
same_sign = np.sign((X @ V_hat)[0] * (U_hat @ np.diag(Lambda))[0])
V_hat = V_hat * same_sign.reshape(1, -1)

V_hat_T = V_hat.transpose()
X_tilde = U_hat[:,:r] @ np.diag(Lambda)[0:r,:r] @ V_hat_T[:r,:]

",<python><numpy><pca><image>
"I want to implement an efficient and vectorized Maxout activation function using python numpy.
Here is the paper in which &quot;Maxout Network&quot; was introduced (by Goodfellow et al).
For example, if k = 2:
def maxout(x, W1, b1, W2, b2):
    return np.maximum(np.dot(W1.T,x) + b1, np.dot(W2.T, x) + b2)

Where x is a N*D matrix.
Suppose k is an arbitrary value(say 5). Is it possible to avoid for loops when calculating each wx + b? I couldn't come up with any vectorized solutions.
","If you could combine all the weight vectors into a matrix W and all b's into a vector b, then you could do
np.maximum(np.dot(W.T,x)  + b)

",<machine-learning><python><deep-learning><numpy><activation-function>
"The df is as follows:
&gt;&gt;&gt; df['csl']
 0       250/500
 1      500/1000
 2      500/1000
 3      500/1000
 4       100/300
695    500/1000
696     250/500
697     100/300
698     250/500
699     250/500
Name: csl, Length: 700, dtype: object

by the dtype is in the form of array objects:
df.unique()
array(['250/500', '500/1000', '100/300'], dtype=object)

The following code gives me an error:
df['csl'] = df['csl'].astype(float)
ValueError: could not convert string to float: '250/500'

","The easiest way of achieving this would probably to split the string column using the fraction character and then dividing the first value by the second value:
import pandas as pd

df = pd.DataFrame({&quot;col&quot;: [&quot;250/500&quot;, &quot;100/300&quot;, &quot;500/1000&quot;]})
df[&quot;result&quot;] = df[&quot;col&quot;].str.split(&quot;/&quot;).apply(lambda x: float(x[0]) / float(x[1]))

#      col    result
#  250/500  0.500000
#  100/300  0.333333
# 500/1000  0.500000

If you have a very large dataframe it is faster to save the intermediate result and then perform the division to make use of vectorization:
df[[&quot;numerator&quot;, &quot;denominator&quot;]] = df[&quot;col&quot;].str.split(&quot;/&quot;, expand=True)
df[&quot;result&quot;] = df[&quot;numerator&quot;].astype(float) / df[&quot;denominator&quot;].astype(float)

#      col numerator denominator    result
#  250/500       250         500  0.500000
#  100/300       100         300  0.333333
# 500/1000       500        1000  0.500000

",<python><pandas><data-science-model><numpy>
"I am having a pandas dataframe divided to X_train, X_test, y_train, y_test by train_test_split and I am using it to train my neural network model for binary classification. It is taking some time and I was wondering if it would be faster if I changed it from pandas.dataframe to numpy.array or tensor.
","It's hard to say for sure without knowing more about the size of your dataset, the architecture of your model, and the libraries you are using.
Changing from pandas.Dataframe to numpy.array is very unlikely to make your training faster.  Pandas dataframes are already backed by numpy arrays.
Depending on what neural network library you are using and what hardware you have available, changing from Dataframes to tensors may help.  Libraries like Tensorflow and PyTorch allow you to move computation to a GPU/TPU, which may speed up training.
The benefit of using tensor data structures from DL libraries is that they can be moved to a GPU/TPU.  They aren't inherently faster.  So if you don't have a GPU, or if you are using scikit-learn to train the model, then there's no point switching from pandas to tensors
",<neural-network><keras><numpy>
"I have a list of tensors created by
[some_function(x) for x in something]

and this list becomes like this
[&lt;tf.Tensor: shape=(), dtype=string, numpy=b'I have gone and you will go'&gt;, &lt;tf.Tensor: shape=(), dtype=string, numpy=b'we will go'&gt;]

But I want to have an object like this
&lt;tf.Tensor: shape=(3,), dtype=string, numpy=array([b'I have gone and you will go', b'we will go'], dtype=object)&gt;

Which is a tensor of list of strings (Opposite to the previous one). Is there any way to this, assuming that I have access to all these tensors, i.e. some_function(x) for x in something separately.
","In order to stack list of tf tensors you could use the tf function stack hence the name
for ur case,

tf.stack([some_function(x) for x in something],axis=0)



or you could stack them in numpy and then convert the array to a tensor, to do so using numpy np.stack inputting a list and axis
",<python><tensorflow><numpy>
"Let's start by considering one-dimensional data, i.e., $d=1$. In OLS regression, we would learn the function
$$
f(x)=w_{0}+w_{1} x,
$$
where $x$ is the data point and $\mathbf{w}=\left(w_{0}, w_{1}\right)$ is the weight vector. To achieve a polynomial fit of degree $p$, we will modify the previous expression into
$$
f(x)=\sum_{j=0}^{p} w_{j} x^{j}
$$
where $p$ is the degree of the polynomial. We will rewrite this expression using a set of basis functions as
$$
f(x)=\sum_{j=0}^{p} w_{j} \phi_{j}(x)=\mathbf{w}^{\top} \boldsymbol{\phi}
$$
where $\phi_{j}(x)=x^{j}$ and $\phi=\left(\phi_{0}(x), \phi_{1}(x), \ldots, \phi_{p}(x)\right)$. We simply apply this transformation to every data point $x_{i}$ to get a new dataset $\left\{\left(\phi\left(x_{i}\right), y_{i}\right)\right\}$. Then we use linear regression on this dataset, to get the weights $\mathbf{w}$ and the nonlinear predictor $f(x)=\sum_{j=0}^{p} w_{j} \phi_{j}(x),$ which is a polynomial (nonlinear) function in the original observation space. Notes
How does this work? Could anyone give me an example and explain it to me in simple terms? How would I go about implementing this in Numpy?
","It is quite simple to understand (and to implement using matrices).
Consider a specific example (to generalise later). You have a polynomial function of a single feature $x$):
$$ f(x) = \omega_0 x^0 + \omega_1 x^1 + \ldots \omega_n x^n $$
You can organise coefficients and features in vectors and get $f$ by a scalar product:
$$ \mathbf{\omega} = \begin{pmatrix} \omega_0, \\ \vdots \\ \omega_n \end{pmatrix}, \qquad \mathbf{x} =  \begin{pmatrix} 1, \\ x \\ x^2 \\ \vdots \\ x^n \end{pmatrix}$$
Hence $$ f(x) = \omega^T\mathbf{x}$$.
This is nothing else than a multi-feature linear regression where the $i$-th feature is now the $i$-th power of $x$.
In numpy, imagine you have an array of data x.
To create the vector $\mathbf{x}$ above, you can do (for $n=3$, for instance)
X = np.ones((len(x), 4))
X[:,1] = x
X[:,2] = np.power(x,2)
X[:,3] = np.power(x,3)

And then using sklearn LinearRegression,
model = LinearRegression()
model.fit(X, y)

UPDATE: In sklearn has been recently introduced PolynomialFeatures that precisely performs the transformation I described in numpy (you asked in numpy, but this might be useful as well).
",<machine-learning><regression><linear-regression><numpy>
"I have a prediction numpy array. How can I make a .nii or .nii.gz mask file from the array?
","You can this using nibabel:
import nibabel as nb

ni_img = nib.Nifti1Image(numpy_array, affine=np.eye(4))
nib.save(ni_img, &quot;dicom_volume_image.nii&quot;)

",<pytorch><numpy><image-segmentation>
"I've constructed a CNN in Python using Numpy, which is trained with mini batch gradient descent for MNIST digit classification. When training with a batch size of 1, the time needed for 5 epochs is about 1200 s, which is only about 40% slower than with batch sizes 16 to 256. For batch size 4 and 8 it takes about 940 s and 890 s respectively. When i train it over 5 epochs with batch sizes 16, 32 or 256 the time is about 850 s (about the same for all three sizes).
I expected the time to shorten with bigger batch sizes, as seen with batch size 1 vs batch size 16, but why is the time the same for batch sizes 16, 32 and 256? The larger the batch size, the fewer the iterations. I would expect the computational cost of training a larger batch to be minimized with my vectorized code. None of the batch sizes seem to use any disk memory. Why is the time difference so small and why does the time stagnate at about 850 s for 5 epochs?
","You can answer that question for yourself by profiling and benchmarking the code. Python has Standard library modules for profiling.
",<deep-learning><neural-network><cnn><convolutional-neural-network><numpy>
"I am trying to convert files stored in a CSV file dataset to grayscale. I run the code which worked on my previous dataset but I now get this error:
TypeError: Image data of dtype object cannot be converted to float
The code I produced is below:
import pandas as pd 
import numpy as np 
from sklearn.datasets import load_digits
import matplotlib.pyplot as plt

data_path = &quot;pe_section_headers.csv&quot;

data1 = pd.read_csv(data_path);

data = data1

#data = data.drop(&quot;Malware&quot;, axis=1)
#data = data.drop(&quot;Name&quot;, axis=1)
data = data.values
data = data.reshape(data.shape[0], data.shape[1], 1)
data = np.tile(data, (1, data.shape[1]))

for i in range(data.shape[0]):
    plt.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)
    plt.imshow(data[i], cmap=&quot;gray&quot;)
    plt.savefig(f&quot;output_image_{data1.iloc[i,0]}.png&quot;)
    plt.close()

#print(data[0].shape)

And the error message I receive is:
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-8-9f9e3cc13a98&gt; in &lt;module&gt;
     21 for i in range(data.shape[0]):
     22     plt.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)
---&gt; 23     plt.imshow(data[i], cmap=&quot;gray&quot;)
     24     plt.savefig(f&quot;output_image_{data1.iloc[i,0]}.png&quot;)
     25     plt.close()

~/opt/anaconda3/envs/jacksprojectnew/lib/python3.6/site-packages/matplotlib/pyplot.py in imshow(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, filternorm, filterrad, resample, url, data, **kwargs)
   2728         filternorm=filternorm, filterrad=filterrad, resample=resample,
   2729         url=url, **({&quot;data&quot;: data} if data is not None else {}),
-&gt; 2730         **kwargs)
   2731     sci(__ret)
   2732     return __ret

~/opt/anaconda3/envs/jacksprojectnew/lib/python3.6/site-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs)
   1445     def inner(ax, *args, data=None, **kwargs):
   1446         if data is None:
-&gt; 1447             return func(ax, *map(sanitize_sequence, args), **kwargs)
   1448 
   1449         bound = new_sig.bind(ax, *args, **kwargs)

~/opt/anaconda3/envs/jacksprojectnew/lib/python3.6/site-packages/matplotlib/axes/_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, filternorm, filterrad, resample, url, **kwargs)
   5521                               resample=resample, **kwargs)
   5522 
-&gt; 5523         im.set_data(X)
   5524         im.set_alpha(alpha)
   5525         if im.get_clip_path() is None:

~/opt/anaconda3/envs/jacksprojectnew/lib/python3.6/site-packages/matplotlib/image.py in set_data(self, A)
    701                 not np.can_cast(self._A.dtype, float, &quot;same_kind&quot;)):
    702             raise TypeError(&quot;Image data of dtype {} cannot be converted to &quot;
--&gt; 703                             &quot;float&quot;.format(self._A.dtype))
    704 
    705         if self._A.ndim == 3 and self._A.shape[-1] == 1:

TypeError: Image data of dtype object cannot be converted to float

Could anyone suggest what this means? Thanks
","The data you are trying to plot (data[i]) contains data of type object (i.e. a string) which of course cannot be plotted as they are not numbers. This is caused by the fact that one of the columns in your dataset is a string which you are trying to plot, likely the Name column. Try removing the Name column from the dataset and see if there are other column which contains strings.
",<python><numpy><matplotlib>
"I have created some code that reads my CSV file and converts the dataset to a grayscale image. I want to know if there is any possible way to read through each row in the dataset and save each of the images created from the rows?
So far, I have got this code that reads the CSV files and creates an image using .imshow
import pandas as pd 
import numpy as np 
from sklearn.datasets import load_digits
from keras.preprocessing.image import array_to_img
import matplotlib.pyplot as plt

data_path = &quot;dataset_malwares.csv&quot;

data = pd.read_csv(data_path);

label = data.Malware.values

data = data.drop(&quot;Malware&quot;, axis=1)
data = data.drop(&quot;Name&quot;, axis=1)
data = data.values
data = data.reshape(data.shape[0], data.shape[1], 1)
data = np.tile(data, (1, data.shape[1]))

plt.imshow(data[1], cmap=&quot;gray&quot;)

plt.title(&quot;label: {0:}&quot;.format(label[1]))
plt.show()

print(data[0].shape)

I want to go through the dataset and save each image but not too sure where to start. Any suggestions would be great. Thanks :)
Rows/format of the data - I've provided a shared version of the dataset:
https://1drv.ms/x/s!AqFNg8FC48SSgtZSObDmmGHs3utWog
","Are you looking to simply save the file produced by plt.imshow? If yes, then you should be able to use plt.savefig as follows:
plt.imshow(data[1], cmap=&quot;gray&quot;)
plt.title(&quot;label: {0:}&quot;.format(label[1]))
plt.savefig(&quot;output_image.png&quot;)
plt.show()

You can either remove or keep the plt.show() call depending on whether you want the image to still be shown or just save the image.
EDIT: If you want to do this for all rows you can just loop through the numpy array as follows:
for i in range(data.shape[0])):
    plt.imshow(data[i], cmap=&quot;gray&quot;)
    plt.title(&quot;label: {0:}&quot;.format(label[1]))
    plt.savefig(f&quot;output_image_{i}.png&quot;)
    plt.close()

",<machine-learning><dataset><pandas><numpy>
"I'm wondering how does ndimage.measurements.center_of_mass calculate the center of mass values for a two dimensional numpy array.
For example let's say we have an array m:
m = numpy.array([[1,2,3,1],[4,4,4,1],[5,6,7,2]])

Then one would get the following for the coordinates of the center of mass (x, y):
x, y = ndimage.measurements.center_of_mass(m) 

x = 1.325  and  y = 1.3

So, how are those values of x and y mathematically derived (actual derivation)?
When I follow the steps in the function definition center_of_mass, I don't get that result.
","If you go to the source page for this method you can find the function:
def center_of_mass(input, labels=None, index=None):
    &quot;&quot;&quot;
    Calculate the center of mass of the values of an array at labels.
    Parameters
    ----------
    input : ndarray
        Data from which to calculate center-of-mass.
    labels : ndarray, optional
        Labels for objects in `input`, as generated by `ndimage.label`.
        Only used with `index`.  Dimensions must be the same as `input`.
    index : int or sequence of ints, optional
        Labels for which to calculate centers-of-mass. If not specified,
        all labels greater than zero are used.  Only used with `labels`.
    Returns
    -------
    center_of_mass : tuple, or list of tuples
        Coordinates of centers-of-mass.
    &quot;&quot;&quot;
    normalizer = sum(input, labels, index)
    grids = numpy.ogrid[[slice(0, i) for i in input.shape]]

    results = [sum(input * grids[dir].astype(float), labels, index) / normalizer
               for dir in range(input.ndim)]

    if numpy.isscalar(results[0]):
        return tuple(results)

    return [tuple(v) for v in numpy.array(results).T]

Whereby sum is (sum - centre)^2 of the input labels
",<python><numpy><scipy>
"I'm working with data(with 4 columns which are p(product), M(name of the store)), I want predict the demand of store for that I sued SVR on the data by theses formulation:
dfn = pd.get_dummies(df)
x = dfn.drop([&quot;demand&quot;],axis=1)
y = dfn.demand
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(feature_range=(0,1))
dfn = scaler.fit_transform(dfn)
.
.
.
from sklearn.metrics import r2_score
pred = regressor.predict(testX)
SVM_R2 = print('r2= ' +str(r2_score(testY,pred)))
print(pred)
# array example is between 0 and 1
array = np.array(pred)
#scaled from 200 to 800
minimo = 200
maximo = 800
output=array * minimo + (maximo - minimo)
print(output)
df2=pd.DataFrame(output)
df2.to_excel(r'/content/Book1.xlsx', index = False)

and now I get the output of this prediction. My question is, how can I match these outputs to inputs, or how can I found which demands are related to each market?



","The outputs of your model are in the same order as your inputs, so the first row in your output array corresponds to the first row in the testX array. If you want to have both the inputs and the model prediction in one table you can just concatenate them along the column axis.
",<python><scikit-learn><predictive-modeling><data><numpy>
"I'm working with data(with 4 columns which are p(product), M(name of the store)), I want predict the demand of store for that I sued SVR on the data by theses formulation:
dfn = pd.get_dummies(df)
x = dfn.drop([&quot;demand&quot;],axis=1)
y = dfn.demand
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(feature_range=(0,1))
dfn = scaler.fit_transform(dfn)
.
.
.
from sklearn.metrics import r2_score
pred = regressor.predict(testX)
SVM_R2 = print('r2= ' +str(r2_score(testY,pred)))
print(pred)
# array example is between 0 and 1
array = np.array(pred)
#scaled from 200 to 800
minimo = 200
maximo = 800
output=array * minimo + (maximo - minimo)
print(output)
df2=pd.DataFrame(output)
df2.to_excel(r'/content/Book1.xlsx', index = False)

and now I get the output of this prediction. My question is, how can I match these outputs to inputs, or how can I found which demands are related to each market?



","I used this code and get output from this(with the help of Oxbowerce).
df2=pd.DataFrame(testX,columns=['p','M','Date'])
df3=pd.DataFrame(pred,columns=['pred'])
df4=pd.concat([df2,df3],axis=1)
df4.to_excel(r'/content/Book1.xlsx', index = False)

I saved the output in an excel file. You can see in the below picture.

",<python><scikit-learn><predictive-modeling><data><numpy>
"

As a beginner, can someone please help me with this? Is there any way to show both pictures in a single cell's output? My output could only display one picture. Thank you for your time and attention!
Here is my code:
from skimage import data
image_coffee = data.coffee()
image_horse = data.horse()
fig = plt.figure(dpi=100)
plt.imshow(image_coffee)
plt.imshow(image_horse)

","One way to do it (without getting into lots of the inner-workings of Jupyter notebooks), it to use two matplotlib Axes in one plot. Then you show one image in each of these:
from skimage import data
image_coffee = data.coffee()
image_horse = data.horse()
fig, axs = plt.subplots(1, 2, figsize=(15, 8))  # one row of Axes, two columns = 2 plots

The axs variables is a list containing the two Axes, so just access each one and plot your image on it like this:
axs[0].imshow(image_coffee)
axs[1].imshow(image_horse)

If the plots don't pop automatically, either run plt.show() or make sure your notebook has executed %matplotlib inline in a cell.
",<python><numpy><matplotlib><jupyter>
"I have 2 dataframes with the same columns names and rows number but the elements' values are different. I need to change the value of each element in the first dataframe to 1 if its value in the the other dataframe is 1. I am stuck on this task. Any help pls?
","The easiest way to do it for your case is something like this:

In [1]: import pandas as pd                                                                                                         

In [2]: df1 = pd.DataFrame([0, 1, 2, 3, 1, 2, 3, 1, 2], columns=[&quot;A&quot;])                                                              

In [3]: df2 = pd.DataFrame([9] * len(df1), columns=[&quot;A&quot;])                                                                           

In [4]: df1                                                                                                                         
Out[4]: 
   A
0  0
1  1
2  2
3  3
4  1
5  2
6  3
7  1
8  2

In [5]: df2                                                                                                                         
Out[5]: 
   A
0  9
1  9
2  9
3  9
4  9
5  9
6  9
7  9
8  9

Wherever the row of column A in df1 is equal to 1, insert 1 into the same row of column A in df2
In [6]: df2.A[df1.A == 1] = 1                                                                                                       

In [7]: df2                                                                                                                         
Out[7]: 
   A
0  9
1  1
2  9
3  9
4  1
5  9
6  9
7  1
8  9



You could also use the pd.DataFrame.where method, to replace the part df1.A == 1.
There are also many other ways to get the same thing done, but my example is the most straighforward :)
",<python><pandas><data-cleaning><numpy><dataframe>
"This code should return a new column called orc_4 with the value of 1 if the value of the row in df['indictment_charges'] contains 2907.04 or 0 if not.
Instead it is returning all 0's
for index, item in enumerate(df.indictment_charges):
    s = '2907.04' 
    if s in str(item):
        df['orc_4'] = np.where(item == s, 1, 0)

Why won't it return 1?
Example output for df.indictment_charges:
['2903.112907.022907.042907.04']

","You are first checking if the item contains the string, but then in np.where you are checking if the values are equal (item == s), which is obviously different. In addition, you set the whole column equal to the value from np.where (and overwriting it after each row), which results in the whole column getting the value based on the final row of the dataframe.
To avoid looping over the rows (which is relatively quite slow) you can use pandas.Series.str.contains to check if a string contains a certain value like this:
df[&quot;orc_4&quot;] = np.where(df[&quot;indictment_charges&quot;].str.contains(&quot;2907.04&quot;), 1, 0)

",<python><pandas><numpy>
"I am trying to save contents of physiobank Normal Sinus Rhythm RR Interval Database into a numpy array but I keep getting an error:
Traceback (most recent call last):
  File &quot;AverageRRI.py&quot;, line 20, in &lt;module&gt;
    averageArray = np.fromfile(file,dtype=float)
FileNotFoundError: [Errno 2] No such file or directory: 'nsr001.ecg'

but the file does exist in the directory.
import os
import numpy as np

for root, dirs, files in os.walk('normal-sinus-rhythm-rr-interval-database-1.0.0'):
    for file in files:
        if file.endswith(&quot;.ecg&quot;):
            print(file)
            averageArray = np.fromfile(file,dtype=float)
            print(averageArray)

When I add the pathname like:
averageArray = np.fromfile('normal-sinus-rhythm-rr-interval-database-1.0.0/nsr001.ecg',dtype=float)
            print(averageArray)

It works.
Thanks so much!
","You just need to add the root directory in front of the filename so that the full filepath is correct. For this you can use os.path.join:
import os
import numpy as np

for root, dirs, files in os.walk('normal-sinus-rhythm-rr-interval-database-1.0.0'):
    for file in files:
        if file.endswith(&quot;.ecg&quot;):
            print(os.path.join(root, file))
            averageArray = np.fromfile(os.path.join(root, file), dtype=float)

",<python><numpy><databases>
"I am trying to save contents of physiobank Normal Sinus Rhythm RR Interval Database into a numpy array but I keep getting an error:
Traceback (most recent call last):
  File &quot;AverageRRI.py&quot;, line 20, in &lt;module&gt;
    averageArray = np.fromfile(file,dtype=float)
FileNotFoundError: [Errno 2] No such file or directory: 'nsr001.ecg'

but the file does exist in the directory.
import os
import numpy as np

for root, dirs, files in os.walk('normal-sinus-rhythm-rr-interval-database-1.0.0'):
    for file in files:
        if file.endswith(&quot;.ecg&quot;):
            print(file)
            averageArray = np.fromfile(file,dtype=float)
            print(averageArray)

When I add the pathname like:
averageArray = np.fromfile('normal-sinus-rhythm-rr-interval-database-1.0.0/nsr001.ecg',dtype=float)
            print(averageArray)

It works.
Thanks so much!
","FileNotFoundError: [Errno 2] No such file or directory: ---&gt;'nsr001.ecg'
The directory path you wrote doesnt have the file which you are passing, check the path.
",<python><numpy><databases>
"I am trying to understand the numpy array object, and a little mystified by the following:
A = np.array([[1,2,3],[4,5,6]]) # A is a 2x3 matrix
B = A

A = A.T # A is now the matrix [[1,4],[2,5],[3,6]]
A[0,1]= -1 # A is now the matrix [[1,-1],[2,5],[3,6]]

print(B)

I get for B:
[[1,2,3],[-1,5,6]]

So changing A using the assignment A[0,1]= -1 changed B (which is to be expected since numpy arrays are mutable objects, and python assignments are by reference). But what is mystifying is that A = A.T did not transpose B. Furthermore, if I ask (after the A = A.T statement):
A is B
I get False. What exactly is A.T returning? It seems to return a view of the data, but a copy of the alignment of data into a matrix.
","You are right that B points to the original array and A (at the end) is a transposed view of the array.
The key point here is something that trips up a lot of people: whenever you assign a new value to a variable by using = with no indexing (e.g., A = &lt;anything&gt;), it causes that variable to point to a new location, but it doesn't alter the data previously held in that variable.
Here is an annotated version of what is happening:
A = np.array([[1,2,3],[4,5,6]])
# create the array [[1,2,3],[4,5,6]] at some location in memory
# (let's call it loc 1)
# point variable A to loc 1

B = A
# point variable B to the same location as A (loc 1)

A = A.T
# right-hand side:
#    - get the array that A currently points to (from loc 1)
#    - create a transposed view of this array and store the view 
#      somewhere in memory (let's call it loc 2)
# left-hand side: set variable A to point to loc 2
# Note: the `=` assignment does not alter the object at loc 1; instead, 
# it points A to the new object that was created on the right-hand side

A[0,1]= -1
# Push the value -1 into position (0, 1) of the object that A
# currently points to, i.e., the view at loc 2. This updates 
# the A view, but also propagates back to the original, 
# un-transposed array at loc 1 (which B still points to).

# Note that the last two lines are equivalent to this:
X = A.T     # doesn't transpose B
X[0,1]= -1  # propagates through view back to B
# i.e., the A.T could have been assigned to anything, e.g. a new
# variable X. When you do an assignment with `=`, it creates a new
# variable every time, even if it happens to have the same name as 
# an old one, as in your case.

This is the general issue of changing mutable data and getting unexpected side effects. The easiest way to think about it is that a bare variable assignment (A = &lt;something&gt;) never alters the underlying data that A currently points to. Instead, it just causes A to point somewhere new. On the other hand, assigning A[index] = &lt;something&gt; or A.attribute = &lt;something&gt; or using A.data_altering_method(...) will alter the underlying object that A points to, in place, so anything else that points to the same object will also see the alteration.
In your case, the only thing you did that changed the underlying data was A[0,1]= -1, and that did indeed propagate to B. A = A.T did not alter the underlying data, it just pointed A to something new (which happened to be a new view of the original data). So the change to the A variable didn't propagate to B. However, it is a little surprising that  A = A.T creates a view rather than a copy, so that later changes to A affect B. Your middle two lines are equivalent to B.T[0,1] = -1.
These answers also help:

https://stackoverflow.com/a/7838047/3830997
https://stackoverflow.com/a/38925257/3830997

",<python><numpy>
"I am trying to understand the numpy array object, and a little mystified by the following:
A = np.array([[1,2,3],[4,5,6]]) # A is a 2x3 matrix
B = A

A = A.T # A is now the matrix [[1,4],[2,5],[3,6]]
A[0,1]= -1 # A is now the matrix [[1,-1],[2,5],[3,6]]

print(B)

I get for B:
[[1,2,3],[-1,5,6]]

So changing A using the assignment A[0,1]= -1 changed B (which is to be expected since numpy arrays are mutable objects, and python assignments are by reference). But what is mystifying is that A = A.T did not transpose B. Furthermore, if I ask (after the A = A.T statement):
A is B
I get False. What exactly is A.T returning? It seems to return a view of the data, but a copy of the alignment of data into a matrix.
","You get a view of the matrix, with only the the axes transposed. So your B doesn't get transposed.
More nuances depend on the shape of your original array:
In  [1]: import numpy as np

In  [2]: np.ndarray.transpose??
Out [2]:

Returns a view of the array with axes transposed.
For a 1-D array, this has no effect. (To change between column and
row vectors, first cast the 1-D array into a matrix object.)                                                      
For a 2-D array, this is the usual matrix transpose.                                                              
For an n-D array, if axes are given, their order indicates how the                                               
axes are permuted (see Examples). [truncated]


NOTE: there is a small difference between, a.T and a.transpose():

Same as self.transpose(), except that self is returned if self.ndim &lt; 2.


There are other methods, such as np.ndarray.reshape(), which might or might not return a copy of the data, also depending on things like the shape of the array.
",<python><numpy>
"I want to input a numpy 2d array into MLP but I have an array of 50395 rows that contains many 2d array of shape (x, 129). x because some matrices have different row numbers. Here is an example :
train['spec'].shape
&gt;&gt;(50395,)
train['spec'][0].shape
&gt;&gt;(41, 129)
train['spec'][5].shape
&gt;&gt;(71, 129)

Here an snippet of my code :
X_train = train['spec'].values;     X_valid = valid['spec'].values
y_train = train['label'].values;    y_valid = valid['label'].values
model.add(Dense(12, input_shape=(50395, ), activation='relu'));
model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=500, batch_size=1);

I get this error on last line (model.fit) :
ValueError: Error when checking input: expected dense_54_input to have shape (50395,) but got array with shape (1,)
How to fix this problem so that the network can take as input all 50395 matrices of shape (x, 129)?
","First, I made the matrices (x, 129) to have the same shape by padding in with the rows filled with zeros.
Second, I transformed my large matrices into dimension 3 by the following process:
train = list(train_df['spec'])
np.array(train).shape
&gt;&gt;&gt; (50395, 71, 129)


X_train = list(train['spec']);     X_valid = list(valid['spec'])
y_train = train['label'].values;    y_valid = valid['label'].values
model.add(Dense(12, input_shape=(50395, 71, 129), activation='relu'));
model.fit(np.array(X_train), y_train, validation_data=(np.array(X_valid), y_valid), epochs=500, batch_size=1);

",<keras><numpy><mlp>
"I'm learning how to implement and evaluate a Logistic Regression Model, for this I need to change the values of my array from strings to 0 &amp; 1.
I have the following numpy ndarray as a result of a DataFrame.values call
['PAIDOFF', 'COLLECTION', 'COLLECTION', 'PAIDOFF', 'PAIDOFF', 'PAIDOFF', ...]
I would like to know how can I change the values like:
'PAIDOFF' to 0 and 'COLLECTION' to 1
Any help will be much appreciated
","import numpy as np

a = np.array(['PAIDOFF', 'COLLECTION', 'COLLECTION', 'PAIDOFF'])

f = lambda x: 1 if x == &quot;COLLECTION&quot; else 0

np.fromiter(map(f,a),dtype=np.int)

Alternative:
np.where(a == &quot;COLLECTION&quot;,1,0)

",<python><numpy>
"I'm trying to run a Random Forest model from sklearn but I keep getting an error: ValueError: Input contains NaN, infinity or a value too large for dtype('float32').
I tried following steps in ValueError: Input contains NaN, infinity or a value too large for dtype(&#39;float32&#39;)
fillna(0) on my pandas dataframe still gave the ValueError.
So I tried working with my numpy array:
val = setTo.ravel().nan_to_num(0)

But I keep getting an error: 'numpy.ndarray' object has no attribute 'nan_to_num'
I'm wondering how I can deal with the nan values if I have ndarray?
Update
Thanks so much to @Beniamin H for all the help, as suggested, I rescaled the data, which I based on
https://stackoverflow.com/questions/34771118/sklearn-random-forest-error-on-input and it worked!
","You are using the right method but in a wrong way :)
nan_to_num is a method of numpy module, not numpy.ndarray. So instead of calling nan_to_num on you data, call it on numpy module giving your data as a paramter:
import numpy as np
data = np.array([1,2,3,np.nan,np.nan,5])
data_without_nan = np.nan_to_num(data)

prints:
array([1., 2., 3., 0., 0., 5.])

In your example:
import numpy as np
val = np.nan_to_num(setTo.ravel())

",<python><scikit-learn><pandas><random-forest><numpy>
"I am quite a newbie to Machine Learning, now trying to implement from scratch in Python (using numpy) a logistic regression algorithm.
I took the gender/height/weight data from here.
Then I did the following:

Normalized the dimensions using MixMax ([0, 1] range): the result is in here.
Replaced Mail/Female by 1/0 in a separate file: the result is in here.

Here is my Python code with some printouts:
import numpy as np
np.set_printoptions(suppress=True)

def sigmoid(x):
  return 1 / (1 + np.exp(-x))
  
def predict(X, W, b):
    return (sigmoid(X.dot(W) + b))


from numpy import genfromtxt
X = genfromtxt('c:\\temp\\\\ml1\\data1.csv', delimiter=',',skip_header=1)
TT = genfromtxt('c:\\temp\\ml1\\val.csv', delimiter=',',skip_header=1)
T = TT.T

b = 2.0
W = np.repeat(1.0, 3)

lr = 0.001
num_epochs = 1000
for epoch in range(num_epochs):
    Y = predict(X, W, b)
    W = W - lr * X.T.dot(Y - T)
    b = b - lr * np.sum(Y - T)
    print(W)
    print(b)
    
print(predict(X, W, b))

I am reading the data (I am aware of losing the 1st line, to overcome some weird issue) - then choosing some learning rate 'lr' and initial values for W and b parameters, then running the algorithm from the &quot;textbook&quot; for 1,000 iterations.
In terms of the convergence, I see that the W and b values are quite stable at the end of the run. Here is the tail of my printouts:
[-0.07538705 -0.06014817  0.19189458]
-0.11792230458161261
[-0.07527279 -0.06033511  0.19213628]
-0.11806707431160607
[-0.07515862 -0.06052191  0.19237781]
-0.11821173599078225
[-0.07504453 -0.06070857  0.19261915]
-0.11835628969685595
[-0.07493052 -0.06089509  0.19286032]
-0.11850073550750553
[-0.0748166  -0.06108146  0.1931013 ]
-0.11864507350037269
[-0.07470277 -0.0612677   0.1933421 ]
-0.11878930375306242
[-0.07458902 -0.0614538   0.19358272]
-0.11893342634314288
[-0.07447536 -0.06163975  0.19382316]
-0.11907744134814532
[-0.07436178 -0.06182557  0.19406342]
-0.11922134884556394
[-0.07424828 -0.06201125  0.1943035 ]
-0.11936514891285584
[-0.07413487 -0.06219678  0.1945434 ]
-0.11950884162744084
[-0.07402154 -0.06238218  0.19478312]
-0.11965242706670146
[-0.0739083  -0.06256744  0.19502266]
-0.11979590530798276

However, when checking my &quot;predictions&quot; - I see almost all of them close to 0.5, so the algorithm is quite meaningless.
[0.46893913 0.4864133  0.47447843 0.49496884 0.47632166 0.51064392
 0.50576891 0.48296325 0.49249426 0.46803355 0.49891959 0.4758681
 0.4847823  0.46963354 0.50660758 0.50484516 0.50754398 0.5040612
 0.49615924 0.50484516 0.50066406 0.49327619 0.49209486 0.47724512
 0.49376066 0.47720157 0.46336566 0.49611527 0.49206556 0.5045744
 0.467709   0.46051103 0.50094951 0.49874499 0.48199598 0.47233766
 0.5035388  0.49446713 0.47116303 0.48814551 0.50774432 0.48341893
 0.50245697 0.48514832 0.47178667 0.48392745 0.48770583 0.48952584
 0.50425966 0.49807824 0.46794947 0.50299757 0.4912579  0.48212484
 0.47906037 0.49313446 0.49603214 0.4889151  0.51240564 0.45366836
 0.50514138 0.45293572 0.46919429 0.49586108 0.49128368 0.49250891
 0.479629   0.49694385 0.47986689 0.48533612 0.48769118 0.51421002
 0.47341634 0.47823677 0.47629082 0.49363102 0.49474045 0.48761986
 0.45725005 0.48813342 0.49785046 0.47579691 0.48495003 0.49979884
 0.45209092 0.49492264 0.50778636 0.50522932 0.49193848 0.49212225
 0.47925849 0.47372671 0.47636617 0.48128716 0.49436743 0.50584186
 0.49985458 0.46004473 0.45447197 0.47558174 0.50655089 0.4972952
 0.47751121 0.47998011 0.48108802 0.49503508 0.49803427 0.48833114
 0.49418684 0.50309888 0.50178637 0.48131452 0.46770741 0.49084644
 0.49531    0.48086376 0.5092934  0.50922206 0.49252005 0.49505326
 0.48411204 0.48176849 0.48930865 0.49900112 0.49675813 0.4828347
 0.50351045 0.49129833 0.48449809 0.49745258 0.47198546 0.49837545
 0.49195377 0.49415434 0.49108528 0.48808724 0.4740387  0.49524914
 0.50241396 0.47988471 0.49555082 0.49216524 0.501277   0.51473554
 0.48050621 0.4743507  0.50519808 0.47525855 0.48140009 0.4526402
 0.49899159 0.49475574 0.49729616 0.50558416 0.4893542  0.48134538
 0.49448083 0.47718694 0.48848748 0.46409926 0.47197085 0.4758681
 0.49239297 0.49047606 0.47292017 0.48933955 0.50584026 0.49344787
 0.47039821 0.48208092 0.49557661 0.49591681 0.47993813 0.47707121
 0.48516296 0.49228119 0.48030869 0.49988389 0.47731474 0.48902846
 0.48787584 0.49549316 0.4737283  0.47477584 0.48732347 0.49826301
 0.48134538 0.50026713 0.49691454 0.51342852 0.50733283 0.49347462
 0.50589503 0.51217799 0.50903735 0.50025599 0.48787584 0.49738122
 0.48778924 0.46506286 0.48101835 0.4985921  0.48827447 0.50168666
 0.50059622 0.51028823 0.48134379 0.4768611  0.50412239 0.50106547
 0.50136013 0.50113331 0.48577187 0.4558112  0.49743888 0.50818415
 0.51308757 0.50764367 0.5044473  0.47469163 0.49280543 0.49972907
 0.4941553  0.45458444 0.50089184 0.48836108 0.4837712  0.49900305
 0.49519341 0.48229377 0.4815862  0.48029501 0.48329037 0.50366686
 0.48550546 0.49307521 0.49618662 0.47666369 0.50835518 0.48248087
 0.47987008 0.46226317 0.49230697 0.45667171 0.50622344 0.47931509
 0.50276691 0.47871978 0.4969292  0.49242451 0.50551088 0.49677022
 0.49648479 0.49155439 0.48163011 0.49063243 0.50581095 0.49630099
 0.48347238 0.50522932 0.50002565 0.5014608  0.48388768 0.50198932
 0.51327221 0.49578972 0.50092019 0.49620128 0.47115001 0.50590969
 0.50760259 0.48411204 0.46197999 0.48279333 0.4782797  0.48779083
 0.46966179 0.48573305 0.46395983 0.46106193 0.5057963  0.51033122
 0.49279237 0.48013628 0.48364231 0.50247322 0.49134037 0.48425685
 0.50899532 0.50062554 0.47800942 0.48996655 0.49660268 0.49682692
 0.49750832 0.47504562 0.50530131 0.47859195 0.50100877 0.49156904
 0.48764916 0.47024235 0.48825982 0.50302496 0.48502133 0.49061618
 0.51203632 0.45967757 0.49603118 0.49524914 0.49122699 0.48930961
 0.47503292 0.47560051 0.50264077 0.47731474 0.50620783 0.4932195
 0.50212852 0.48365791 0.48770583 0.50707675 0.49119961 0.50541567
 0.48634422 0.46518872 0.49555082 0.48875876 0.47792453 0.49558934
 0.50552906 0.47930206 0.47786953 0.48341893 0.48400097 0.495604
 0.5004382  0.48308957 0.49870294 0.49171396 0.48956627 0.49867715
 0.48242424 0.49040473 0.48722318 0.47119225 0.48163011 0.4944681
 0.48279397 0.47103539 0.46620747 0.48312108 0.49024677 0.45447197
 0.48296325 0.48324581 0.49109993 0.46619447 0.48073173 0.46168387
 0.49489589 0.49996702 0.47812102 0.49040313 0.51421002 0.47901743
 0.48369991 0.506677   0.50449191 0.49407345 0.50204443 0.46441209
 0.48048013 0.49192639 0.50236999 0.51246231 0.49384251 0.50780198
 0.48272206 0.5028832  0.46226317 0.48787584 0.50043628 0.49161108
 0.46854375 0.50444538 0.4980808  0.49160058 0.48641553 0.47996963
 0.47419629 0.47233925 0.48050781 0.50025151 0.48875876 0.47796746
 0.49755229 0.49496884 0.49924356 0.47447843 0.49020634 0.47364348
 0.49139961 0.48101676 0.50323711 0.47266575 0.49775075 0.50663496
 0.49894538 0.46687379 0.49022099 0.49119865 0.4972952  0.50593996
 0.4931198  0.5060387  0.48069293 0.50055161 0.47636617 0.46113286
 0.50519808 0.49078719 0.48909819 0.46850155 0.46603877 0.48111537
 0.48286205 0.50108013 0.4948822  0.47368541 0.49674091 0.50893863
 0.48591511 0.48008286 0.49479363 0.5030103  0.4897777  0.46271521
 0.49597351 0.48969428 0.47317556 0.50380509 0.50150285 0.48354366
 0.5000677  0.5043922  0.48761986 0.50976248 0.49944203 0.49679761
 0.49344724 0.49958571 0.4570385  0.47202738 0.47439423 0.48298997
 0.47398278 0.50024134 0.49118496 0.48654031 0.50337983 0.46687379
 0.50434567 0.47260826 0.50212948 0.47731474 0.50418926 0.50292621
 0.47277723 0.50345022 0.50561251 0.4755576  0.48692264 0.48674119
 0.48399778 0.49809193 0.4790327  0.48526482 0.50845487 0.50495856
 0.49078879 0.51308757 0.50220244 0.48486505 0.49010666 0.50407779
 0.47058234 0.47876016 0.49237832 0.48073588 0.5031941  0.45209092
 0.46994587 0.50822811 0.50096224 0.48503501 0.50623617 0.50925137
 0.49688459]

Would like an expert opinion in 2 areas:

Can you detect a mistake in the way I handle the data, or in my Python code?
Assuming the code is ok, what can I do in order to succeed with the learning? (for example choosinf better values on W and B, learning rate - how?)

","Firstly, the Learning rate controls how much the weight changes in each iteration. You should set a good value for this parameter. maybe a value between 0.1 to 0.3 here works very well. you can read this article too.
Secondly, it is a good idea to consider bias like other weights. you can add a new column of ones to the X (like a feature column where all the records are one), then bias will be multiplied with this column.
bias_raw = np.array([1 for i in range(X.shape[0])]).astype(&quot;float&quot;).reshape(-1, 1)
X_bias =np.append(X, bias_raw, axis=1)

right now you don't need to have the variable b as bias and you can have 4 weights
weights = np.random.rand(X_bias.shape[1])

",<logistic-regression><numpy>
"I am quite a newbie to Machine Learning, now trying to implement from scratch in Python (using numpy) a logistic regression algorithm.
I took the gender/height/weight data from here.
Then I did the following:

Normalized the dimensions using MixMax ([0, 1] range): the result is in here.
Replaced Mail/Female by 1/0 in a separate file: the result is in here.

Here is my Python code with some printouts:
import numpy as np
np.set_printoptions(suppress=True)

def sigmoid(x):
  return 1 / (1 + np.exp(-x))
  
def predict(X, W, b):
    return (sigmoid(X.dot(W) + b))


from numpy import genfromtxt
X = genfromtxt('c:\\temp\\\\ml1\\data1.csv', delimiter=',',skip_header=1)
TT = genfromtxt('c:\\temp\\ml1\\val.csv', delimiter=',',skip_header=1)
T = TT.T

b = 2.0
W = np.repeat(1.0, 3)

lr = 0.001
num_epochs = 1000
for epoch in range(num_epochs):
    Y = predict(X, W, b)
    W = W - lr * X.T.dot(Y - T)
    b = b - lr * np.sum(Y - T)
    print(W)
    print(b)
    
print(predict(X, W, b))

I am reading the data (I am aware of losing the 1st line, to overcome some weird issue) - then choosing some learning rate 'lr' and initial values for W and b parameters, then running the algorithm from the &quot;textbook&quot; for 1,000 iterations.
In terms of the convergence, I see that the W and b values are quite stable at the end of the run. Here is the tail of my printouts:
[-0.07538705 -0.06014817  0.19189458]
-0.11792230458161261
[-0.07527279 -0.06033511  0.19213628]
-0.11806707431160607
[-0.07515862 -0.06052191  0.19237781]
-0.11821173599078225
[-0.07504453 -0.06070857  0.19261915]
-0.11835628969685595
[-0.07493052 -0.06089509  0.19286032]
-0.11850073550750553
[-0.0748166  -0.06108146  0.1931013 ]
-0.11864507350037269
[-0.07470277 -0.0612677   0.1933421 ]
-0.11878930375306242
[-0.07458902 -0.0614538   0.19358272]
-0.11893342634314288
[-0.07447536 -0.06163975  0.19382316]
-0.11907744134814532
[-0.07436178 -0.06182557  0.19406342]
-0.11922134884556394
[-0.07424828 -0.06201125  0.1943035 ]
-0.11936514891285584
[-0.07413487 -0.06219678  0.1945434 ]
-0.11950884162744084
[-0.07402154 -0.06238218  0.19478312]
-0.11965242706670146
[-0.0739083  -0.06256744  0.19502266]
-0.11979590530798276

However, when checking my &quot;predictions&quot; - I see almost all of them close to 0.5, so the algorithm is quite meaningless.
[0.46893913 0.4864133  0.47447843 0.49496884 0.47632166 0.51064392
 0.50576891 0.48296325 0.49249426 0.46803355 0.49891959 0.4758681
 0.4847823  0.46963354 0.50660758 0.50484516 0.50754398 0.5040612
 0.49615924 0.50484516 0.50066406 0.49327619 0.49209486 0.47724512
 0.49376066 0.47720157 0.46336566 0.49611527 0.49206556 0.5045744
 0.467709   0.46051103 0.50094951 0.49874499 0.48199598 0.47233766
 0.5035388  0.49446713 0.47116303 0.48814551 0.50774432 0.48341893
 0.50245697 0.48514832 0.47178667 0.48392745 0.48770583 0.48952584
 0.50425966 0.49807824 0.46794947 0.50299757 0.4912579  0.48212484
 0.47906037 0.49313446 0.49603214 0.4889151  0.51240564 0.45366836
 0.50514138 0.45293572 0.46919429 0.49586108 0.49128368 0.49250891
 0.479629   0.49694385 0.47986689 0.48533612 0.48769118 0.51421002
 0.47341634 0.47823677 0.47629082 0.49363102 0.49474045 0.48761986
 0.45725005 0.48813342 0.49785046 0.47579691 0.48495003 0.49979884
 0.45209092 0.49492264 0.50778636 0.50522932 0.49193848 0.49212225
 0.47925849 0.47372671 0.47636617 0.48128716 0.49436743 0.50584186
 0.49985458 0.46004473 0.45447197 0.47558174 0.50655089 0.4972952
 0.47751121 0.47998011 0.48108802 0.49503508 0.49803427 0.48833114
 0.49418684 0.50309888 0.50178637 0.48131452 0.46770741 0.49084644
 0.49531    0.48086376 0.5092934  0.50922206 0.49252005 0.49505326
 0.48411204 0.48176849 0.48930865 0.49900112 0.49675813 0.4828347
 0.50351045 0.49129833 0.48449809 0.49745258 0.47198546 0.49837545
 0.49195377 0.49415434 0.49108528 0.48808724 0.4740387  0.49524914
 0.50241396 0.47988471 0.49555082 0.49216524 0.501277   0.51473554
 0.48050621 0.4743507  0.50519808 0.47525855 0.48140009 0.4526402
 0.49899159 0.49475574 0.49729616 0.50558416 0.4893542  0.48134538
 0.49448083 0.47718694 0.48848748 0.46409926 0.47197085 0.4758681
 0.49239297 0.49047606 0.47292017 0.48933955 0.50584026 0.49344787
 0.47039821 0.48208092 0.49557661 0.49591681 0.47993813 0.47707121
 0.48516296 0.49228119 0.48030869 0.49988389 0.47731474 0.48902846
 0.48787584 0.49549316 0.4737283  0.47477584 0.48732347 0.49826301
 0.48134538 0.50026713 0.49691454 0.51342852 0.50733283 0.49347462
 0.50589503 0.51217799 0.50903735 0.50025599 0.48787584 0.49738122
 0.48778924 0.46506286 0.48101835 0.4985921  0.48827447 0.50168666
 0.50059622 0.51028823 0.48134379 0.4768611  0.50412239 0.50106547
 0.50136013 0.50113331 0.48577187 0.4558112  0.49743888 0.50818415
 0.51308757 0.50764367 0.5044473  0.47469163 0.49280543 0.49972907
 0.4941553  0.45458444 0.50089184 0.48836108 0.4837712  0.49900305
 0.49519341 0.48229377 0.4815862  0.48029501 0.48329037 0.50366686
 0.48550546 0.49307521 0.49618662 0.47666369 0.50835518 0.48248087
 0.47987008 0.46226317 0.49230697 0.45667171 0.50622344 0.47931509
 0.50276691 0.47871978 0.4969292  0.49242451 0.50551088 0.49677022
 0.49648479 0.49155439 0.48163011 0.49063243 0.50581095 0.49630099
 0.48347238 0.50522932 0.50002565 0.5014608  0.48388768 0.50198932
 0.51327221 0.49578972 0.50092019 0.49620128 0.47115001 0.50590969
 0.50760259 0.48411204 0.46197999 0.48279333 0.4782797  0.48779083
 0.46966179 0.48573305 0.46395983 0.46106193 0.5057963  0.51033122
 0.49279237 0.48013628 0.48364231 0.50247322 0.49134037 0.48425685
 0.50899532 0.50062554 0.47800942 0.48996655 0.49660268 0.49682692
 0.49750832 0.47504562 0.50530131 0.47859195 0.50100877 0.49156904
 0.48764916 0.47024235 0.48825982 0.50302496 0.48502133 0.49061618
 0.51203632 0.45967757 0.49603118 0.49524914 0.49122699 0.48930961
 0.47503292 0.47560051 0.50264077 0.47731474 0.50620783 0.4932195
 0.50212852 0.48365791 0.48770583 0.50707675 0.49119961 0.50541567
 0.48634422 0.46518872 0.49555082 0.48875876 0.47792453 0.49558934
 0.50552906 0.47930206 0.47786953 0.48341893 0.48400097 0.495604
 0.5004382  0.48308957 0.49870294 0.49171396 0.48956627 0.49867715
 0.48242424 0.49040473 0.48722318 0.47119225 0.48163011 0.4944681
 0.48279397 0.47103539 0.46620747 0.48312108 0.49024677 0.45447197
 0.48296325 0.48324581 0.49109993 0.46619447 0.48073173 0.46168387
 0.49489589 0.49996702 0.47812102 0.49040313 0.51421002 0.47901743
 0.48369991 0.506677   0.50449191 0.49407345 0.50204443 0.46441209
 0.48048013 0.49192639 0.50236999 0.51246231 0.49384251 0.50780198
 0.48272206 0.5028832  0.46226317 0.48787584 0.50043628 0.49161108
 0.46854375 0.50444538 0.4980808  0.49160058 0.48641553 0.47996963
 0.47419629 0.47233925 0.48050781 0.50025151 0.48875876 0.47796746
 0.49755229 0.49496884 0.49924356 0.47447843 0.49020634 0.47364348
 0.49139961 0.48101676 0.50323711 0.47266575 0.49775075 0.50663496
 0.49894538 0.46687379 0.49022099 0.49119865 0.4972952  0.50593996
 0.4931198  0.5060387  0.48069293 0.50055161 0.47636617 0.46113286
 0.50519808 0.49078719 0.48909819 0.46850155 0.46603877 0.48111537
 0.48286205 0.50108013 0.4948822  0.47368541 0.49674091 0.50893863
 0.48591511 0.48008286 0.49479363 0.5030103  0.4897777  0.46271521
 0.49597351 0.48969428 0.47317556 0.50380509 0.50150285 0.48354366
 0.5000677  0.5043922  0.48761986 0.50976248 0.49944203 0.49679761
 0.49344724 0.49958571 0.4570385  0.47202738 0.47439423 0.48298997
 0.47398278 0.50024134 0.49118496 0.48654031 0.50337983 0.46687379
 0.50434567 0.47260826 0.50212948 0.47731474 0.50418926 0.50292621
 0.47277723 0.50345022 0.50561251 0.4755576  0.48692264 0.48674119
 0.48399778 0.49809193 0.4790327  0.48526482 0.50845487 0.50495856
 0.49078879 0.51308757 0.50220244 0.48486505 0.49010666 0.50407779
 0.47058234 0.47876016 0.49237832 0.48073588 0.5031941  0.45209092
 0.46994587 0.50822811 0.50096224 0.48503501 0.50623617 0.50925137
 0.49688459]

Would like an expert opinion in 2 areas:

Can you detect a mistake in the way I handle the data, or in my Python code?
Assuming the code is ok, what can I do in order to succeed with the learning? (for example choosinf better values on W and B, learning rate - how?)

","I replicate your code using a toy data set and I did not find anything wrong with your implementation:
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_breast_cancer

from sklearn.metrics import accuracy_score

import matplotlib.pyplot as plt
plt.style.use(&quot;seaborn-whitegrid&quot;)

import warnings
warnings.filterwarnings(&quot;ignore&quot;)


X, y = load_breast_cancer(return_X_y= True)
X_train,X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state = 42)

sc = StandardScaler().fit(X_train, y_train)

X_train = sc.transform(X_train)
X_test = sc.transform(X_test)


def sigmoid(x):
  return 1 / (1 + np.exp(-x))
  
def predict(X, W, b):
    return (sigmoid(X.dot(W) + b))



b = 2.0
W = np.repeat(1.0, X_train.shape[1])
m = X_train.shape[0]

cost = list()

lr = 0.001
num_epochs = 100
for epoch in range(num_epochs):
    Y = predict(X_train, W, b)
    W = W - lr * X_train.T.dot(Y - y_train.T)
    b = b - lr * np.sum(Y - y_train.T)

    loss = -1/m * np.sum(y_train * np.log(Y) + (1 - y_train) * np.log(1 - Y))
    # print(W)
    # print(b)
    cost.append(loss)
print(f&quot;params are W:{W} and b:{b}&quot;)

probs = predict(X_test, W, b)
preds = np.where(probs &gt; .5, 1,0)


test_acc = accuracy_score(y_true = y_test, y_pred = preds)
    
plt.plot(cost)
plt.title(f&quot;Accuracy score is: {round(test_acc,3)}&quot;);


Nonetheless, when using your data the results are inferior:
data  =pd.read_csv(&quot;https://raw.githubusercontent.com/abhiwalia15/500-Person-Gender-Height-Weight-Body-Mass-Index/master/500_Person_Gender_Height_Weight_Index.csv&quot;, error_bad_lines= False)
data.drop([&quot;Index&quot;], axis = 1, inplace = True)

X = data.drop([&quot;Gender&quot;], axis = 1)
y = data.Gender.map({&quot;Male&quot;:0,&quot;Female&quot;:1})
X_train,X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state = 42)

sc = MinMaxScaler().fit(X_train, y_train)

X_train = sc.transform(X_train)
X_test = sc.transform(X_test)


b = 2.0
W = np.repeat(1.0, X_train.shape[1])
m = X_train.shape[0]

cost = list()

lr = 0.001
num_epochs = 100
for epoch in range(num_epochs):
    Y = predict(X_train, W, b)
    W = W - lr * X_train.T.dot(Y - y_train.T)
    b = b - lr * np.sum(Y - y_train.T)

    loss = -1/m * np.sum(y_train * np.log(Y) + (1 - y_train) * np.log(1 - Y))
    # print(W)
    # print(b)
    cost.append(loss)
print(f&quot;params are W:{W} and b:{b}&quot;)

probs = predict(X_test, W, b)
preds = np.where(probs &gt; .5, 1,0)


test_acc = accuracy_score(y_true = y_test, y_pred = preds)
    
plt.plot(cost)
plt.title(f&quot;Accuracy score is: {round(test_acc,3)}&quot;);


Going deeper on your data I see the problem as confirmed above, is not in your implementation of SGD but in the separateness of your data, this is not linearly separable:
from sklearn.decomposition import PCA
pca = PCA(n_components= 2).fit(X_train, y_train)
X2D = pca.transform(X_test)

ev = pca.explained_variance_.sum()

plt.scatter(X2D[:,0], X2D[:,1], c = y_test, cmap = &quot;RdYlBu&quot;)
plt.colorbar()
plt.title(f&quot;PCA projection in 2D\nExplaneid variance is: [{round(ev,3)}]&quot;)


",<logistic-regression><numpy>
"
I am trying to built a function to calculate the Geometric mean for Growth column and Avg_growth.





Year
Company Name
FH_PBIDT
Growth
Avg_growth




2009-10
Aayush Food &amp; Herbs Ltd.
0.044881
0.000000
-0.773403


2010-11
Aayush Food &amp; Herbs Ltd.
0.063545
0.415839
-0.773403





For that, I am using the below code:

from scipy.stats.mstats import gmean

I created a column name:

df['Geometric Mean'] = gmean(df.iloc[:,3])
df['Avg_Growth Mean'] = gmean(df.iloc[:,4])

I am getting an error that gmean is not defined. Could someone help me here?

","Can also write a geometric mean function by a little help  from Numpy.
import numpy as np

def geometric_mean(i):
    x = np.array(i)
    return x.prod()**(1.0/len(x))

",<python><pandas><numpy><pytorch-geometric>
"
I am trying to built a function to calculate the Geometric mean for Growth column and Avg_growth.





Year
Company Name
FH_PBIDT
Growth
Avg_growth




2009-10
Aayush Food &amp; Herbs Ltd.
0.044881
0.000000
-0.773403


2010-11
Aayush Food &amp; Herbs Ltd.
0.063545
0.415839
-0.773403





For that, I am using the below code:

from scipy.stats.mstats import gmean

I created a column name:

df['Geometric Mean'] = gmean(df.iloc[:,3])
df['Avg_Growth Mean'] = gmean(df.iloc[:,4])

I am getting an error that gmean is not defined. Could someone help me here?

","As far as your error 'Series' object has no attribute 'gmean', make sure you are not doing this anywhere in your code:
# wrong way of calling gmean on a series
df.iloc[:,4].gmean() # accessing gmean as a function on series object


Instead how you have called gmean in your question description above is correct and works:
df['Avg_Growth Mean'] = gmean(df.iloc[:,4]) #works!

However, there might be more than one thing wrong here,
I tried testing code on data given by you and while gmean is available from the scipy.stats.mstats module, and it can be executed; the data is not apt for calculation of a geometric mean.
There is a zero value in Growth column, and negative values given in Avg Growth column. You can not find Geometric mean in case of negatives and zeroes: see this link for more details
See the image below for code I executed:

",<python><pandas><numpy><pytorch-geometric>
"I'm trying to apply a retrained model of mobilenet_v2 presented in https://github.com/balajisrinivas/Face-Mask-Detection
The main part of my adapted code is the following:
faces = []
locs = []
preds = []

for coord in coords:

    x = person[0]
    y = person[1]
    w = coord [2]
    h = person[3]

    # extract the face ROI, convert it from BGR to RGB channel
    # ordering, resize it to 224x224, and preprocess it
    face = frame[y:y+h, x:x+h]
    face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)

    if (face.shape) &gt; (224, 224):
        face = cv.resize(face, (224, 224))
        
    face = img_to_array(face)
    face = preprocess_input(face) #from tensorflow.keras.applications.mobilenet_v2

    # add the face and bounding boxes to their respective lists
    faces.append(face)
    locs.append((x, y, w, h))

    # only make a predictions if at least one face was detected
    if len(faces) &gt; 0:
        # for faster inference we'll make batch predictions on *all*
        # faces at the same time rather than one-by-one predictions
        faces = np.array(faces, dtype=&quot;float32&quot;)
        preds = maskNet.predict(faces, batch_size=32) #maskNet = load_model(&quot;mask_detector.model&quot;)

The error is raised when I try to apply maskNet.predict, which is the is the mobilenet_v2 model saved in https://github.com/balajisrinivas/Face-Mask-Detection/blob/master/mask_detector.model.
The error message is the following: ValueError: Negative dimension size caused by subtracting 3 from 1 for '{{node model/Conv1/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=&quot;NHWC&quot;, dilations=[1, 1, 1, 1], explicit_paddings=[], padding=&quot;VALID&quot;, strides=[1, 2, 2, 1], use_cudnn_on_gpu=true](model/Conv1_pad/Pad, model/Conv1/Conv2D/ReadVariableOp)' with input shapes: [?,1,1,3], [3,3,3,32].
","I found out my mistake!
I shouldn't use a condition before resizing the frame:
face = cv.resize(face, (224, 224)) #without if (face.shape) &gt; (224, 224):

I've used the condition because I had to do that for SSD Resnet in the past, but I just realized it's not necessary for mobilenet_v2
",<machine-learning><keras><tensorflow><cnn><numpy>
"What happends in this numpy function:
https://numpy.org/doc/stable/reference/generated/numpy.unique.html
a = np.array([1, 2, 5, 3, 2])
u, indices = np.unique(a, return_inverse=True)

The results are:
u

array([1, 2, 3, 5)]
indices

array([0,1,3,2,1), dtype=int64)
u[indices]

array([1, 2, 5, 3, 1])
u are the single values inside this array. What is indices? why is it a 3 and not a 5? and what is going on in u[indices]?
","Firstly the function returns the variables u and indices:

u contains the unique elements sorted. In other words no element is repeated (the number 2 does not appear twice) and the elements will be listed from the smallest to the largest value
indices is the same size as a and basically it contains the index in u you should used to recover a. So when they give you [0,1,3,2,1], in this case the number 3 refers to the 3rd index of u = [1, 2, 3, 5] which in this case is 5

You can see how u and indices can be used to recover a by running the following code:
for element in indices:
      print(u[element])

This also explains your last question: u[indices] it basically gives you back a. Note that in your question u[indices] should return array([1, 2, 5, 3, 2]). I assume you made a typo
",<numpy>
"I have a query regarding Pandas data manipulation.
Let's say I have a dataframe, df with following structure.
A B C
1 1 7
5 3 3
3 3 2
7 5 2
5 NaN 2

We have 3 columns in the dataframe A, B &amp; C.
B column consists of mean values wrt A.
For example,
Value of B in 3rd row (which is 3) is mean of first 3 rows of A (9/3)
Similarly, value of B in 4th row = (Sum of values in 2nd,3rd and 4th row of A)/3
Now, let's say I have many NaN values in B and there are no NaN values in A, how do I write a function or code to fill the NaN values as per the logic discussed above?
I tried using loc and iloc but I guess I made some mistake.
","Thank you for the above answer!
That definitely works. However, I found a more efficient way in terms of computation using np.rolling
df['D'] = df['A'].rolling(min_periods=1, window=3).mean()
df['B'] = np.where(df['B'].isnull,df['D'],df['B'])

np.rolling helps to compute the cumulative sum of previous n values.
np.where helps to apply some output based on a condition: syntax: np.where(condition,
value if true, value if false).
Column D can be dropped once it is used.

",<pandas><data-cleaning><numpy>
"I have a query regarding Pandas data manipulation.
Let's say I have a dataframe, df with following structure.
A B C
1 1 7
5 3 3
3 3 2
7 5 2
5 NaN 2

We have 3 columns in the dataframe A, B &amp; C.
B column consists of mean values wrt A.
For example,
Value of B in 3rd row (which is 3) is mean of first 3 rows of A (9/3)
Similarly, value of B in 4th row = (Sum of values in 2nd,3rd and 4th row of A)/3
Now, let's say I have many NaN values in B and there are no NaN values in A, how do I write a function or code to fill the NaN values as per the logic discussed above?
I tried using loc and iloc but I guess I made some mistake.
","Assuming you don't have NaNs in the first two entries of column B, the following code works
index_nan = df.index[df['B'].isna()] #get all indices where B has NaNs

new_df = pd.DataFrame({'B': [np.mean(df['A'][i-2:i+1]) for i in index_nan]}, index=index_nan) 

df.update(new_df) #update those values of column B in df

",<pandas><data-cleaning><numpy>
"I'm trying to do a simple reshape of a 60000,28,28 list of mnist digits into a 60000,784 numpy array where the digits have been unrolled.
To do this the code is this:
(xdata,xlabel),(ydata,ylabel)=tf.keras.datasets.mnist.load_data()
newxdata=np.array([])
cnt=0
for i in xdata:
 tmpx=i.ravel()
 if cnt == 0:
  newxdata=np.concatenate((newxdata,tmpx))
 else:
  newxdata=np.vstack((newxdata,tmpx))
 cnt=cnt+1


Why does this take so long to run? Is there a way to speed it up? Ultimately the data will be fed into a keras model in smaller batches. Would writing a generator that does the loop unrolling when a batchsize is asked for be more performant or would it not make a difference?
","You can reshape a numpy array simply by:
newxdata =  xdata.reshape((60000,28*28))

for example. Or simply:
newxdata =  xdata.reshape((len(xdata),-1))

Note that reshape is a numpy function which can used also as:
import numpy as np
newxdata =  np.reshape(xdata, (60000,-1))

To speed up your loop you could alternatively use libraries like multiprocessing.Pool or CuPy or Numba.
",<keras><tensorflow><numpy>
"I'm trying to do a simple reshape of a 60000,28,28 list of mnist digits into a 60000,784 numpy array where the digits have been unrolled.
To do this the code is this:
(xdata,xlabel),(ydata,ylabel)=tf.keras.datasets.mnist.load_data()
newxdata=np.array([])
cnt=0
for i in xdata:
 tmpx=i.ravel()
 if cnt == 0:
  newxdata=np.concatenate((newxdata,tmpx))
 else:
  newxdata=np.vstack((newxdata,tmpx))
 cnt=cnt+1


Why does this take so long to run? Is there a way to speed it up? Ultimately the data will be fed into a keras model in smaller batches. Would writing a generator that does the loop unrolling when a batchsize is asked for be more performant or would it not make a difference?
","I went with that suggestion and had a few additions to the implementation.
Ultimately the newly transformed data would be ingested by a keras function that requires an input of ( batch_size, unrolled image), so i created a generator function .returnbatch(batch_size) that returns a ( batchsize, unrolledimage )
",<keras><tensorflow><numpy>
"Edit: the context is as follows: I've trained some ML model that predicts some feature vector. Thats a. But I know that a can take some values from discrete range, but the model outputs continuous values. After applying predict I want to enforce the outputs to take the discrete values they are allowed to be in. I've also edited my example to be more informative.
I have this continuous data:
a = [0.003 0.994 1.334 3.2 1.464 2.454 2.941 999.999] (outputs)
How can I easily convert it to the following discrete array:
b = [0 1 2 3 3] (allowed values)
so that every element of a will be mapped to its closest counterpart in b:
a_scaled = [0 1 1 3 1 2 3 3] (what I need)
I know how to implement this, but I want to stick with &quot;don't invent the wheel&quot;. So is there any nice function from some library that does this?
","Update Accordingly: Your question was not clear before, therefore, sorry for the irrelevant solution.
To achieve that, I don't think there is a public library function, however, you can build your solution using some beautiful &quot;ready&quot; functions.
Two solutions come into my mind:

First one's time complexity is O(N*M)

N is your prediction (list a in your case) size, and M is your dictionary (list b in your case) size.
import numpy as np

def findClosest(dictionary, value):
    idx = (np.abs(dictionary - value)).argmin()
    return dictionary[idx]

#[findClosest(b, elem) for elem in a] 

print([findClosest(b, elem) for elem in a])

This just subtracts your prediction value from the values in your dictionary and takes the absolute value of them. Then in the resulting array, it looks for the location of value that is smallest.

Second one's time complexity is O(N*log(M))

N and M denote the same thing as the first solution.
from bisect import bisect_left
def findClosestBinary(myList, myNumber):
    pos = bisect_left(myList, myNumber)
    if pos == 0:
        return myList[0]
    if pos == len(myList):
        return myList[-1]
    before = myList[pos - 1]
    after = myList[pos]
    if after - myNumber &lt; myNumber - before:
        return after
    else:
        return before

#[findClosestBinary(b, elem) for elem in a] 

print([findClosestBinary(b, elem) for elem in a])

Note: To save time I didn't implement myself but took the findClosestBinary() function from here.
This one is a better algorithmic approach in terms of time complexity. This does the same thing but uses a binary search to efficiently find the closest value in the dictionary.  However, it assumes your dictionary (list b) is sorted. Since your dictionary is a predefined list, you can improve the performance by providing a sorted array.
However, if your dictionary that you will map predictions to is not a very big one, then you can just use the first one. In the case of the dictionary being small, these two functions will behave the same in terms of time.
",<numpy><feature-scaling>
"I need to run a Random Forest process with scikit-learn. To train the model, I have a database table with 10 million rows of features. The question is: what is the best way to approach this, should I load into memory the 10 million rows, for example with numpy or pandas or there's a better way to load the data progressively by chunks?
","There are multiple possiblities from dusk, to others model etc.
Here are my 2 favorites, not to loose you in the number of possibilities:

www.h5py.org/ &quot;It lets you store huge amounts of numerical data, and easily manipulate that data from NumPy. For example, you can slice into multi-terabyte datasets stored on disk, as if they were real NumPy arrays. Thousands of datasets can be stored in a single file, categorized and tagged however you want.&quot;

Try online learning with Cousin models of random forest (light-gbm). He has online learning capabilities.


",<python><scikit-learn><pandas><numpy>
"How can I find the mean for each of the channels (RGB) across an array of images?
For example train_dataset[0]['image'].shape is (600, 800, 3) and len(train_dataset) is 720 meaning it includes 720 images of dimension 600x800 and 3 channels. train_dataset[0]['image'] is an ndarray.
I am looking to end up with 3 numbers each representing the mean for each of the channels across all these 720 images.
I have this very dumb solution but I wonder if there's a better solution?

I also did it this other way and got almost the same answer:

","Could probably be more elegant, but here's an idea:
mean[:, :, 0:2] = np.mean(imgs[:, :, :, 0:2], axis=0)

",<python><numpy><mean>
"I have a dataset with a message (string) and an associated mood. I am trying to use an ANN to predict one of the 6 moods using the encoded inputs.
This is how my X_train looks like:
array([list([1, 60, 2]),
   list([1, 6278, 14, 9137, 334, 9137, 8549, 1380, 7]),
   list([5, 107, 1, 2, 156]), ..., list([1, 2, 220, 41]),
   list([1, 2, 79, 137, 422, 877, 5, 230, 621, 18]),
   list([1, 11, 66, 1, 2, 9137, 175, 1, 6278, 5624, 1520])],
  dtype=object)

Since every array has a different length, it's not being accepted. What can I do about it?
PS: The encoded values were generated using keras.preprocessing.Tokenizer()
","I am not sure how well it will perform, but what about padding the shorter messages with special characters (let's say zeros) so that they are as long as the longest message?
However, some kind of embedding would definitely be better (also for the sake of the target - predicting the sentiment) if you have enough data.
",<deep-learning><numpy><ann>
"I have a dataset with a message (string) and an associated mood. I am trying to use an ANN to predict one of the 6 moods using the encoded inputs.
This is how my X_train looks like:
array([list([1, 60, 2]),
   list([1, 6278, 14, 9137, 334, 9137, 8549, 1380, 7]),
   list([5, 107, 1, 2, 156]), ..., list([1, 2, 220, 41]),
   list([1, 2, 79, 137, 422, 877, 5, 230, 621, 18]),
   list([1, 11, 66, 1, 2, 9137, 175, 1, 6278, 5624, 1520])],
  dtype=object)

Since every array has a different length, it's not being accepted. What can I do about it?
PS: The encoded values were generated using keras.preprocessing.Tokenizer()
","One way is to encode your input in a fixed size dimension. That is, you can use an RNN, like an LSTM, with padding and its output should be the input of an ANN.
",<deep-learning><numpy><ann>
"I am a very inexperienced programmer, this is my first question on the Data Science StackExchange, I sorry if it is formatted poorly or comes across as basic. For some strange reason, in Python, whenever I try to run a correlation function on the population density &amp; total cases per million columns of my COVID-19 DataFrame (which I imported/read into Spyder as a csv), I keep getting the same long error message, namely, &quot;ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().&quot; regardless of whether I use the correlation function from Pandas or Numpy.
My first thought was that this error was caused by the presence of null values in those columns, so I used df.dropna(), then ran the correlation function again but I got the same &quot;ValueError&quot;, so I have no idea what is going on here, I was able to run a correlation on those same columns just fine in RStudio which I am equally unskilled and inexperienced with.

","You should look at the documentation.  You do not pass the column names as an argument.
subset_df = df[['col1', 'col2']]
subset_df.corr()

That should solve this for you.
",<python><pandas><correlation><numpy><ipython>
"I have a data set I loaded with cv2, but when I try to format it I get the above error. I start by moving the data into X_train and X_test (the loaded data is in x_train and x_test).
X_train = []
X_test = []

# Image matrices are different sizes so I am making them the same size
for i in range(len(x_train)-1):
  resized = cv2.resize(x_train[i], (img_width, img_height))
  X_train.append(resized)
for i in range(len(x_test)-1):
  resized = cv2.resize(x_test[i], (img_width, img_height))
  X_test.append(resized)

# Convert to numpy arrays
X_test = np.array(X_test)
X_train = np.array(X_train)

# Gather statistics 
print(X_train.shape)    # -&gt; (2734, 132, 126, 3)
print(X_train.size)     # -&gt; 136415664
print(len(X_train))     # -&gt; 2734

# Convert to black and white
X_train = X_train/ 255.
X_test = X_test/ 255.

# First line throws error
X_train = np.reshape(X_train, (len(X_train), img_height, img_width, 1))
X_test = np.reshape(X_test, (len(X_test), img_height, img_width, 1))

What am I doing wrong?
","You need $2734 \times 132\times 126\times 1=45,471,888$ values in order to reshape into that tensor. Since you have $136,415,664$ values, the reshaping is impossible. If your fourth dimension is $4$, then the reshape will be possible.
",<numpy>
"I have written the following code for a neural network to perform regression on a dataset, but I am getting a ValueError. I have looked up to different answers and they suggested to use df = df.values to get a numpy array. I tried it but it still produced the same error. How to fix this?
CODE
from keras import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.optimizers import Adam
from sklearn.model_selection import train_test_split

#Define Features and Label
features = ['posted_by', 'under_construction', 'rera', 'bhk_no.', 'bhk_or_rk',
            'square_ft', 'ready_to_move', 'resale', 'longitude',
            'latitude'] 

X=train[features].values
y=train['target(price_in_lacs)'].values

#Train Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 23, shuffle = True)

#Model
model = Sequential()
model.add(Dense(10, activation='relu', kernel_initializer='random_normal', input_dim = 10))
model.add(Dense(1, activation = 'relu', kernel_initializer='random_normal'))

#Compiling the neural network
model.compile(optimizer = Adam(learning_rate=0.1) ,loss='mean_squared_logarithmic_error', metrics =['mse'])

#Fitting the data to the training dataset  
model.fit(X_train,y_train, batch_size=256, epochs=100, verbose=0)

ERROR
ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type int).

","Before fitting the model the training data write the below 2 lines:
X_train=np.asarray(X_train).astype(np.int)

y_train=np.asarray(y_train).astype(np.int)

",<python><neural-network><keras><pandas><numpy>
"I have written the following code for a neural network to perform regression on a dataset, but I am getting a ValueError. I have looked up to different answers and they suggested to use df = df.values to get a numpy array. I tried it but it still produced the same error. How to fix this?
CODE
from keras import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.optimizers import Adam
from sklearn.model_selection import train_test_split

#Define Features and Label
features = ['posted_by', 'under_construction', 'rera', 'bhk_no.', 'bhk_or_rk',
            'square_ft', 'ready_to_move', 'resale', 'longitude',
            'latitude'] 

X=train[features].values
y=train['target(price_in_lacs)'].values

#Train Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 23, shuffle = True)

#Model
model = Sequential()
model.add(Dense(10, activation='relu', kernel_initializer='random_normal', input_dim = 10))
model.add(Dense(1, activation = 'relu', kernel_initializer='random_normal'))

#Compiling the neural network
model.compile(optimizer = Adam(learning_rate=0.1) ,loss='mean_squared_logarithmic_error', metrics =['mse'])

#Fitting the data to the training dataset  
model.fit(X_train,y_train, batch_size=256, epochs=100, verbose=0)

ERROR
ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type int).

","To convert numpy array to tensor,
import tensor as tf
#Considering y variable holds numpy array
y_tensor = tf.convert_to_tensor(y, dtype=tf.int64) 

#You can use any of the available datatypes that suits best - https://www.tensorflow.org/api_docs/python/tf/dtypes/DType
",<python><neural-network><keras><pandas><numpy>
"I have written the following code for a neural network to perform regression on a dataset, but I am getting a ValueError. I have looked up to different answers and they suggested to use df = df.values to get a numpy array. I tried it but it still produced the same error. How to fix this?
CODE
from keras import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.optimizers import Adam
from sklearn.model_selection import train_test_split

#Define Features and Label
features = ['posted_by', 'under_construction', 'rera', 'bhk_no.', 'bhk_or_rk',
            'square_ft', 'ready_to_move', 'resale', 'longitude',
            'latitude'] 

X=train[features].values
y=train['target(price_in_lacs)'].values

#Train Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 23, shuffle = True)

#Model
model = Sequential()
model.add(Dense(10, activation='relu', kernel_initializer='random_normal', input_dim = 10))
model.add(Dense(1, activation = 'relu', kernel_initializer='random_normal'))

#Compiling the neural network
model.compile(optimizer = Adam(learning_rate=0.1) ,loss='mean_squared_logarithmic_error', metrics =['mse'])

#Fitting the data to the training dataset  
model.fit(X_train,y_train, batch_size=256, epochs=100, verbose=0)

ERROR
ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type int).

","It maybe due to the fact that some of your columns may not have complete integer values, before fitting you should convert it
X = np.asarray(X).astype(np.int_)
Y = np.array(Y).astype(np.int_)

",<python><neural-network><keras><pandas><numpy>
"I have written the following code for a neural network to perform regression on a dataset, but I am getting a ValueError. I have looked up to different answers and they suggested to use df = df.values to get a numpy array. I tried it but it still produced the same error. How to fix this?
CODE
from keras import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.optimizers import Adam
from sklearn.model_selection import train_test_split

#Define Features and Label
features = ['posted_by', 'under_construction', 'rera', 'bhk_no.', 'bhk_or_rk',
            'square_ft', 'ready_to_move', 'resale', 'longitude',
            'latitude'] 

X=train[features].values
y=train['target(price_in_lacs)'].values

#Train Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 23, shuffle = True)

#Model
model = Sequential()
model.add(Dense(10, activation='relu', kernel_initializer='random_normal', input_dim = 10))
model.add(Dense(1, activation = 'relu', kernel_initializer='random_normal'))

#Compiling the neural network
model.compile(optimizer = Adam(learning_rate=0.1) ,loss='mean_squared_logarithmic_error', metrics =['mse'])

#Fitting the data to the training dataset  
model.fit(X_train,y_train, batch_size=256, epochs=100, verbose=0)

ERROR
ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type int).

","As X_train and y_train are pandas.core.series.Series they can't be parsed.
Try converting them to list as below:
X=train[features].to_list()

y=train['target(price_in_lacs)'].to_list()

",<python><neural-network><keras><pandas><numpy>
"I want to generate a grid search for which I need the scoring parameter based on which the search will take place. I have defined the following function to provide me a Root Mean Squared Logarithmic Error. But I feel that the scorer is considering the greater value to be a better score, whereas it should consider the lower value as better score. Please let me know if I have defined a correct scorer.
Function for RMSLE
def score_func(y_true, y_pred, **kwargs):
  y_true = np.abs(y_true)
  y_pred = np.abs(y_pred)

  return np.sqrt(mean_squared_log_error(y_true, y_pred))

scorer = make_scorer(score_func)

I had to use np.abs in the above code, otherwise it was giving an error that RMSLE cannot be used when Target has negative values.
","The make_scorer function takes a greater_is_better parameter (bool, default=True) which defines whether 'high is good' or 'low is good'. Changing its value to False might solve the issue you are having.
",<machine-learning><python><scikit-learn><numpy>
"I want to generate a grid search for which I need the scoring parameter based on which the search will take place. I have defined the following function to provide me a Root Mean Squared Logarithmic Error. But I feel that the scorer is considering the greater value to be a better score, whereas it should consider the lower value as better score. Please let me know if I have defined a correct scorer.
Function for RMSLE
def score_func(y_true, y_pred, **kwargs):
  y_true = np.abs(y_true)
  y_pred = np.abs(y_pred)

  return np.sqrt(mean_squared_log_error(y_true, y_pred))

scorer = make_scorer(score_func)

I had to use np.abs in the above code, otherwise it was giving an error that RMSLE cannot be used when Target has negative values.
","Another option is to import the scikit-learn version to avoid bugs.
from sklearn.metrics import mean_squared_log_error as msle

msle(y_true, y_pred, squared=False) # Calculate RMSLE

",<machine-learning><python><scikit-learn><numpy>
"When I subtract these two arrays, it returns a (354,354) shaped array because as per the documentation there is a mismatch in the shape of the arrays.
Why does it happen and what else can I do for this except numpy.reshape ?

","This is a problem that I have also run into before, right now your ytrain is a one dimensional array (advisable to avoid). Check this answer.
expanding(adding) additional dimension while assigning ytrain should fix your problem
x = np.array([1, 2])
x.shape
(2,)
y = np.expand_dims(x, axis=1)
y
array([[1],
       [2]])
y.shape
(2, 1)

",<numpy>
"I came to data science/machine learning from another background in computer science and i feel that i'm lacking of experience with matricial/vectorial operations.
Python or Matlab, for instance, provide awesome features like numpy to easily manipulate tabular data. However, the first solution that comes to my mind when coding is for or while loops. I've faced situations where my code could be easily reduced to 1 line or  so with numpy.
I'd like to know if there are good lectures/books you can recommend to change this way of thinking when it comes to problem solving/coding approaches.
Thanks in advance.
","tl;dr: refresh your linear algebra and check for built-in functions.
In addition to brevity/elegance, vectorization often dramatically improves performance. Tools like MATLAB and numpy use high-level vectorized wrappers for low-level optimized implementations, which in the end are composed of your traditional loop operations. Where MATLAB and Python are interpreted (nuances exist, but are irrelevant here), the low-level subroutines that they wrap are obviously much faster. Also, tools that they wrap, like BLAS (Basic Linear Algebra Subprograms), are so heavily optimized that it'd be difficult for someone write a low-level subroutine that matches or exceeds the performance achieved with such libraries; many people over many decades worked to optimize them!
Considering problems from a math-oriented perspective helps with vectorization. For example, say you wanted to find the $L_p$ norm of a vector $\vec{x}$. A traditional CS program would look something like:
norm = 0
for i in vec_x:
    norm = norm + pow(i,p)
norm = pow(norm,1/p)

but a mathematician would simply write $||\vec{x}||_p$. First, you should check to see if a built-in function exists to perform your task, as a mathematician might. Then you should look to vectorize using math operators such as matrix multiplication, element-wise operations, etc., which these high-level tools implement. Going back to the $L_p$ example, you could write vectorized code (in MATLAB): (sum(x.^p))^(1/p), but it'd be even better to write norm(x,p). Knowing that such functions exist is a combination of familiarity with the tool and Googling.
",<machine-learning><python><numpy><matlab><matrix>
"Say I have a data frame in Python.
If I want to replace all values in the say, the Size column that are not 'M' or 'S' or 'L' with nan, how do I do so?
Thanks in advance
","I don't know if it's the best way, but I'd write a function and apply it to the df['size']:
def rename_size(size):
    if size not in ['M','S','L']:
        return np.nan
    else:
        return size
df['size']= df['size'].apply(rename_size)

",<pandas><numpy>
"import numpy as np 
demo_matrix=np.array(([13,35.74,48],[23,37,37,38],[73,39,93,39]))

demo_matrix[2,3]

I'm getting Index Error when I tried to run this code.
Can anyone help?
Thanks in advance
","There are two ways to correct this

you passed three lists to np.array, and there are not all same size

demo_matrix = np.array(([13,35,74,48],[23,37,37,38],[73,39,93,39]))
demo_matrix[2,3] 
#demo_matrix (list of lists)
array([[13, 35, 74, 48],
       [23, 37, 37, 38],
       [73, 39, 93, 39]])

you used 35.74, I guess it might be typing mistake

If you think you are passing list values correctly
use below code
demo_matrix[2][3]


if passed lists have different sizes
then output array will be array of lists
array([list([13, 35.74, 48]), list([23, 37, 37, 38]),
       list([73, 39, 93, 39])], dtype=object)

",<python><numpy>
"I am trying to check if there is a correlation between spam emails and weekdays.
My dataset looks like as follows:
  Spam? Day
0   1.0 Saturday
1   1.0 Saturday
3   0.0 Saturday
5   1.0 Saturday
7   0.0 Friday
... ... ...
346 0.0 Friday
348 1.0 Friday
361 0.0 Saturday
383 1.0 Thursday
387 1.0 Friday

where 1 means spam and 0 not spam.
I have tried as follows
corr = (numpy.corrcoef(df['Spam?'],df['Days']))

I do not know how to explain a possible relationship between these two variables and if a plot could help to better visualise data and relationship.
","(started as a comment but it turned out to be longer than expected)
With a dataset like this a simple barplot could be very insightful: on the X axis the days of the week, on the Y axis the frequency, with two bars (spam/not spam using different color) for each day. A slightly more advanced version: two boxplots, one for weekdays the other for weekends. A boxplot is kind of overkill for only 5 (mon-fri) and 2 (sat-sun) values but it's easy to do and shows the big picture.
In order to test whether any difference (e.g. weekdays vs weekends) is significant I think this is a good case for a chi square test.
",<python><correlation><numpy>
"I am trying to check if there is a correlation between spam emails and weekdays.
My dataset looks like as follows:
  Spam? Day
0   1.0 Saturday
1   1.0 Saturday
3   0.0 Saturday
5   1.0 Saturday
7   0.0 Friday
... ... ...
346 0.0 Friday
348 1.0 Friday
361 0.0 Saturday
383 1.0 Thursday
387 1.0 Friday

where 1 means spam and 0 not spam.
I have tried as follows
corr = (numpy.corrcoef(df['Spam?'],df['Days']))

I do not know how to explain a possible relationship between these two variables and if a plot could help to better visualise data and relationship.
","numpy.corrcoef will gives you Pearson Correlation but your Features are Categorical.
You should calculate Crammer'v.

You can get details/code in this answer as both the questions are a bit similar DS.SE
On Plot
 What Erwan has suggested seems good.
Also, try to plot(line-plot) between days-of-week and spam/total ratio(i.e. Stanardizing for total volume) since a single figure is easier to comprehend.
",<python><correlation><numpy>
"How do I get a cosine to peak up to a certain value as it hits 0? With the above code, the y-value peaks to infinity as it approaches 0.
import matplotlib.pyplot as plt
import numpy as np

amp = 1
time = np.linspace(-5, 5, 1000)

signal = np.cos(1 * np.pi * time) * abs(1/time)

plt.figure(figsize=(10, 10))
plt.ylim(-4, 4)
plt.plot(time, signal, color=&quot;black&quot;)
plt.savefig('output.png', bbox_inches='tight', pad_inches=0,
            transparent=True)

Put another way, based on my google-fu, how can I create a symmetrical exponentially decaying sinusoid with a maximum y-value as it approaches 0?
","$\alpha e^{-\beta x^2}cos(x)$ where $\alpha$ is your peak and $\beta$ is proportional to the strength of the decay
This one's slightly longer and the peak is at $\alpha+1$ but the graph is easier to manipulate:

$cos(x)e^{-\beta x^2}+\alpha e^{-x^2}$
",<numpy>
"I was trying to implement Logistic Regression from scratch in python to learn better how it works under the hood. In particular I am following this video tutorial from Andrew Ng.
This is the dataset I am using for testing the algorithm: marks.txt
I've found that without normalizing the data, the algorithm does not converge and the loss is not decreasing (sometimes it is a NaN).
This is the implementation:
import numpy as np
import pandas as pd


def normalize(X, axis=0):
    return (X - np.mean(X, axis=axis)) / np.std(X, axis=axis)


class LogisticRegression():
    def __init__(self, num_epochs=10000, lr=0.001 ):
        self.num_epochs = num_epochs
        self.lr = lr

    def __sigmoid(self, Z):
        return 1 / (1 + np.exp(-Z))

    def __loss(self, y, A):
        return - np.mean(y * np.log(A) + (1 - y) * np.log(1 - A))


    def fit(self, X, y):
        n, m = X.shape
        print(f'Number of features = {n}')
        print(f'Number of samples = {m}')

        W = np.zeros((n, 1))
        b = 0

        for epoch in range(1, self.num_epochs + 1):
            Z = np.dot(W.T, X) + b
            A = self.__sigmoid(Z)

            dZ = A - y
            dW = 1/m * np.dot(X, dZ.T)
            db = 1/m * np.sum(dZ)

            W -= self.lr * dW
            b -= self.lr * db

            if epoch == 1 or epoch % 100 == 0:
                J = self.__loss(y, A)
                print(f'Epoch {epoch} - Loss = {J}')


columns = [
    'mark_1',
    'mark_2',
    'y'
]

data = pd.read_csv('marks.txt', names=columns, header=None)

X = data.iloc[:, :-1].values
y = data.iloc[:, -1:].values

lr = LogisticRegression(num_epochs=10000, lr=0.01)
lr.fit(X.T, y.T)

If I execute this, I got the following output:
Number of features = 2
Number of samples = 100
Epoch 1 - Loss = 0.6931471805599453
Epoch 100 - Loss = nan
Epoch 200 - Loss = 4.804976603222295
Epoch 300 - Loss = 7.859811065112183
Epoch 400 - Loss = nan
Epoch 500 - Loss = 4.7897185742553186
Epoch 600 - Loss = 7.836867515204696
Epoch 700 - Loss = nan
Epoch 800 - Loss = 4.774454897975551
Epoch 900 - Loss = 7.813880674612202
Epoch 1000 - Loss = nan
Epoch 1100 - Loss = 4.759205019552172
Epoch 1200 - Loss = 7.790844866695895
Epoch 1300 - Loss = nan
Epoch 1400 - Loss = 4.743971469023722
Epoch 1500 - Loss = 7.7677542901506875
Epoch 1600 - Loss = nan
Epoch 1700 - Loss = 4.728752106484584
Epoch 1800 - Loss = 7.744603001274255
Epoch 1900 - Loss = nan
Epoch 2000 - Loss = 4.713554131813897
Epoch 2100 - Loss = 7.721384895223854
Epoch 2200 - Loss = nan
Epoch 2300 - Loss = 4.698361852011675
Epoch 2400 - Loss = 7.698093686352158
Epoch 2500 - Loss = nan
Epoch 2600 - Loss = 4.683196015386
Epoch 2700 - Loss = 7.674722887733128
Epoch 2800 - Loss = nan
Epoch 2900 - Loss = 4.6680544815204925
Epoch 3000 - Loss = 7.6512657900116805
Epoch 3100 - Loss = nan
Epoch 3200 - Loss = 4.65294492110793
Epoch 3300 - Loss = 7.627715439737382
Epoch 3400 - Loss = nan
Epoch 3500 - Loss = 4.637870088341966
Epoch 3600 - Loss = 7.604064617373076
Epoch 3700 - Loss = nan
Epoch 3800 - Loss = 4.62281379407897
Epoch 3900 - Loss = 7.580305815203287
Epoch 4000 - Loss = nan
Epoch 4100 - Loss = 4.60781250038029
Epoch 4200 - Loss = 7.556431215405509
Epoch 4300 - Loss = nan
Epoch 4400 - Loss = 4.592835472351133
Epoch 4500 - Loss = 7.532432668589291__main__:19: RuntimeWarning: divide by zero encountered in log
__main__:19: RuntimeWarning: invalid value encountered in multiply

Epoch 4600 - Loss = nan
Epoch 4700 - Loss = 4.57789045326783
Epoch 4800 - Loss = 7.508301673152992
Epoch 4900 - Loss = nan
Epoch 5000 - Loss = 4.563010160089178
Epoch 5100 - Loss = 7.48402935585585
Epoch 5200 - Loss = nan
Epoch 5300 - Loss = 4.548178514140011
Epoch 5400 - Loss = 7.459606454052721
Epoch 5500 - Loss = nan
Epoch 5600 - Loss = 4.533383810118562
Epoch 5700 - Loss = 7.4350233000888215
Epoch 5800 - Loss = nan
Epoch 5900 - Loss = 4.518654642394596
Epoch 6000 - Loss = 7.4102698084014715
Epoch 6100 - Loss = nan
Epoch 6200 - Loss = 4.503979863783454
Epoch 6300 - Loss = 7.385335465922019
Epoch 6400 - Loss = nan
Epoch 6500 - Loss = 4.489373494021938
Epoch 6600 - Loss = 7.3602093264129
Epoch 6700 - Loss = nan
Epoch 6800 - Loss = 4.474823890959029
Epoch 6900 - Loss = 7.334880009407734
Epoch 7000 - Loss = nan
Epoch 7100 - Loss = 4.46034588627989
Epoch 7200 - Loss = 7.309335704445365
Epoch 7300 - Loss = nan
Epoch 7400 - Loss = 4.445943673235942
Epoch 7500 - Loss = 7.283564181295987
Epoch 7600 - Loss = nan
Epoch 7700 - Loss = 4.431627025808597
Epoch 7800 - Loss = 7.257552806867497
Epoch 7900 - Loss = nan
Epoch 8000 - Loss = 4.4174006755844095
Epoch 8100 - Loss = 7.231288569447483
Epoch 8200 - Loss = nan
Epoch 8300 - Loss = 4.403243132948716
Epoch 8400 - Loss = 7.2047581108777345
Epoch 8500 - Loss = nan
Epoch 8600 - Loss = 4.3891854547270475
Epoch 8700 - Loss = 7.177947767170446
Epoch 8800 - Loss = nan
Epoch 8900 - Loss = 4.3752278129468944
Epoch 9000 - Loss = 7.150843617955713
Epoch 9100 - Loss = nan
Epoch 9200 - Loss = 4.361364188554799
Epoch 9300 - Loss = 7.123431544995584
Epoch 9400 - Loss = nan
Epoch 9500 - Loss = 4.34760573682999
Epoch 9600 - Loss = 7.095697299812474
Epoch 9700 - Loss = nan
Epoch 9800 - Loss = 4.333968348729086
Epoch 9900 - Loss = 7.067626580257764
Epoch 10000 - Loss = nan

Otherwise, if I normalize the data (zero mean unit variance) before fitting the model, it seems to work correctly (I can see the loss decreasing):
def normalize(X, axis=0):
    return (X - np.mean(X, axis=axis)) / np.std(X, axis=axis)

# [...]

data = pd.read_csv('data/marks.csv', names=columns, header=0)

X = data.iloc[:, :-1].values
y = data.iloc[:, -1:].values

X = normalize(X)  # Normalize data  

lr = LogisticRegression(num_epochs=10000, lr=0.01)
lr.fit(X.T, y.T)

Output:
Number of features = 2
Number of samples = 100
Epoch 1 - Loss = 0.6931471805599453
Epoch 100 - Loss = 0.5733935847364559
Epoch 200 - Loss = 0.4967653811151946
Epoch 300 - Loss = 0.4456019909522728
Epoch 400 - Loss = 0.4094643825544129
Epoch 500 - Loss = 0.38268993233906584
Epoch 600 - Loss = 0.3620711093033572
Epoch 700 - Loss = 0.34569287144258726
Epoch 800 - Loss = 0.33235335562007345
Epoch 900 - Loss = 0.3212643235482712
Epoch 1000 - Loss = 0.3118887927384652
Epoch 1100 - Loss = 0.3038488197031603
Epoch 1200 - Loss = 0.29687082702689194
Epoch 1300 - Loss = 0.29075191833313574
Epoch 1400 - Loss = 0.28533840424281665
Epoch 1500 - Loss = 0.28051169317796093
Epoch 1600 - Loss = 0.2761787696062344
Epoch 1700 - Loss = 0.27226561328471377
Epoch 1800 - Loss = 0.2687125532958896
Epoch 1900 - Loss = 0.2654709248024843
Epoch 2000 - Loss = 0.2625006214821513
Epoch 2100 - Loss = 0.25976827555417215
Epoch 2200 - Loss = 0.25724588518032776
Epoch 2300 - Loss = 0.25490976581258473
Epoch 2400 - Loss = 0.2527397395020597
Epoch 2500 - Loss = 0.2507185013239025
Epoch 2600 - Loss = 0.24883111923883924
Epoch 2700 - Loss = 0.24706463561630515
Epoch 2800 - Loss = 0.24540774701833637
Epoch 2900 - Loss = 0.24385054481312596
Epoch 3000 - Loss = 0.24238430349560172
Epoch 3100 - Loss = 0.24100130673780307
Epoch 3200 - Loss = 0.23969470351293196
Epoch 3300 - Loss = 0.2384583883670421
Epoch 3400 - Loss = 0.23728690121408835
Epoch 3500 - Loss = 0.23617534301823132
Epoch 3600 - Loss = 0.23511930448367577
Epoch 3700 - Loss = 0.23411480545586144
Epoch 3800 - Loss = 0.23315824319134465
Epoch 3900 - Loss = 0.2322463480086729
Epoch 4000 - Loss = 0.23137614511221863
Epoch 4100 - Loss = 0.23054492160266965
Epoch 4200 - Loss = 0.2297501978647279
Epoch 4300 - Loss = 0.22898970266443042
Epoch 4400 - Loss = 0.22826135140291767
Epoch 4500 - Loss = 0.22756322706622129
Epoch 4600 - Loss = 0.22689356348620354
Epoch 4700 - Loss = 0.22625073058962492
Epoch 4800 - Loss = 0.22563322136316274
Epoch 4900 - Loss = 0.22503964030418896
Epoch 5000 - Loss = 0.22446869316192905
Epoch 5100 - Loss = 0.22391917780259743
Epoch 5200 - Loss = 0.22338997605632013
Epoch 5300 - Loss = 0.2228800464239599
Epoch 5400 - Loss = 0.22238841753904443
Epoch 5500 - Loss = 0.2219141822944289
Epoch 5600 - Loss = 0.22145649255554345
Epoch 5700 - Loss = 0.22101455439246653
Epoch 5800 - Loss = 0.22058762377191163
Epoch 5900 - Loss = 0.22017500265778595
Epoch 6000 - Loss = 0.21977603547546334
Epoch 6100 - Loss = 0.21939010590048816
Epoch 6200 - Loss = 0.21901663393723186
Epoch 6300 - Loss = 0.21865507325717176
Epoch 6400 - Loss = 0.2183049087700573
Epoch 6500 - Loss = 0.21796565440434787
Epoch 6600 - Loss = 0.21763685107601996
Epoch 6700 - Loss = 0.2173180648272089
Epoch 6800 - Loss = 0.2170088851182173
Epoch 6900 - Loss = 0.2167089232582348
Epoch 7000 - Loss = 0.2164178109617001
Epoch 7100 - Loss = 0.21613519901863631
Epoch 7200 - Loss = 0.21586075606851618
Epoch 7300 - Loss = 0.21559416746830468
Epoch 7400 - Loss = 0.21533513424628176
Epoch 7500 - Loss = 0.2150833721340996
Epoch 7600 - Loss = 0.21483861067028356
Epoch 7700 - Loss = 0.2146005923690525
Epoch 7800 - Loss = 0.21436907194893295
Epoch 7900 - Loss = 0.21414381561617263
Epoch 8000 - Loss = 0.21392460039843075
Epoch 8100 - Loss = 0.21371121352464903
Epoch 8200 - Loss = 0.21350345184738453
Epoch 8300 - Loss = 0.213301121304228
Epoch 8400 - Loss = 0.21310403641523415
Epoch 8500 - Loss = 0.21291201981356775
Epoch 8600 - Loss = 0.21272490180681325
Epoch 8700 - Loss = 0.2125425199666215
Epoch 8800 - Loss = 0.21236471874456622
Epoch 8900 - Loss = 0.21219134911226462
Epoch 9000 - Loss = 0.21202226822398146
Epoch 9100 - Loss = 0.21185733910008378
Epoch 9200 - Loss = 0.21169643032984978
Epoch 9300 - Loss = 0.21153941579225452
Epoch 9400 - Loss = 0.21138617439347157
Epoch 9500 - Loss = 0.211236589819924
Epoch 9600 - Loss = 0.21109055030581808
Epoch 9700 - Loss = 0.21094794841416828
Epoch 9800 - Loss = 0.2108086808304075
Epoch 9900 - Loss = 0.21067264816773867
Epoch 10000 - Loss = 0.21053975478345366

I can see two possible reasons:

My implementation is wrong.
Normalization is mandatory to make the algorithm working properly (but I have read here that it is only recommended, not mandatory).

Could anyone give an hint if there are issues in the implementation or any other consideration why the algorithm is not working without normalization?
","Normalization/Standardization is suggested because it makes the convergence easy and faster.
Andrew Ng has explained the process and the reason with a bowl shape 2-dimensional loss space in his course.
If you are not doing that, you should be very slow with your LR and spend a lot more epochs.
I found this combination working
lr = LogisticRegression(num_epochs=500000, lr=0.001)
",<python><logistic-regression><numpy><normalization>
"I have a matrix of 200d vectors stored as follows:
$ X = \begin{pmatrix} \text{id}_1 &amp; 0.5 &amp; -2 &amp; \dots &amp; 10
                \\    \text{id}_2 &amp; -4 &amp; 6 &amp; \dots &amp; -0.3 
                \\     \vdots &amp; \vdots &amp; \ddots &amp; \vdots 
                 \\    \text{id}_{N} &amp; 0.3 &amp; 6 &amp; \dots &amp; -0.1 \end{pmatrix}$
The id is a number that identifies what the vector refers to in the particular dataset. Ideally, I would like to construct a new datastructure $Y$ such that I can supply two IDs $\text{id}_i$ and $\text{id}_j$ and find the cosine similarity between the corresponding vectors.
In case it is important, $N=8000$ for this dataset.
My original plan was to use sklearn's cosine_similarity function to return a matrix of similarities. However, I don't see how I will be able to keep the ID tages if I do that. Any suggestions?
","This will create a matrix. Rows/Cols represent the IDs.
You can check the result like a lookup table

i.e. (ID1, ID91) - Can look at either (0,90) Or (90, 0)

import numpy as np, pandas as pd
from numpy.linalg import norm

x = np.random.random((8000,200))
cosine = np.zeros((200,200))

for i in range(200):
    for j in range(200):
        c_tmp = np.dot(x[i], x[j])/(norm(x[i])*norm(x[j])) 
        cosine[i][j] = c_tmp

pd.DataFrame(cosine)


 Not an optimized code.
",<pandas><data><numpy><cosine-distance>
"In the network (model of Keras, Sequential), the input layer must have 4 neurons. The input must be 1 list, the length of which is 4, each element is a number.
print(&quot;SHAPE:&quot;, np.array([1, 1, 1, 1]).shape)

self.model.fit(np.array([1, 1, 1, 1]),
self.rightAnswer, 
epochs = 1,
batch_size = 1)

Here is the conclusion:
SHAPE: (4,)

ValueError: Error when checking input: expected dense_1_input to have shape (4,) but got array with shape (1,)

Why is this happening, and how can I fix it?
","Keras needs you to pass one more dimension than it says in the error message: the batch dimension.
I.e.
If you have a model that requires each sample to have an input shape of (4,) and you have 1000 training samples you need to feed it with an array of (1000, 4).
In your case since you want to feed it with just one sample you need to pass a shape of (1, 4). Try self.model.fit(np.array([[1, 1, 1, 1]])) (notice the one extra bracket with makes the array's shape be (1, 4)) .
",<python><neural-network><keras><numpy>
"Hey I have the following Dataset
import pandas as pd
df = pd.DataFrame({    
'column1': [0,0,1,0,1,0,0,1,1,0,1,1,1]})

I want to be able to count the number of consecutive 1 and 0 and generate 2 columns as such:
consec0: 1,2,_,1,_,1,2,_,_,1,_,_,_
consec1: _,_,1,_,1,_,_,1,2_,1,2,3

I then want to take the max number of consecutive and create two lists:
max_consec0: 2,1,2,1
max_consec1: 1,1,2,3

My dataset in the end will be just max_consec0 and max_consec1
","For this sort of problem you can use np.where and multiple boolean expressions to get your answer.
#1 We need to test if the column is equal to your target values, 0 or 1
second, we have to ensure the column value is not equal to the column value above.
3rd, for any value that is not equal to our input value we return a nan as its easier to use with numeric values.
import numpy as np

df['col2'] = np.where(
    df[&quot;column1&quot;].eq(0),
    df.groupby(df.column1.ne(df.column1.shift()).cumsum()).cumcount() + 1,
    np.nan,
)

df['col3'] = np.where(
    df[&quot;column1&quot;].eq(1),
    df.groupby(df.column1.ne(df.column1.shift()).cumsum()).cumcount() + 1,
    np.nan,
)


print(df)

   column1  col2  col3
0         0   1.0   NaN
1         0   2.0   NaN
2         1   NaN   1.0
3         0   1.0   NaN
4         1   NaN   1.0
5         0   1.0   NaN
6         0   2.0   NaN
7         1   NaN   1.0
8         1   NaN   2.0
9         0   1.0   NaN
10        1   NaN   1.0
11        1   NaN   2.0
12        1   NaN   3.0

we need to create a surrogate key for each group of numbers before a blank row and take the max of each group.
df.assign(
    key1=df.groupby(df[&quot;col2&quot;].isnull())[&quot;col2&quot;].transform(&quot;cumcount&quot;).cumsum()
).groupby(&quot;key1&quot;)[&quot;col2&quot;].max().dropna()

[2.0, 1.0, 2.0, 1.0]

df.assign(
    key2=df.groupby(df[&quot;col3&quot;].isnull())[&quot;col3&quot;].transform(&quot;cumcount&quot;).cumsum()
).groupby(&quot;key2&quot;)[&quot;col3&quot;].max().dropna().tolist()

[1.0, 1.0, 2.0, 3.0]

",<python><pandas><preprocessing><numpy>
"Hey I have the following Dataset
import pandas as pd
df = pd.DataFrame({    
'column1': [0,0,1,0,1,0,0,1,1,0,1,1,1]})

I want to be able to count the number of consecutive 1 and 0 and generate 2 columns as such:
consec0: 1,2,_,1,_,1,2,_,_,1,_,_,_
consec1: _,_,1,_,1,_,_,1,2_,1,2,3

I then want to take the max number of consecutive and create two lists:
max_consec0: 2,1,2,1
max_consec1: 1,1,2,3

My dataset in the end will be just max_consec0 and max_consec1
","To check if a value has changed, you can use .diff and check if it's non-zero with .ne(0) (the NaN in the top will be considered different than zero), and then count the changes with .cumsum, like this:
df['counter'] = df.diff().ne(0).cumsum()

Afterward, you can create a second dataframe, where the indices are the groups of consecutive values, and the column values are the value (0 or 1, in your case) and length (which is what you ultimately want):
df2 = df.groupby('counter')['column1'].min().to_frame(name='value').join(
df.groupby('counter')['column1'].count().rename('number'))

The resulting max_consec0, max_consec1 are just the values in the [number] column, filtered by the [value] column:
max_consec0 = df2[df2['value']==0]['number'].tolist()
max_consec1 = df2[df2['value']==1]['number'].tolist()

You can verify that the result is [2, 1, 2, 1] and [1, 1, 2, 3], as desired.
",<python><pandas><preprocessing><numpy>
"Hey I have the following Dataset
import pandas as pd
df = pd.DataFrame({    
'column1': [0,0,1,0,1,0,0,1,1,0,1,1,1]})

I want to be able to count the number of consecutive 1 and 0 and generate 2 columns as such:
consec0: 1,2,_,1,_,1,2,_,_,1,_,_,_
consec1: _,_,1,_,1,_,_,1,2_,1,2,3

I then want to take the max number of consecutive and create two lists:
max_consec0: 2,1,2,1
max_consec1: 1,1,2,3

My dataset in the end will be just max_consec0 and max_consec1
","You can try this implementation:
num0=0
num1=0
consec0=[]
consec1=[]
for i in range(len(df)):
  if(df.iloc[i,0])==0:
    num0=num0+1;
    num1=0;
  if(df.iloc[i,0])==1:
    num0=0;
    num1=num1+1;
  consec0.append(num0)
  consec1.append(num1)
df['consec0']=consec0
df['consec1']=consec1
max_consec0=df['consec0'].max()
max_consec1=df['consec1'].max()

",<python><pandas><preprocessing><numpy>
"#Text Representation
from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer(sublinear_tf=True, min_df=2,max_df= 0.3, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')
features = tfidf.fit_transform(df.ContextualText).toarray()
labels = df.category_id
features.shape

#Running Linear SVC
from sklearn.model_selection import train_test_split

model = LinearSVC()

X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, df.index, test_size=0.2, random_state=0)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

#Predictions based on model 
vectorized_text = features.transform(&quot;Heathcare is good&quot;).toarray()

","Your question leaves a lot unexplained, but the error you're receiving probably came from the last line vectorized_text = features.transform(&quot;Heathcare is good&quot;).toarray(). 
The variable features is itself an array (it came from features = ... .toarray()).
I'm assuming you meant to replace it with 
vectorized_text = tfidf.transform(&quot;Heathcare is good&quot;).toarray()?
",<numpy><tfidf><text-classification>
"what is the difference between slope of the line and slope of the curve? Is it valid to use numpy.gradient to find the slope of the line and slope of the curve  at any point?
    #slope of line at any point 
    tanθ= y2-y1/x2-x1

    #slope of curve at any point 
    tanθ  =dy/dx

is it valid to use numpys np.gradient() to get both slopes of curve and line ? or is it meant only to find the slope of line?
Reference slope of line vs curve 
","
what is the difference between slope of line and slope of curve

It is really a matter of perspective. The slope of a line is the same over the entire span of that line, i.e. until the line changes direction. The slope of a curve is like the slope of millions of tiny lines all connected, so the slope is only the same value over tiny spans. So we can only talk about the slope of a curve at a give point (e.g. a given x value) and then we normally talk about the gradient of the line at that point.

Using your words, the gradient computed by numpy.gradient is the slope of a curve, using the differences of consecutive values.
However, you might like to imagine that your changes, when measured over smaller and smaller distances, become the slope (by your definitions). So when e.g. x2 - x1 almost reaches zero, you have the same meaning as $dx$.
Coarse example with 10 points
Here is a coarse example of numpy.gradient, where $dx$ has s size of 1 (equally spaced values):
In [1]: import matplotlib.pyplot as plt
In [2]: import numpy as np
In [3]: N = 10                                  # Use ten samples
In [4]: x = np.linspace(0, np.pi*2, N)          # Equally spaced x values
In [5]: y = np.sin(x)                           # Corresponding sine values
In [6]: grads = np.gradient(y)                  # compute the gradients

Plotting the values and the gradients - I shifted the gradients by 0.5 to the right, so their values line up with the middle of the segment for which they describe the slope:
In [7]: fig, ax = plt.subplots()           
In [8]: ax.plot(x, y, &quot;-b&quot;, label=&quot;values&quot;)                  # the y values
        ax.plot(x + 0.5, grads, &quot;--r&quot;, label=&quot;gradients&quot;)    # the computed gradients
        plt.legend()
In [9]: plt.show()


Fine example with 1,000,000 points
Now we do example same as before, just use one million points: N = 1_000_000.
The blue line would look much more like a true sine wave, but the red line is now measuring the gradient with a much higher resolution than before, giving us the exact value at 1,000,000 points - for each tiny line segment of the blue line.

So the gradient values look like they are all zero! Well this is just because we made x2-x1 become almost zero (1 / 1e6), so the values of y2 - y1 also were essentially zero! We have started to approximate $\frac{dy}{dx}$.
Let change the axis scale to see that the gradients do still match the pattern we might expects, and are indeed very smooth - looking like a curve:

Much better  :)
(Notice the scale difference - 1e-6).

Here are a few explanations of what np.gradient really does.
",<python><scikit-learn><gradient-descent><numpy><derivation>
"how to find slope at certain points circled in blue in below curve ? Are these below 2 approaches valid ? though they give different results .
How to automatically find the points where the slope changes drastically in curve  like around at point 5,6 in below graph
x=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, ]

y=[512, 256, 128, 64 , 32 , 16 , 8  , 7  , 6  , 5  , 4  , 3  , 2  , 1  ]


Numpy gradient give below result

np.gradient(y)
[-256. , -192. ,  -96. ,  -48. ,  -24. ,  -12. ,   -4.5,   -1. ,-1. ,   -1. ,   -1. ,   -1. ,   -1. ,   -1. ]

can we use numpy.gradient to find the slope of curve ? since finding slope of line and curve is bit different Shown in this link
2.Using custom slope function
def slope(x1, y1, x2, y2):
    m = (y2-y1)/(x2-x1)
    return m


slope_value=[]
for i in range(len(y)):
    i += 1
    v=slope(y[i], x[i], y[i-1], x[i-1])
    print(i,v)
    slope_value.append(v)



result: [-0.00390625,  -0.0078125,  -0.015625,  -0.03125,  -0.0625,  -0.125,  -1.0,  -1.0,  -1.0,  -1.0,  -1.0,  -1.0,  -1.0]


","The numpy calculation is the correct one to use, but may be a bit tricky to understand how it is calculated
Your custom calculation is accidentally returning the inverse slope, the x and y values are reversed in the slope function (x1 -&gt; y[i], etc). The slope should be delta_y/delta_x
def slope(x1, y1, x2, y2):
v=slope(y[i], x[i], y[i-1], x[i-1])

Also, you are calculating the slope at x = 1.5, 2.5, etc but numpy is calculating the slope at x = 1, 2, 3
In the gradient calculation, numpy is calculating the gradient at each x value, by using the x-1 and x+1 values and dividing by the difference in x which is 2. You are calculating the inverse of the x + .5 values.
x=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, ]

y=[512, 256, 128, 64 , 32 , 16 , 8  , 7  , 6  , 5  , 4  , 3  , 2  , 1  ]

numpy answer:
[-256. , -192. ,  -96. ,  -48. ,  -24. ,  -12. ,   -4.5,   -1. ,-1. ,   -1. ,   -1. ,   -1. ,   -1. ,   -1. ]

each calculation in numpy function:
-256 = (-256)/1 there is no x-1 value so the default is to use x
-192 = (128-512)/2
-96 =  (64-256)/2
...
",<python><clustering><k-means><gradient-descent><numpy>
"I have a function that returns the predicted accuracy of a time-series model. I have two equally-sized numpy arrays, one for the actual direction and one for the predicted direction. I'm classifying whether there is a change in the data's direction from the previous t-1 step.  '1' stands for an increase from t-1, '0' stands for no change in direction, and '-1' stands for a decrease from t-1. I'm trying to compare the elements of both arrays to determine if they both contain the same data to determine accuracy.
I can match indexes and count the number of '1s' and '-1s' that match,  but I cannot count the number of matching zeros. (It's kinda hard summing zeros). :-)  Anyway, I've tried the numpy sum function specifying '0' as the argument for both arrays but it only returns an array of zeros but no count.  I'm not trying to create a confusion matrix...the goal here is to create an accuracy score.  I plan to take all the matching ones, negative ones and zeros and divide that by the total length to get an accuracy score.
Thanks in advance.
","You can compare the predictions with the expected results directly, using simple comparisons, in this case just ==. This returns boolean values - True or False, which you can sum up because True == 1 and False = 0.
Here is an example for your case using some randomly generated dummy data:
In [1]: import numpy as np                                                                                                           

In [2]: y = np.random.choice([-1, 0, 1], 10)                                                                                         

In [3]: preds = np.random.choice([-1, 0, 1], 10)                                                                                     

In [4]: y                                                                                                                            
Out[4]: array([ 1,  1,  1, -1,  1, -1, -1,  1,  1,  0])

In [5]: preds                                                                                                                        
Out[5]: array([ 0, -1,  1,  0,  1,  1, -1,  1, -1,  0])

The real part that checks where your predictions are correct is then done using two checks:

where are the predictions equal to the ground truth i.e. where were you correct in predicting the direction, and
where are the predictions equal to the direction you are interested in i.e. downwards (-1), no change (0) or upwards (1)

This can be done as follows:
In [6]: (y == preds) &amp; (preds == 0)   #   &amp;   means we needs both checks to be True 
Out[6]:
array([False, False, False, False, False, False, False, False, False, True])

We can see only the final position is True after both these checks, because that is the only place that the prediction was True and the value direction was 0.
You can then write a loop to check all values and do something with them:
In [7]: n = len(preds)    # the number of test samples (= 10 in my dummy example)

In [8]: for direction in [-1, 0, 1]:
            score = sum((y == preds) &amp; (preds == 0)) 
            accuracy = score / n 
            print(f&quot;Direction {direction:&gt;2}: {score}/{n} = {accuracy * 100:.1f}%&quot;)

Which gives:
Direction -1: 1/10 = 10.0%
Direction  0: 1/10 = 10.0%
Direction  1: 1/10 = 10.0%

",<python><data><numpy>
"I'm trying to apply the Expectation Maximization algorithm (EM) to a Gaussian Mixture Model (GMM) using Python and NumPy. The PDF document I am basing my implementation on can be found here.
Below are the equations:
$\mathrm{E}-\text{step:}$
$$w_{ik} = \frac{\pi_k \cdot p_k(x_i|z_k, \mu_k, \Sigma_k)}{\sum_{m=1}^{K} \pi_m \cdot p_m(x_i|z_m, \mu_m, \Sigma_m)}, \; [1]$$
$\text{where:}$
$${\displaystyle (2\pi )^{-{1}}|{\Sigma_k}|^{-{\frac {1}{2}}}\,\mathrm e^{-{\frac {1}{2}}(x_i -{\mu_k})^{\!{\mathsf {T}}}{{\Sigma_k }}^{-1}(x_i -{\mu_k})}.} 
$$
$\mathrm{M}-\text{step:}$
$$\pi_k^{\text{new}} = \frac{N_k}{N}, \; [2]$$
$\text{where:}$
$$N_k = \sum_{i=1}^{N} w_{ik}.$$
$$\mu_k^{\text{new}} = \frac{1}{N_k} \sum_{i=1}^{N} w_{ik} \cdot x_i, \; [3]$$
$$\Sigma_k^{\text{new}} = \frac{1}{N_k} \sum_{i=1}^N w_{ik} (x_i - \mu_k)(x_i - \mu_k)^\mathsf {T}. \; [4]$$
When applying the algorithm I get the mean of the first and second cluster equal to:
array([[2.50832195],
       [2.51546208]])

When the actual vector means for the first and second cluster are, respectively:
array([[0],
       [0]])

and:
array([[5],
       [5]])

The same thing happens when getting the values of the covariance matrices I get:
array([[7.05168736, 6.17098629],
       [6.17098629, 7.23009494]])

When it should be:
array([[1, 0],
       [0, 1]])

for both clusters.
Here is the code:
np.random.seed(1)

# first cluster
X_11 = np.random.normal(0, 1, 1000)
X_21 = np.random.normal(0, 1, 1000)

# second cluster
X_12 = np.random.normal(5, 1, 1000)
X_22 = np.random.normal(5, 1, 1000)

X_1 = np.concatenate((X_11,X_12), axis=None)
X_2 = np.concatenate((X_21,X_22), axis=None)

# data matrix of k x n dimensions (2 x 2000 dimensions)
X = np.concatenate((np.array([X_1]),np.array([X_2])), axis=0)

# multivariate normal distribution function gives n x 1 vector (2000 x 1 vector)
def normal_distribution(x, mu, sigma):
  mvnd = []
  for i in range(np.shape(x)[1]):
    gd = (2*np.pi)**(-2/2) * np.linalg.det(sigma)**(-1/2) * np.exp((-1/2) * np.dot(np.dot((x[:,i:i+1]-mu).T, np.linalg.inv(sigma)), (x[:,i:i+1]-mu)))
    mvnd.append(gd)
  return np.reshape(np.array(mvnd), (np.shape(x)[1], 1))

# Initialized parameters
sigma_1 = np.array([[10, 0],
                    [0, 10]])
sigma_2 = np.array([[10, 0],
                    [0, 10]])
mu_1 = np.array([[10], 
                 [10]])
mu_2 = np.array([[10], 
                 [10]])
pi_1 = 0.5
pi_2 = 0.5

Sigma_1 = np.empty([2000, 2, 2])
Sigma_2 = np.empty([2000, 2, 2])

for i in range(10):
  # E-step:
  w_i1 = (pi_1*normal_distribution(X, mu_1, sigma_1))/(pi_1*normal_distribution(X, mu_1, sigma_1) + pi_2*normal_distribution(X, mu_2, sigma_2))
  w_i2 = (pi_2*normal_distribution(X, mu_2, sigma_2))/(pi_1*normal_distribution(X, mu_1, sigma_1) + pi_2*normal_distribution(X, mu_2, sigma_2))
  # M-step:
  pi_1 = np.sum(w_i1)/2000
  pi_2 = np.sum(w_i2)/2000
  mu_1 = np.array([(1/(np.sum(w_i1)))*np.sum(w_i1.T*X, axis=1)]).T
  mu_2 = np.array([(1/(np.sum(w_i2)))*np.sum(w_i2.T*X, axis=1)]).T
  for i in range(2000):
    Sigma_1[i:i+1, :, :] = w_i1[i:i+1,:]*np.dot((X[:,i:i+1]-mu_1), (X[:,i:i+1]-mu_1).T)
    Sigma_2[i:i+1, :, :] = w_i2[i:i+1,:]*np.dot((X[:,i:i+1]-mu_2), (X[:,i:i+1]-mu_2).T)
    sigma_1 = (1/(np.sum(w_i1)))*np.sum(Sigma_1, axis=0)
    sigma_2 = (1/(np.sum(w_i2)))*np.sum(Sigma_2, axis=0)

Would really appreciate if someone could point out the mistake in my code or in my misunderstanding of the algorithm.
","One reason why you aren't getting fitted values close to the true values could be the initial values of the parameters used.
It's likely what you have found is a local maxima. You have to try a number of initial starts and then pick the one with that gives the highest likelihood.
",<machine-learning><python><algorithms><unsupervised-learning><numpy>
"I have few difficulties in picking values from an array. Here is a simplified version of the concerned part of the code. (If you would like to put the whole code please mention it)
n=2
m=8
test= test[:,-n*m:] 
test=test.reshape(test.shape[0],n,m)

So, I end up with :
test.shape= (350, 2, 8)

when I run the code I've got this:
array([[[0.01911469, 0.32352942, 0.18032786, ..., 0.006101  ,
         0.        , 0.        ],
        [0.01810865, 0.32352942, 0.18032786, ..., 0.006101  ,
         0.        , 0.        ]],

       [[0.01810865, 0.32352942, 0.18032786, ..., 0.006101  ,
         0.        , 0.        ],
        [0.01710262, 0.32352942, 0.1967213 , ..., 0.01297103,
         0.        , 0.        ]],

       [[0.01710262, 0.32352942, 0.1967213 , ..., 0.01297103,
         0.        , 0.        ],
        [0.01408451, 0.32352942, 0.18032786, ..., 0.00763907,
         0.        , 0.        ]],

       ...,

       [[0.01006036, 0.2647059 , 0.26229507, ..., 0.40558836,
         0.        , 0.        ],
        [0.01006036, 0.2647059 , 0.26229507, ..., 0.41399646,
         0.        , 0.        ]],

       [[0.01006036, 0.2647059 , 0.26229507, ..., 0.41399646,
         0.        , 0.        ],
        [0.00804829, 0.2647059 , 0.24590163, ..., 0.4208665 ,
         0.        , 0.        ]],

       [[0.00804829, 0.2647059 , 0.24590163, ..., 0.4208665 ,
         0.        , 0.        ],
        [0.01207243, 0.2794118 , 0.26229507, ..., 0.42621556,
         0.        , 0.        ]]], dtype=float32)

My question is how can I get only the values of the first columns like this:
array([[0.01911469],
       [0.01810865],
       [0.01810865],
       [0.01710262],
       [0.01710262],
       [0.01408451],
        ...,
       [0.01006036],
       [0.01006036], 
       [0.01006036],
       [0.00804829],
       [0.00804829],
       [0.01207243]], dtype=float32)

I have tried this:
test=test[:,:,:1]

but I've got this instead of what I'm looking for:
array([[[0.01911469],
        [0.01810865]],

       [[0.01810865],
        [0.01710262]],

       [[0.01710262],
        [0.01408451]],

       ...,

       [[0.01006036],
        [0.01006036]],

       [[0.01006036],
        [0.00804829]],

       [[0.00804829],
        [0.01207243]]], dtype=float32)

","Try the following code. As you want all first value of zeroth dimension.
test=test[:,0,0]

",<machine-learning><python><deep-learning><numpy><reshape>
"I have a pd.Series with, for instance, n lines. I would like to transform this series in a pd.DataFrame as follows:
Ex:
Input: pd.Series([10,11,12,13,14,15]) and a variable chunk_size = 2 that will be the number of columns.
Target:
    0 | 1
    _   _
    10  11
    12  13
    14  15

The target DataFrame will have a shape of (n / chunk_size) rows by chunk_size columns.
Thanks in advance.
","Here is a quick solution that does not do it in-place but takes up extra space:
def transform_series(x, chunk_size):
    df = pd.DataFrame()
    for i in range(chunk_size):
        df[f'column_{i+1}'] = x[i::chunk_size].reset_index(drop=True)
    return df


input_series = pd.Series([10,11,12,13,14,15])
transformed_df = transform_series(input_series, chunk_size=2)

Output:
print(transformed_df)

   column_1  column_2
0        10        11
1        12        13
2        14        15

",<dataset><pandas><numpy><dataframe>
"I just started Data Science and I want to know what the np.ndarray() function's use is, and in which scenario(s) it is used.
","numpy.ndarray is not a function but a class, as you can see in the documentation.

An ndarray is meant to store data, it is just a multidimensional matrix. You store your matrix data in an ndarray and then you operate on it, adding to other ndarrays, or any other matrix operation you want to compute. From the documentation of ndarray:


  An ndarray is a (usually fixed-size) multidimensional container of items of the same type and size. The number of dimensions and items in an array is defined by its shape, which is a tuple of N non-negative integers that specify the sizes of each dimension. The type of items in the array is specified by a separate data-type object (dtype), one of which is associated with each ndarray.


While you can create yourself instances of numpy.ndarray by invoking its constructor, most of the times, arrays are created with the convenience function numpy.array, which ends up creating and returning an instance of numpy.ndarray. For instance, let's create a 3x3 identity matrix manually:

import numpy as np

a = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])


And now we can compute operations on it, e.g.:

b = a + a
print(b)


The result is:

[[2 0 0]
 [0 2 0]
 [0 0 2]]

",<numpy>
"I am getting an error for &quot;no attribute columns&quot;. Because of this, I can't see the selected K features and can't build plots:
First occurrence of error:

Second occurrence of error:
I'm also not able to plot the score for each selected feature, I've tried some alternatives but I'm failing:

","get_support() will only return an array, there are no column names in that object. Check this website out for usage:

https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html

In your code, mask will show an array of True or False values

print(mask)
[ True  True  True  True False ... False]


If you want the column names,  you can get them from the X_train dataset source.

Also, if you want to plot the scores, you can try something like this. 

Example:

import matplotlib.pyplot as plt

X_indices = np.arange(X.shape[-1])
...
scores = selector.scores_
scores /= scores.max()
plt.bar(X_indices, scores, width=.2)


Output: 


",<machine-learning><scikit-learn><feature-selection><numpy>
"I have a 10-year sst monthly data. I want to plot the interannual monthly climatology for the month of October. By using the code I have for python with me I am able to plot the interannual monthly climatology for all the 12 months. If anyone can help me by going over my code and say where I have to put index so that I can have monthly climatology only for the month of October it will be much appreciated. Below is my code:

import xarray as xr
import numpy as np
import matplotlib.pyplot as plt
#time_slice = slice(9,10)
lat_slice = slice(30, 0)
lon_slice = slice(40, 80)
nc = xr.open_dataset('E:/X49.37.14.247.143.9.43.4.nc')
np.disp(nc)
ds = nc.groupby('time.month', squeeze = False).mean('time')
dt = ds.sst.sel(lat = lat_slice,lon =lon_slice)
np.disp(dt)
dt.plot.pcolormesh(x = 'lon', y = 'lat', col = 'month', col_wrap =3) 

","ds = nc.groupby('time.month', squeeze = False).mean('time')


Here, you may put a ""filter"" either before or after group by.

Either you can do this-

df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',
                          'foo', 'bar'],
                   'B' : [1, 2, 3, 4, 5, 6],
                   'C' : [2.0, 5., 8., 1., 2., 9.]})
grouped = df.groupby('A')
grouped.filter(lambda x: x['B'].mean() &gt; 3.)


Source - https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.DataFrameGroupBy.filter.html

Or something like this-

gdf.apply(lambda g: g[g['team'] == 'A']).reset_index(drop=True).groupby(gdf.grouper.names)


Source- https://stackoverflow.com/questions/27488080/python-pandas-filter-rows-after-groupby
",<python><pandas><numpy><python-3.x><ipython>
"I was implementing a decision tree on a dataset. Before that, I wanted to transform a particular column with CountVectorizer. For this, I am using pipeline to make it simpler.
But there is an error of incompatible row dimensions.
code
# Imported the libraries....
from sklearn.feature_extraction.text import CountVectorizer as cv
from sklearn.preprocessing import OneHotEncoder as ohe
from sklearn.compose import ColumnTransformer as ct
from sklearn.pipeline import make_pipeline as mp
from sklearn.tree import DecisionTreeClassifier as dtc

transformer=ct(transformers=[('review_counts',cv(),['verified_reviews']),
                             ('variation_dummies', ohe(),['variation'])
                            ],remainder='passthrough')

pipe= mp(transformer,dtc(random_state=42))

x= data[['rating','variation','verified_reviews']].copy()
y= data.feedback

x_train,x_test,y_train,y_test= tts(x,y,test_size=0.3,random_state=42,stratify=y)
print(x_train.shape,y_train.shape)             # ((2205, 3), (2205,))

pipe.fit(x_train,y_train)                       # Error on this line

Error
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-79-a981c354b190&gt; in &lt;module&gt;()
----&gt; 1 pipe.fit(x_train,y_train)

7 frames
/usr/local/lib/python3.6/dist-packages/scipy/sparse/construct.py in bmat(blocks, format, dtype)
    584                                                     exp=brow_lengths[i],
    585                                                     got=A.shape[0]))
--&gt; 586                     raise ValueError(msg)
    587 
    588                 if bcol_lengths[j] == 0:

ValueError: blocks[0,:] has incompatible row dimensions. Got blocks[0,1].shape[0] == 2205, expected 1.


Questions

How is this error of incompatible row dimension forming?
How it can be solved?

","As per the documentation, whenever the transformer expects a 1D array as input, the columns were specified as a string (""xxx""). For the transformers which expects 2D data, we need to specify the column as a list of strings ([""xxx""]).

so the code below will work.

## Important: i have passed the columns a string to CV and list of columns to OHE

transformer=ct(transformers=[('review_counts',cv(),'verified_reviews'), 
                             ('variation_dummies', ohe(),['variation'])
                            ],remainder='passthrough')


Credit goes to Another man who helped me on this.
",<machine-learning><classification><scikit-learn><decision-trees><numpy>
"I have an excel file which contains the lat and long values of the center of a Tropical Cyclone(TC). The excel file is as given below:
19.8  69.4
20    69
20.4  68.2
20.5  67.2
20.5  65.7
20.3  65
20.2  64.2
20.2  63.7
20.2  62.9
20.2  62.3
20.2  61.5
20.1  61
20.1 60.3
20    59.5
19.9  58.9
19.8  58.3

Also, I have an NC(NetCDF) file which is of that of air temperature(The link to the data is given)air_temp.nc. Now what I intend to do is average over an area of radius 2.5◦ on the storm center for the variable in the NC data i.e. for each lat long value I need to find the average over an area of average 2.5◦. I know how to find the simple average using NumPy mean for individual lat-long, but I am confused about how to find over an area for a given radius.
","Define a function to calculate distance between two latitudes and longitudes. I found php implementation of below here. I converted it to python.

import numpy as np 
import math 

def getDistanceBetweenPoints(latitude1, longitude1, latitude2, longitude2):
        theta = longitude1 - longitude2
        distance = math.sin(np.radians(latitude1)) * math.sin(np.radians(latitude2)) + math.cos(np.radians(latitude1)) * math.cos(np.radians(latitude2)) * math.cos(np.radians(theta))

        distance = math.acos(distance)
        distance = np.degrees(distance)

        return distance


Import TC data in the pandas dataframe and apply below function to call on the DataFrame. The function would call above function getDistanceBetweenPoints for each latitude and longitude

# create a dataframe
df = pd.DataFrame(tc_csv_file_path)

# take subset of the data frame with columns we want
df_lat_long = df[['latitude', 'longitude']]

# insert a new column in dataframe which would hold distance between the latitudes and longitudes
df_lat_long.insert(df_lat_long.shape[1], ""distance"", [0.0 for val in range(df_lat_long.shape[0])], True)


Define a function which would be invoked row wise on a data frame

def getDistanceBetweenPoints_df(row):
    # Place latitude/longitude for a centre point here
    # I have filled 0.0, you replace with with actual values
    centre_lat = 0.0
    centre_long = 0.0
    row['distance'] = getDistanceBetweenPoints(centre_lat, centre_long, row['longitude'], row['latitude'], unit='Km')


Now invoke this function on data frame. It will called row wise and it will populate the column distance in the dataframe with the distance between center latitude/longitude and the row's latitude/longitude

# Invoke function on dataframe
df_lat_long.agg(getDistanceBetweenPoints_df, axis='columns')

# View the dataframe for values only where distance is less than or equal to 2.5◦
df_lat_long[df_lat_long['distance'] &lt;= 2.5]


So, these are the latitudes and longitudes where distance from the given centre is less than of equal to 2.5◦. Do whatever you want to do with these
",<python><pandas><numpy><matlab>
"I have an excel file which contains the lat and long values of the center of a Tropical Cyclone(TC). The excel file is as given below:
19.8  69.4
20    69
20.4  68.2
20.5  67.2
20.5  65.7
20.3  65
20.2  64.2
20.2  63.7
20.2  62.9
20.2  62.3
20.2  61.5
20.1  61
20.1 60.3
20    59.5
19.9  58.9
19.8  58.3

Also, I have an NC(NetCDF) file which is of that of air temperature(The link to the data is given)air_temp.nc. Now what I intend to do is average over an area of radius 2.5◦ on the storm center for the variable in the NC data i.e. for each lat long value I need to find the average over an area of average 2.5◦. I know how to find the simple average using NumPy mean for individual lat-long, but I am confused about how to find over an area for a given radius.
","I think your problem is not so much calculating the average, as it is to create a subset within a radius. Once you have that subset, calculating the average is trivial.

You find an example of subsetting on radius here: https://stackoverflow.com/questions/59060532/calculate-coordinates-inside-radius-at-each-time-point

Note however that that example works with a 2d circle, which isn't exactly what you are looking for because Earth is a globe. In order to adjust for it, you'll need the great circle distance (or Haversine distance). Why? Look here: https://en.wikipedia.org/wiki/Haversine_formula

You'll find a python implementation of that concept here: https://stackoverflow.com/questions/52889566/calculate-euclidean-distance-for-latitude-and-longitude-pandas-dataframe-pytho

1. Load the data into a dataframe

import pandas as pd
import xarray as xr

data = xr.open_dataset('file')
df = data.to_dataframe()

",<python><pandas><numpy><matlab>
"The last time I used Python is some time ago so some things have faded away.

I have a given dataset, with multiple columns. I want to create a new column and set the values based on multiple values (text or value) of other columns. 

So in the example below, c1 consists of [a,a,b,b] and c2 of [a,b,a,b]. Now I want the new column c3 to be [1,2,3,4]

All help is appreciated!

import pandas as pd
data = pd.read_excel('data')

data['c3'] = 0

for i in range(len(data.c3)):
    if data.c1[i] == a and data.c2[i] == a:
        data.c3[i] == 1
    elif data.c1[i] == a and data.c2[i] != a:
        data.c3[i] == 2
    elif data.c1[i] != a and data.c2[i] == a:
        data.c3[i] == 3
    elif data.c1[i] != a and data.c2[i] != a:
        data.c3[i] == 4```


","I would use apply. It allows you to perform a function row-wise or column-wise. Check this post on StackOverflow, it provides good examples of what this looks like.

The gist is:


create a function that creates the value you want based on a row
use apply to create a dataframe containing the results
if you're happy with the results, merge that dataframe into your existing one

",<python><pandas><numpy>
"I'm trying to implement LMS algorithm in python. I have the following code:

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


def compute_cost(theta, X , y):
    inner = np.power(X.dot(theta) - y, 2)
    return np.sum(inner/2*len(X))


if __name__ == '__main__':
    path = os.getcwd() + '/data/ex1data1.txt'
    data = pd.read_csv(path, header=None, names=['Population', 'Profit'])
    data.plot(x='Population', y='Profit', kind='scatter', figsize=(10, 10))
    plt.show()
    # append a ones column to the front of the data set
    data.insert(0, 'Ones', 1)

    # set X (training data) and y (target variable)
    cols = data.shape[1]
    X = data.iloc[:, 0:cols - 1]
    y = data.iloc[:, cols - 1:cols]
    theta = np.array([0,0])
    print(X.shape, theta.shape, y.shape)
    print(compute_cost(theta, X, y))


The first print statement (printing X.shape, theta.shape, y.shape) prints the following:

(97, 2) (2,) (97, 1)


When I try to compute the cost function I get the following:

Profit    0.0
0         0.0
1         0.0
2         0.0
3         0.0
         ... 
92        0.0
93        0.0
94        0.0
95        0.0
96        0.0
Length: 98, dtype: float64


However, I'm supposed to (according to the exercise) get 32.07
I think that the bug is related to the theta shape but I tried initializing theta like this:

theta = np.zeros(shape=(2, 1))


And still this doesn't work..
To be clear, I'm not looking for help writing the algorithm simply find the syntax bug, understand why it happened and fix it.
","The Equation for Least Square method shall be as below-

theta(0)+theta(1).X , since you have 1 variable.

if theta(0) =0 and theta(1)=0 since you are adding it theta = np.zeros(shape=(2, 1)).

the value of Y shall be 0 hence error is 0.

To breakdown nicely you can add it like-

n = X.shape[1]
theta = np.zeros((1, n))


Now in the next run the values of theta shall change as per learning rates and in then the errors shall change too, that part you haven't reached yet, by looking at your code if I might add.

Here have a look-

https://towardsdatascience.com/linear-regression-using-python-b136c91bf0a2
",<machine-learning><python><pandas><linear-regression><numpy>
"I'm new to data science with a moderate math background. I'm playing around with numpy and can across the following:



So after reading np.linalg.norm, to my understanding it computes the 2-norm of the matrix. Wanting to see if I understood properly, I decided to compute it by hand using the 2 norm formula I found here:



Following computing the dot product, the characteristic equation, applying the formula for quadratic equation and taking square root of the max, I end up with a different result, namely:



So here is my question. What went wrong? Did I use the right formula? Also I couldn't find a conclusive way to get the 5. The doc he doc that says: 


  If this is set to True, the axes which are normed over are left in the result as dimensions with size one. With this option the result will broadcast correctly against the original x. 


But I can't wrap my head around it. What does it represent and how you compute it?

I hope the formatting and the question are clear enough.
","The formula you cited is not the formula scipy is using. According to the documentation it calculates the ""Frobenius norm"" which is defined to be the root of the squared sum of the elements of each row (row since you specified dim=1)

$ O_i = { \biggl(\sum_{n=1}^N \mathbf{A}_{i, n}^2\biggr) }^{1/2} $

Thus, $(3^2+4^2)^{1/2}=5$ and $(2^2+6^2+4^2)^{1/2}=56^{1/2}$

EDIT: Also, the keepdims argument just says whether to collapse the dimension that you calculated or not, a simple example is to look at the output shape of the matrix that you get with and without the argument.

a = np.array([[0, 3, 4], [2, 6, 4]])

np.linalg.norm(a, axis=1, keepdims=True).shape # output is (2, 1)
np.linalg.norm(a, axis=1, keepdims=False).shape # output is (2,) because the second dimension was collapsed

",<python><deep-learning><numpy><normalization><matrix>
"While plotting the bar graph for number of attempt on x-aixs and time-taken on y-axis as per below code, getting Value error.Trace of error is included below.

import time as t
import matplotlib.pyplot as plt

times=[];
mistakes=0;
input(""press enter to continue"")

while len(times)&lt;5:
    start=t.time();
    word=input(""Type the word==&gt;"")
    end=t.time();
    t_cal=end-start;

    times.append(t_cal)

    if(word.lower() != ""programing""):
        mistakes+=1

    print(""You mistake"", str(mistakes),"" Times. "")

    print(""see your progress"")
    t.sleep(5);

    x=[1,2,3,4,5]
    y=times

    plt.bar(x,y)

    grnd=[""1"",""2"",""3"",""4"",""5""]
    plt.xticks(x,grnd)
    plt.xlabel(""ur attempt"")
    plt.ylabel(""time in secconds"")
    plt.title(""ur typing speed"")
    plt.show()


ValueError                                Traceback (most recent call last) &lt;ipython-input-1-8cc693c6dda0&gt; in &lt;module&gt;
     25     y=times
     26 
---&gt; 27     plt.bar(x,y)
     28 
     29     grnd=[""1"",""2"",""3"",""4"",""5""]

~\anaconda3\lib\site-packages\matplotlib\pyplot.py in bar(x, height, width, bottom, align, data, **kwargs)    2439     return gca().bar(    2440         x, height, width=width, bottom=bottom, align=align,
-&gt; 2441         **({""data"": data} if data is not None else {}), **kwargs)    2442     2443 

~\anaconda3\lib\site-packages\matplotlib\__init__.py in inner(ax, data, *args, **kwargs)    1597     def inner(ax, *args, data=None,
**kwargs):    1598         if data is None:
-&gt; 1599             return func(ax, *map(sanitize_sequence, args), **kwargs)    1600     1601         bound = new_sig.bind(ax, *args, **kwargs)

~\anaconda3\lib\site-packages\matplotlib\axes\_axes.py in bar(self, x, height, width, bottom, align, **kwargs)    2372         x, height, width, y, linewidth = np.broadcast_arrays(    2373             # Make args iterable too.
-&gt; 2374             np.atleast_1d(x), height, width, y, linewidth)    2375     2376         # Now that units have been converted, set the tick locations.

&lt;__array_function__ internals&gt; in broadcast_arrays(*args, **kwargs)

~\anaconda3\lib\site-packages\numpy\lib\stride_tricks.py in broadcast_arrays(*args, **kwargs)
    262     args = [np.array(_m, copy=False, subok=subok) for _m in args]
    263 
--&gt; 264     shape = _broadcast_shape(*args)
    265 
    266     if all(array.shape == shape for array in args):

~\anaconda3\lib\site-packages\numpy\lib\stride_tricks.py in
_broadcast_shape(*args)
    189     # use the old-iterator because np.nditer does not handle size 0 arrays
    190     # consistently
--&gt; 191     b = np.broadcast(*args[:32])
    192     # unfortunately, it cannot handle 32 or more arguments directly
    193     for pos in range(32, len(args), 31):

ValueError: shape mismatch: objects cannot be broadcast to a single shape

","The error is because as it says in trace ,at the second and consecutive runs values of x and y are/shall mis matching. You are trying to plot 2 or more values against 1. Arrange the values of x as per the values of Y.


  x = np.arange(len(y)


import time as t
import matplotlib.pyplot as plt
import numpy as np

times=[];
mistakes=0;
input(""press enter to continue"")

while len(times)&lt;5:
    start=t.time();
    word=input(""Type the word==&gt;"")
    end=t.time();
    t_cal=end-start;

    times.append(t_cal)

    if(word.lower() != ""programing""):
        mistakes+=1

    print(""You mistake"", str(mistakes),"" Times. "")

    print(""see your progress"")
    t.sleep(5);

    #x=[1,2,3,4,5]
    y=times
    x = np.arange(len(y))
    print(""lenght of y is :"",len(y))

    plt.bar(x,y)

    grnd=[""1"",""2"",""3"",""4"",""5""]
    plt.xticks(x,grnd)
    plt.xlabel(""ur attempt"")
    plt.ylabel(""time in secconds"")
    plt.title(""ur typing speed"")
    plt.show()





",<python><numpy><matplotlib>
"I'm building a NN to predict stock prices but I'm getting an accuracy of 0, I really don't know what hyper parameters to play with anymore but it seems to always get this accuracy

Build LSTM model


model.add(LSTM(
    50, activation=""tanh"", recurrent_activation=""sigmoid"",
    return_sequences=True, batch_input_shape=(batch, timesteps, features)))#stateful=True,
model.add(layers.Dropout(0.2))

model.add(LSTM(100, activation=""tanh"", recurrent_activation=""sigmoid"",
               stateful=True, return_sequences=True)) #stateful=True
model.add(layers.Dropout(0.4))

model.add(layers.LSTM(50, activation=""tanh"", recurrent_activation=""sigmoid"",
               stateful=True, return_sequences=False)) #stateful=True
model.add(layers.Dropout(0.2))

model.add(Dense(25))
model.add(Dense(1))

# Complie model
model.compile(optimizer=""adam"", loss=""mean_squared_error"",
              metrics=[""accuracy"", ""mse""])

# Train the model
for i in trange(1, desc=""Training the fuck out of this model""):
    model.fit(x_train, y_train,
              batch_size=batch, epochs=1,
              shuffle=False)#shuffle=False
    model.reset_states()```

#OUTPUT
```Epoch 1/1

   1/2918 [..............................] - ETA: 1:47:06 - loss: 0.0318 - acc: 0.0000e+00 - mean_squared_error: 0.0318
   2/2918 [..............................] - ETA: 56:57 - loss: 0.0246 - acc: 0.0000e+00 - mean_squared_error: 0.0246  
   3/2918 [..............................] - ETA: 40:29 - loss: 0.0174 - acc: 0.0000e+00 - mean_squared_error: 0.0174
   4/2918 [..............................] - ETA: 32:15 - loss: 0.0135 - acc: 0.0000e+00 - mean_squared_error: 0.0135
   5/2918 [..............................] - ETA: 27:18 - loss: 0.0141 - acc: 0.0000e+00 - mean_squared_error: 0.0141
   6/2918 [..............................] - ETA: 24:00 - loss: 0.0150 - acc: 0.0000e+00 - mean_squared_error: 0.0150
   7/2918 [..............................] - ETA: 21:40 - loss: 0.0142 - acc: 0.0000e+00 - mean_squared_error: 0.0142
   8/2918 [..............................] - ETA: 19:48 - loss: 0.0125 - acc: 0.0000e+00 - mean_squared_error: 0.0125
   9/2918 [..............................] - ETA: 18:26 - loss: 0.0115 - acc: 0.0000e+00 - mean_squared_error: 0.0115
  10/2918 [..............................] - ETA: 17:20 - loss: 0.0108 - acc: 0.0000e+00 - mean_squared_error: 0.0108
  11/2918 [..............................] - ETA: 16:27 - loss: 0.0100 - acc: 0.0000e+00 - mean_squared_error: 0.0100
  12/2918 [..............................] - ETA: 15:42 - loss: 0.0094 - acc: 0.0000e+00 - mean_squared_error: 0.0094
  13/2918 [..............................] - ETA: 15:08 - loss: 0.0089 - acc: 0.0000e+00 - mean_squared_error: 0.0089
  14/2918 [..............................] - ETA: 14:32 - loss: 0.0083 - acc: 0.0000e+00 - mean_squared_error: 0.0083
  15/2918 [..............................] - ETA: 14:06 - loss: 0.0080 - acc: 0.0000e+00 - mean_squared_error: 0.0080
  16/2918 [..............................] - ETA: 13:39 - loss: 0.0076 - acc: 0.0000e+00 - mean_squared_error: 0.0076
  17/2918 [..............................] - ETA: 13:17 - loss: 0.0072 - acc: 0.0000e+00 - mean_squared_error: 0.0072
  18/2918 [..............................] - ETA: 12:58 - loss: 0.0068 - acc: 0.0000e+00 - mean_squared_error: 0.0068
  19/2918 [..............................] - ETA: 12:40 - loss: 0.0065 - acc: 0.0000e+00 - mean_squared_error: 0.0065```

","Your code is absolutely fine. Also, the model is working as you can see the loss is continuously decreasing.

The thing is, this is a regression problem and accuracy is used for classification tasks. It just calculates how often predictions match labels by comparing if the classes are the same. See here for more details on how it works. So, just remove accuracy from your metrics and you are good to go.
",<keras><numpy><python-3.x>
"I ran the following code in a jupyter notebook cell

ndarrs=np.array([""1.2"",""1.5"",""1.6""], dtype=np.string_)

print(ndarrs.dtype)


It returned |S3 as shown below.

Can someone help me understand the meaning of this symbol?


","In Python 3, you shouldn't really specify the np.string_ dtype, as it is left there for backwards compatibility with Python 2. The S type you see using np.dtype is a map to the bytes_ type, a zero-terminated string buffer, which shouldn't be used.
The S just means string and the number gives the number of bytes.
In [1]: s = &quot;&quot;        # start with an empty string                                                                                                

In [2]: for i in range(5):    # make the string larger
            s += str(i) 
            a = np.array([s], dtype=np.string_) 
            print(f&quot;{a}\t{a.dtype}&quot;) 
                                                                                                                       
    [b'0']  |S1
    [b'01'] |S2
    [b'012']    |S3
    [b'0123']   |S4
    [b'01234']  |S5
    [b'012345'] |S6

For python 3 you should instead use np.unicode:
In [1]: a = np.array([&quot;hello&quot;, &quot;world&quot;], dtype=np.unicode)                                                             

In [2]: type(a)                                                                                                       
Out[2]: numpy.ndarray

In [3]: b.dtype                                                                                                       
Out[3]: dtype('&lt;U5')


&lt; means little-endian
U means a unicode string
5 relates to the number of bytes used to hold the string. If you had really long strings, that number would then increase.

Have a look at this documentation, which states:

Note on string types:
For backward compatibility with Python 2 the S and a typestrings remain zero-terminated bytes and np.string_ continues to map to np.bytes_. To use actual strings in Python 3 use U or np.unicode_.

",<python><numpy>
"I am having a problem trying to reshape my numpy array. i have a 2 dimensional array and i want to make it a 3D array. I have looked everywhere but even if i find some answering my question they just make it complicated. I just want to know is there a mathematical way to calculate reshaping a numpy.
","I managed to solve the problem, so let me explain it so it all makes sense. So I had a 2D array of shape (76, 5) right,  and I wanted to reshape it to a 3D (76, 2, 5) array so i can stick it in to my keras LSTM model. But this is how I was doing it np.reshape(array, (76, 2, 5)), and this gave me the error that looked something like this cannot reshape array of size 380 to array of shape (76, 2, 5). So my confusion lied in the relationship between dimension and size. I than figured out that you could get the size from just knowing the dimension (if you multiply your dimensions you get your size), therefore 76*5 = 380 which is the size and 76*2*5 = 760 which is too big to fit in my old array. In order to reshape my array to become 3D I had to adjust some values

row = int(array.shape[0]/2) #The additional dimension i want to add
array = np.reshape(array, (row, 2, 5))


So now the shape of my array is (38, 2, 5) and the resulting size is now 38*2*5 = 380. So in conclusion if you want to reshape an already existing array, find the size first using the 

array.size


Than make sure that the multiplication of the dimensions of the new array that you want is equivalent to the size.  
",<keras><numpy><python-3.x>
"I am having a problem trying to reshape my numpy array. i have a 2 dimensional array and i want to make it a 3D array. I have looked everywhere but even if i find some answering my question they just make it complicated. I just want to know is there a mathematical way to calculate reshaping a numpy.
","I'm not sure if I fully understand what you mean, but numpy.expand_dims can be used to expand an array at the given index, allowing you to convert a 2 dimensional array to a 3 dimensional array.

import numpy as np

array = np.array([[1,2],[3,4]])
print(array.shape)
# (2, 2)
array = np.expand_dims(array, 0)
print(array.shape)
# (1, 2, 2)

",<keras><numpy><python-3.x>
"I have recently started with fastai lesson 1 and I am using kaggle to run the course notebooks. While going through the ‘lesson1-pets’ notebook we use untar_data(URLs.PETS) to get the data.

What I want to understand is where does this data get downloaded to?
As I can observe, after running the untar_data(URLs.PETS) function, it says downloading… ,the data gets downloaded, but nothing gets added to the data section of kaggle kernel.

Note: I am able to run the whole notebook in kaggle and just want to understand the working of untar_data function.
","It goes to PosixPath('/root/.fastai/data/oxford-iiit-pet') so you won't see it on the data pane.
",<dataset><pandas><computer-vision><numpy><fastai>
"I have two variables as time series, one a consequent of the other, I would like to find the average time delay it takes the dependent variable to act on the independent variable. Additionally, I would like to find the range of variance that is associated with the lag time and its respective confidence level. I am unsure how to go about this in a statistically valid way, but I am using Python. 

Currently I have used np.diff(np.sign(np.diff(df))) to isolate the relative max &amp; mins of the time series to then try to find the time gap between the subsequent pairs of mins &amp; max but that doesn't seem too valid to me -- thoughts? The out put of the mins &amp; maxes return an array like [0, -2,  0,  2,  0,  0, -2] where -2 is the relative min &amp; 2 is relative max. 

Methodological pointers would be greatly appreciated. 

Thank you for your time &amp; stay safe!

All the best, 
RS
","In a first approach, What you are looking for might be lagged correlation. You shift the trailling time serie by some variable time t and you look at correlation with the first time serie while varying the lag. The value of the lag which yield the biggest correlation is a good candidate for an estimation of time delay betweeen the two.
If you want more advanced statitics like variance (meaning that you have a variable lag) it might get more complex as you would have to identify which peak in the first time serie correspond to wich one in the second. Depending on your format of your time serie it might be easy or not. I would suggest to try a simple approach first : for each peak of your first time serie try to find the corresponding one in the second (first peak that appears after it for exemple). It might get complicated if some peaks overlap or the delay is too variable. Another option is to use more complicated things like Dynamic Time Warping Algorithms.
",<python><statistics><numpy><variance>
"I know the theory behind recurrent neural networks or RNN but I am confused about its implementation. This is an rnn equation I got from the web,


I tried to code the forward propagation alone in python's numpy

import numpy as np

outputs = 5
inputs = 3

# Input value
# (batch_size,seq_len, vocab_len)
X = np.ones((10,3,3))


# Initializing rnn weights and hidden states
Wxh = np.random.rand(outputs,inputs)
Whh = np.random.rand(outputs,inputs)
Why = np.random.rand(outputs,inputs)
h = np.zeros((1,inputs))

# Forward propagation
def rnn(x,h):
    h = np.tanh(np.dot(Whh,h.T) + np.dot(Wxh,x.T))
    y = np.dot(Why,h.T)
    return y,h

for i in X:
    _,h = rnn(i,h)


But I get a broadcasting error. How do we implement the forward propogation of rnn?
","In other words, what does the forward pass of a RNN look like. You read about using the inputs plus values from the previous node (here it will be prev_s)
First initialise the weights, than perform the foreward pass. I highlighted what you was looking for.

U = np.random.uniform(0, 1, (hidden_dim, T))
W = np.random.uniform(0, 1, (hidden_dim, hidden_dim))
V = np.random.uniform(0, 1, (output_dim, hidden_dim))


 for i in range(Y.shape[0]):
        x, y = X[i], Y[i]

        layers = []
        prev_s = np.zeros((hidden_dim, 1))
        dU = np.zeros(U.shape)
        dV = np.zeros(V.shape)
        dW = np.zeros(W.shape)

        dU_t = np.zeros(U.shape)
        dV_t = np.zeros(V.shape)
        dW_t = np.zeros(W.shape)

        dU_i = np.zeros(U.shape)
        dW_i = np.zeros(W.shape)

        # forward pass
        for t in range(T):
            new_input = np.zeros(x.shape)
            new_input[t] = x[t]
            mulu = np.dot(U, new_input)
            mulw = np.dot(W, prev_s)
            add = mulw + mulu
            s = sigmoid(add)
            ***mulv = np.dot(V, s)***
            layers.append({'s':s, 'prev_s':prev_s})
            prev_s = s


So the '*  *' area can be roughly translated: mulv = np.dot(V, s) are the weights multiplied with the current state. (same as before, s==input_vector) but the difference is that the s will be calculated with weights from previous output and current input i.e.

mulu = np.dot(U, new_input)
mulw = np.dot(W, prev_s)
add = mulw + mulu
s = sigmoid(add)


Thats why we have 3 initial weights in the first place.
",<python><deep-learning><lstm><numpy><rnn>
"As a way to improve my model, I want to average GloVe vectors over a sentence. However, I can't get np.mean to work. The following code works when not averaging over words. (copied from other code)

embeddings_dict = {}
with open(""glove.6B.50d.txt"", 'r') as f:
    for line in f:
        values = line.split()
        word = values[0]
        vector = np.asarray(values[1:], ""float32"")
        embeddings_dict[word] = vector

g = open(""input.txt"", 'r')

vector_lines = []

for line in g:
    clean_line = line.translate(str.maketrans('', '', string.punctuation))
    array_line = clean_line.lower().split()
    vec_line = []
    i = 0
    for word in array_line:
        i += 1
        try:
            vec_line.append(embeddings_dict[word])
        except:
            vec_line.append(embeddings_dict[""unk""])
    while i &lt; 30: //pad up to thirthy words with zero vectors
        vec_line.append(np.zeros(50))
        i += 1
    vector_lines.append(np.asarray(vec_line))

X = np.asarray(vector_lines)


To average over words, I'm modifying a small part of the code

        try:
            vec_line.append(embeddings_dict[word])
        except:
            vec_line.append(embeddings_dict[""unk""])

    #padding is not necessary anymore
    #while i &lt; 30: 
    #    vec_line.append(np.zeros(50))
    #    i += 1

    vec_mean = np.mean(vec_line, axis=0, keepdims=True)[:,None]
    vector_lines.append(vec_mean)

X = np.asarray(vector_lines)


This gives me the error ""ValueError: could not broadcast input array from shape (1,50) into shape (1,1)"". It feels as if I have tried every possible modification to this code as I could, but I keep on getting shape issues. What is causing all of these issues?
","Change this:

import bumpy as np
a = np.array([1,2,3])
b = np.array([4,5,6])
vec_line = [a,b]

print(np.mean(vec_line, axis=0, keepdims=True)[:,None])
&gt;&gt;[[[2.5 3.5 4.5]]]
np.mean(vec_line, axis=0, keepdims=True)[:,None].shape
&gt;&gt;(1,1,3)


To this:

print(np.mean(vec_line, axis=0))
&gt;&gt;[2.5 3.5 4.5]
np.mean(vec_line, axis=0).shape
&gt;&gt;(3,)

",<word-embeddings><numpy>
"I need to implement least square regression to fit polynomials of degree 1-27. I then need to get the leave-one-out error (kfold cross validation where k = n). After doing a lot of research it seems the best way to get the LOO error is to use sklearn cross_val_score(). My problem is I do not know how or if it is possible to use with regression models. 

n = len(x)
p, res, _, _, _  = numpy.polyfit(x,y,1,full=True)

cv = cross_val_score(?, X, y, scoring=mse, cv=n)



I cannot figure out what the estimator would be or how to make it in cross_val_score. Being new to python and these topics make this twice as challenging. 
","numpy and scikit-learn are quite separate open-source projects. As such, they are compatible in some ways but not in others. All scikit-learn models are variants on the fundamental estimator object (see https://scikit-learn.org/stable/tutorial/statistical_inference/settings.html). The cross_val_score function expects a scikit-learn-style estimator object and there's no easy way to create such an object from the output of numpy.polyfit.

I'd suggest working entirely in scikit-learn, creating a regression estimator that you can cross-validate with cross_val_score. This SO post shows you how to build that polynomial regression estimator.
",<python><regression><cross-validation><numpy>
"I'm trying to upload 17 images into a 4d numpy array, each image size is (256,256,1), so basically I'm using the 0th dimension for collection of different images. Following is my code:

import numpy as np
test_ip=np.zeros(shape=(17, 256, 256, 1))
count=0
for img in image_generator1:
  test_ip[count,:,:,:]=img
  count+=1


But it outputs an error:


  IndexError: index 17 is out of bounds for axis 0 with size 17


Also I printed shape of img in for loop and instead of (256,256,1) it is (1,256,256,1). 

Any help is highly appreciated.
","The index of your test_ip array goes from 0 to 16. So 17 is indeed too big (out of bounds).

You could add a try/except:

for counter, img in enumerate(image_generator1):
    try:
        test_ip[counter] = img
    except IndexError as err:
        print(f""Filled the array with {counter + 1} images"")    # f-string requires &gt;= python 3.6


Or if you know the length of that generator, just use this:

test_ip = np.array([im for im in image_generator1])

",<python><numpy>
"Please help me to replace NR values with corresponding values as in the dataframe.


","I'm not sure if this is most efficient way but it may help you.

df = read_csv('filename.csv')
grouped_df = df.groupby('City')
temp_df = []
for key, item in grouped_df:
    if 'NR' in item['Route'].values.tolist():
        values = item['Route'].values.tolist()
        to_change = [x for x in values if x != 'NR'][0]
        item = item.replace('NR', to_change)
        temp_df.append(item)
    else:
        temp_df.append(item)
final_df = pd.concat(temp_df, axis=0)


Output:

        City Route
4          A     3
5          B     4
6          C     5
7          D     7
8          D     7
9          D     7
10         D     7
2   Kolkatta     2
3   Kolkatta     2
0    Manipur    10
1    Manipur    10

",<python><pandas><data-cleaning><numpy>
"Please help me to replace NR values with corresponding values as in the dataframe.


","A simpler solution:

1) We generate a dictionary with the pairs:

df = pd.DataFrame({'a':['Hyderabad','Chennai','Lucknow','Kolkatta','Manipur','Manipur','Lucknow','Hyderabad','Kolkatta'], 'b':[4, 5, 9, 2, 10, 'NR', 'NR','NR',2]})
df

    a           b
0   Hyderabad   4.0
1   Chennai     5.0
2   Lucknow     9.0
3   Kolkatta    2.0
4   Manipur     10.0
5   Manipur     NR
6   Lucknow     NR
7   Hyderabad   NR
8   Kolkatta    2.0

# Ge get a dictionary with the pairs
pairs = {a:b  for _,(a,b) in df.iterrows() if b!= 'NR'}
pairs 
{'Hyderabad': 4.0,
 'Chennai': 5.0,
 'Lucknow': 9.0,
 'Kolkatta': 2.0,
 'Manipur': 10.0}


2) We use the dictionary to fill the NRs

df.loc[df.b=='NR', 'b'] = df[df.b=='NR'].apply(lambda x: pairs[x.a], axis =1)

    a           b
0   Hyderabad   4.0
1   Chennai     5.0
2   Lucknow     9.0
3   Kolkatta    2.0
4   Manipur     10.0
5   Manipur     10.0
6   Lucknow     9.0
7   Hyderabad   4.0
8   Kolkatta    2.0

",<python><pandas><data-cleaning><numpy>
"So, I am trying to implement a neural network in Python by only using NumPy. I have tried to do this by following 3Blue1Brown's video's about the topic, however, when testing my implementation, the network does not seem to work fully. I am testing the implementation on the AND, OR, and XOR problems - the AND and OR problems run as expected, however, the XOR problem does not:

Input    Output:
0, 0     0.253
1, 0     0.793
0, 1     0.793
1, 1     0.793    &lt;- WRONG!


These results have been generated with a sample size of 1000 samples chosen randomly, 10 epochs, and a neural network with one hidden layer of two nodes, and the sigmoid function in both the hidden layer and in the output layer. If I try a different model, for instance with two hidden layers with each three nodes, I get much worse results:

Input:    Output:
0, 0      0.712 &lt;- Wrong
1, 0      0.712
0, 1      0.712
1, 1      0.712 &lt;- Wrong


However, for some reason a model with no hidden layers does not as bad:

Input:    Output:
0, 0      0.025
1, 0      0.999
0, 1      0.984
1, 1      0.945 &lt;- Still wrong tho


I have tried different amounts of testing data, epochs, dimensions in the hidden layers, and hidden layers, and nothing seems to work

So, my question is if anyone has any idea why i get the wrong result in the XOR problem but not in the AND and the OR problems? And if so, how to fix it?

My implementation is as follows:

import numpy as np
from HiddenLayer import HiddenLayer
from InputLayer import InputLayer

np.random.seed(1) # Picking seed for debugging

class NeuralNetwork:
    def __init__(self, input_dimensions):
        self.n_layers = 1
        self.layers = np.array([InputLayer(input_dimensions)])

    def add_layer(self, dimensions):
        self.layers = np.append(self.layers, HiddenLayer(dimensions, self.layers[self.n_layers - 1].dimensions))
        self.n_layers += 1

    @staticmethod
    def sigmoid_derived(x):
        return np.multiply(x, 1.0 - x)

    def train(self, X, t, n_epoch, learning_rate = 1.0):
        def update_weight(l, j, k):
            self.layers[l].weights[j, k] -= learning_rate * self.layers[l - 1].a[k] * NeuralNetwork.sigmoid_derived(self.layers[l].a[j]) * self.layers[l].grad[j]

        def update_bias(l, j):
            self.layers[l].b[j] -= learning_rate * NeuralNetwork.sigmoid_derived(self.layers[l].a[j]) * self.layers[l].grad[j]

        for _ in range(n_epoch):
            for x, _t in zip(X, t):

                # Forwardpropagation
                self._forwardprop(x)

                # Backpropagation
                for l in reversed(range(1, self.n_layers)): # Iterates through each layer (except input layer)
                    for j in range(self.layers[l].dimensions): # Iterates through each node of layer l
                        self.layers[l].grad[j] = 0

                        for k in range(self.layers[l - 1].dimensions):
                            if (l == self.n_layers - 1): # If layer l is the output layer
                                self.layers[l].grad[j] = 1/len(t[0]) * 2 * (self.layers[l].a[j] - _t[j])
                            else:
                                for _j in range(self.layers[l + 1].dimensions):
                                    self.layers[l].grad[j] += self.layers[l + 1].weights[_j, k] * NeuralNetwork.sigmoid_derived(self.layers[l + 1].a[_j]) * self.layers[l + 1].grad[_j]
                            update_weight(l, j, k)                                
                        update_bias(l, j)

    def _forwardprop(self, x):
        self.layers[0].feedforward(x)
        for l in range(1, self.n_layers):
            self.layers[l].feedforward(self.layers[l].weights, self.layers[l - 1].a)

    def predict(self, x):
        self._forwardprop(x)

        return self.layers[self.n_layers - 1].a


import numpy as np

class HiddenLayer:
    def __init__(self, dimensions, dim_from):
        self.dimensions = dimensions
        self.a = np.zeros((dimensions, 1))
        self.b = np.random.rand(dimensions, 1)
        self.grad = np.zeros((dimensions, 1))
        self.weights = np.random.rand(dimensions, dim_from)

    def feedforward(self, input_weights, input_a):
        for i in range(self.dimensions):
            self.a[i] = 1/(1 + np.exp(-1 * (np.matmul(input_weights[i], input_a) + self.b[i])))


import numpy as np

class InputLayer:
    def __init__(self, dimensions):
        self.dimensions = dimensions
        self.a = np.zeros((dimensions, 1))

    def feedforward(self, x):
        self.a = x


I know it is quit a bit of code - please let me know if there is anything you find ambiguous :)
","1) It appears to be nearly working on you exemple 3. Not reaching exactly 0 or 1 happens because of the sigmoid function, that can only reach 0 and 1 asymptotically. For binary prediction, it is quite usual to put a cut-off on the output, here 0.5 will do : X&lt;0.5 => 0 and X>=0.5 => 1. 

2) As far as I remember this is in line with the theory : you don't need much neurons to reproduce XOR.

3) As to why more complex networks fail, I don't really know. To me your sigmoid derivative looks off. But if it's due to that, it wouldn't explain why it converged on the simpler model.

As a general introduction to practical use of neural nets, I would recomend efficient back-prop from LeCun (https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=2ahUKEwi33Lfiut7nAhWxyIUKHSSLABIQFjAAegQIBBAB&amp;url=http%3A%2F%2Fyann.lecun.com%2Fexdb%2Fpublis%2Fpdf%2Flecun-98b.pdf&amp;usg=AOvVaw26Rjav-SyKYp0gX-SskHTa).
",<machine-learning><python><neural-network><deep-learning><numpy>
"I have the following df

    x1      x2    x3   x4     
   1000   5000    0.8  restaurant1
   2000   7000   0.75  restaurant1
    500   1000    0.5  restaurant2
    700   1400    0.6  restaurant2
   1000   5000    0.8  restaurant2
    100    600    0.9  restaurant3
    200   1200    0.9  restaurant3
     50   1000    0.9  restaurant3


applying a Kmeans Algorithm for 2 clusters what happens is that y:

    x1      x2    x3   x4             Y
   1000   5000    0.8  restaurant1    1
   2000   7000   0.75  restaurant1    1   
    500   1000    0.5  restaurant2    2
    700   1400    0.6  restaurant2    2
   1000   5000    0.8  restaurant2    1
    100    600    0.9  restaurant3    2
    200   1200    0.9  restaurant3    2
     50   1000    0.9  restaurant3    2


Possible Desired Outputs: 

     x1      x2    x3   x4             Y
    1000   5000    0.8  restaurant1    1
    2000   7000   0.75  restaurant1    1   
     500   1000    0.5  restaurant2    2
     700   1400    0.6  restaurant2    2
    1000   5000    0.8  restaurant2    2
     100    600    0.9  restaurant3    2
     200   1200    0.9  restaurant3    2
      50   1000    0.9  restaurant3    2 


or 

     x1      x2    x3   x4             Y
    1000   5000    0.8  restaurant1    1
    2000   7000   0.75  restaurant1    1   
     500   1000    0.5  restaurant2    1
     700   1400    0.6  restaurant2    1
    1000   5000    0.8  restaurant2    1
     100    600    0.9  restaurant3    2
     200   1200    0.9  restaurant3    2
      50   1000    0.9  restaurant3    2


I would like to set this boundary: a restaurant must belong to 1 and only 1 cluster.

I understand why there is this output, but how could I avoid and fix it? 

Below the code that I used in my notebook: 

  #Converting float64 to numpy array 
  x1=df['x1'].to_numpy()
  x2=df['x2'].to_numpy()
  x3=(df['x5']/df['x2']).to_numpy()
  x4=df_joint_raw['x4'].cat.codes.to_numpy()

  X=np.stack((x1,x2,x3,x4),axis=1)
  #Getting clusters
  y_pred=KMeans(n_clusters=2, random_state=0).fit_predict(X)

","Very interesting question! I try my best:

It depends a bit on the number of clusters and number of restaurant but in general I explain a bit.

If the number of restaurants and clusters are the same, then, theoretically, your question has just one trivial answer: ""each restaurant is a cluster"". You even don't need any algorithm. I go a bit deeper on it.

Most of ML algorithms solve an optimization problem to find the answer. Sometimes optimization problems are subject to some constraints. 

Example: 


Cluster restaurants such that all similar restaurants are necessarily assigned to the same cluster.
Cluster restaurants such that the density of same restaurants in same clusters is maximum.


The first one has the trivial answer I gave before but second one can be solved. You run several clustering methods (or just k-means but with several initial conditions) and accept the one in which higher number of similar restaurants are in identical clusters. For this you need to convert ""density of same restaurants in same clusters"" to a mathematical formulation and use it as criterion of choice. If you need help on it just drop a comment so I update the answer.

In any case, you change the output of the clustering and you don't let it ""naturally"" find the clusters as you push a criterion which is not normally considered in the algorithm. But don't worry! The good thing is that at least you have a criterion for ""goodness"" of your clustering which does not exist normally in clustering problem.

UPDATE

Let's try $\chi^2$ test first. It is pretty simplified but try it and if it didn't work we can think of something else. For know how, I prepared it for you in a simple way so you don't get confused with different tutorials on the net.

Imagine you have 4 restaurants and you want 4 clusters. You will end up with such a frequency table which says how many restaurants of which type fall in which cluster:



Then in Python, you simply calculate $\chi^2$ statistic which tells you if clusters and restaurants ""are correlated or not"". 

from scipy.stats import chi2_contingency
obs = np.array([[10,1,2,1], [1,11,0,1], [1,2,8,1], [0,2,2,12]])
chi, p, _,_ = chi2_contingency(obs)
print('The chi-square statistic of {} with p-value of {}'.format(chi,p))


P-value, as you know, tells you if the statistic is significant. There theoretical consideration in this solution but I am not going to confuse you with that. I apologize as I did not go through your proposal in the comment. Will answer accordingly as soon as I find time to have a look at that.

Good Luck!
",<python><clustering><pandas><k-means><numpy>
"Many algorithms and methods in modern Machine Learning techniques contain randomness, and because of that, running the same ML script several times can result in different outputs, therefore accuracy results. For example, running Random Forest can produce accuracy of 0.78, then when run again with no change in data, setup, code, it can result in 0.79. This brings the challenge of inability to perform controlled experiment, when I am testing some changes in input and their effect on output.

So in order to be able to perform perfectly controlled experiments to achieve the best model output, what are the extensive set of random parameters I should fix? I want whole process to be completely deterministic.

PS: I am using Sci-kit Learn environment with additional algorithms such as XGBoost, CatBoost, LightGBM.

I assume there are some parameters, random_state(s) I should fix in NumPy, too.
","Scikit uses numpy for pseudo-random number generation. So to fix random state in various scikit calls, you use numpy.random.seed(12345) and then use scikit. You would want to record the random seed when you log the model so you could reproduce the same run later. 

If your code (or something you call) also uses Python's random number generator, you would set random.seed too.

How to set the seed depends on the library. For example I believe most xgboost APIs expose a seed parameter instead. Not sure about catboost.

You're also depending on the library even exposing a way to seed any pseudo-random choices it makes. It's possible some library doesn't quite do so. (Or in the case of Spark, sometimes the result can even conceivably depend on order of distributed execution, which is hard to control.)
",<machine-learning><scikit-learn><data><data-science-model><numpy>
"The Tensorflow documentation here says that:


  tf.linalg.pinv is ''analogous to numpy.linalg.pinv. It differs only in
  default value of rcond''.
  
  However, tf.linalg.pinv requires the matrix to be float type while
  np.linalg.pinv could be used with complex matrices.


I was wondering why they would only create it for float types and if there is a straightforward way to modify tf.linalg.pinv to be used with complex matrices.
","I just faced the same situation. If you need to explicitly build the inverse, check this paper:

https://pdfs.semanticscholar.org/f278/b548b5121fd0d09c2e589439b97fad16ece3.pdf

In particular, given a Matrix M that you need to invert, you can just do:

    A = tf.math.real(M)
    C = tf.math.imag(M)

    r0  = tf.linalg.pinv(A) @ C
    y11 = tf.linalg.pinv(C @ r0 + A)
    y10 = -r0 @ y11

    M_inverse = tf.cast(tf.complex(y11,y10), dtype = M.dtype)


The complexity is a bit higher than the pure-complex implementation, but so far it has proven to be pretty stable for me.

(just copying my answer from your other post: 
https://stackoverflow.com/questions/60025950/tensorflow-pseudo-inverse-doesnt-work-for-complex-matrices/60128892#60128892)
",<machine-learning><python><tensorflow><numpy><matrix>
"I have 100 2D lists a0 to a99, it is easy to convert them into arrays (I verified already):

print(np.array(a0).shape)   # (2,150)
print(np.array(a1).shape)   # (5,150)
print(np.array(a2).shape)   # (10, 150)
# the sizes of a0 to a99 vary from (1,150) to (10, 150) only


I want to combine these 100 3D arrays into ONE 3D array, for example combined_array:

print(combined_array.shape)  # (100,10,150)
print(combined_array[0].shape) # (2,150)
print(conbined_array[1].shape) # (5,150)
print(combined_array[2].shape) # (10,150)


I use Python 3.7.
","Your arrays have different shapes on the 0 axis, so you cannot use numpy.stack directly.

You can either use padding or put all arrays in a list. Using padding:

import numpy as np

a0 = np.empty((2,150))
a1 = np.empty((5,150))
a2 = np.empty((10,150))

max_shape = [0,0]
for a in [a0, a1, a2]:
    if max_shape[0] &lt; a.shape[0]:
        max_shape[0] = a.shape[0]
    if max_shape[1] &lt; a.shape[1]:
        max_shape[1] = a.shape[1]
arrays = []
for a in [a0, a1, a2]:
    arrays.append(np.pad(a, pad_width=((0, max_shape[0] - a.shape[0]),
                                       (0, max_shape[1] - a.shape[1])),
                         mode='constant'))
stacked_array = np.stack(arrays)

",<python><numpy>
"I have a time series dataset that has several variables for a state/province for fixed periods of time. That is for state A, there are samples from April 2017 to July 2019. Of course, I thought adding precipitation and temperature variables would be a great idea. I tried finding some relevant external data but most of it is abstract and spread out. How would one simulate dynamic data in Python with varying means, highs and lows for say six months on a daily basis, taking into account average temperatures/precipitation for each month?

So if I have the mean temperatures (C) as below for state A:

year    Jan   Feb   Mar   Apr   May   Jun
2017    5.5   6.0   12.0  15.0  20.0  25.0


I would like data to be simulated as below without really doing for each month since that would make the whole task very tedious:

Duration     Temp
2017-01-01   5.0
2017-01-02   5.1
2017-01-03   4.9
.
2017-03-01   7.8
2017-03-02   9.0
2017-03-03   9.5
.
2017-06-30   26.7


Are there ways to achieve this in Python (or R)?
","What about creating a Pandas DataFrame and adding a new column such as ""Temp_simulated"" and simulate the temperature?


",<python><r><data><numpy><simulation>
"I have a time series dataset that has several variables for a state/province for fixed periods of time. That is for state A, there are samples from April 2017 to July 2019. Of course, I thought adding precipitation and temperature variables would be a great idea. I tried finding some relevant external data but most of it is abstract and spread out. How would one simulate dynamic data in Python with varying means, highs and lows for say six months on a daily basis, taking into account average temperatures/precipitation for each month?

So if I have the mean temperatures (C) as below for state A:

year    Jan   Feb   Mar   Apr   May   Jun
2017    5.5   6.0   12.0  15.0  20.0  25.0


I would like data to be simulated as below without really doing for each month since that would make the whole task very tedious:

Duration     Temp
2017-01-01   5.0
2017-01-02   5.1
2017-01-03   4.9
.
2017-03-01   7.8
2017-03-02   9.0
2017-03-03   9.5
.
2017-06-30   26.7


Are there ways to achieve this in Python (or R)?
","I'm not sure it is the best way to do it in r, but you can create a vector simulated temperature for each days of the year by using your reference vector with few temperature by doing the following:

1) You set a dataframe containing few temperatures as references for each month (here, I used lubridate package to manipulate dates):

library(lubridate)
Date = seq(ymd('2019-01-01'),ymd('2020-01-01'),by='months')
Temp_ref = c(5.5,6.0,12,15,20, 25, 25, 20,15,12,6,5.5,5.5)
df_ref &lt;- data.frame(Date,Temp_ref)

         Date Temp_ref
1  2019-01-01      5.5
2  2019-02-01      6.0
3  2019-03-01     12.0
4  2019-04-01     15.0
5  2019-05-01     20.0
6  2019-06-01     25.0
7  2019-07-01     25.0
8  2019-08-01     20.0
9  2019-09-01     15.0
10 2019-10-01     12.0
11 2019-11-01      6.0
12 2019-12-01      5.5
13 2020-01-01      5.5


If you plot it using ggplot2 and passed the function geom_smooth, you can have a simulation of this data every day:

library(ggplot2)
ggplot(df_ref, aes(x = Date, y = Temp_ref))+
  geom_point()+
  geom_smooth()




2) We can recreate this simulation by using loess function:

model &lt;- loess(Temp_ref~as.numeric(Date), data = df_ref)


3) Now, we are using predict function to use this model to define temperature over the year for each day:

library(lubridate)
date &lt;- seq(ymd('2019-01-01'),ymd('2019-12-30'),by='days')
df &lt;- data.frame(date)
df$yfitted &lt;- predict(model2, newdata = as.numeric(df$date))


4) We can confirm our fit by plotting it using ggplot2:

ggplot(df, aes(x = date, y = yfitted))+
  geom_point(size = 1)+
  geom_point(inherit.aes = FALSE, data = df_ref, aes(x = Date, y = Temp_ref), color = ""red"")




I'm not sure it is the perfect way to do it, but I think it gives you a good approximation of the evolution of the temperature over the year based on your reference vector. 

Hope it helps you to figure it out the solution to your question.
",<python><r><data><numpy><simulation>
"I've been trying to implement ""full convolution"" w.r.t to convolution layer inputs. According to this article, it looks like this:



So, I wrote this function: 

def full_convolve(filters, gradient):
    filters = np.ones((5,5))
    gradient = np.ones((8,8))
    result = list()
    output_shape = 12
    filter_r = filters.shape[0] - 1
    filter_c = filters.shape[1] - 1
    gradient_r = gradient.shape[0] - 1
    gradient_c = gradient.shape[1] - 1

    for i in range(0,output_shape):
        if (i &lt;= filter_r):
            row_slice = (0, i + 1)
            filter_row_slice = ( 0 , i + 1)
        elif ( i &gt; filter_r and i &lt;= gradient_r):
            row_slice = (i - filter_r, i + 1)
            filter_row_slice = (0, i + 1)
        else: 
            rest = ((output_shape - 1) -  i )
            row_slice = (gradient_r  - rest, i + 1 )
            filter_row_slice = (0 ,rest + 1)
        for b in range(0,output_shape):
            if (b &lt;= filter_c):
                col_slice = (0, b + 1)
                filter_col_slice = (0, b+1)
            elif (b &gt; filter_c and b &lt;= gradient_c):
                col_slice = (b - filter_c, b + 1)
                filter_col_slice = (0,b+1)
            else:
                rest = (output_shape - 1 ) - b 
                col_slice = (gradient_r - rest , b + 1)
                filter_col_slice = (0, rest + 1)
            r = np.sum(gradient[row_slice[0] : row_slice[1], col_slice[0] : col_slice[1]] * filters[filter_row_slice[0]: filter_row_slice[1], filter_col_slice[0]: filter_col_slice[1]])
            result.append(r)
    result = np.asarray(result).reshape(12,12)


I tested this with ones and the output seems correct (if I get ""full convolution"" right):

[[ 1.  2.  3.  4.  5.  5.  5.  5.  4.  3.  2.  1.]
 [ 2.  4.  6.  8. 10. 10. 10. 10.  8.  6.  4.  2.]
 [ 3.  6.  9. 12. 15. 15. 15. 15. 12.  9.  6.  3.]
 [ 4.  8. 12. 16. 20. 20. 20. 20. 16. 12.  8.  4.]
 [ 5. 10. 15. 20. 25. 25. 25. 25. 20. 15. 10.  5.]
 [ 5. 10. 15. 20. 25. 25. 25. 25. 20. 15. 10.  5.]
 [ 5. 10. 15. 20. 25. 25. 25. 25. 20. 15. 10.  5.]
 [ 5. 10. 15. 20. 25. 25. 25. 25. 20. 15. 10.  5.]
 [ 4.  8. 12. 16. 20. 20. 20. 20. 16. 12.  8.  4.]
 [ 3.  6.  9. 12. 15. 15. 15. 15. 12.  9.  6.  3.]
 [ 2.  4.  6.  8. 10. 10. 10. 10.  8.  6.  4.  2.]
 [ 1.  2.  3.  4.  5.  5.  5.  5.  4.  3.  2.  1.]]


However, I don't like all these manual checks and if/else statements. I feel there is a better way to implement this in NumPy (perhaps, using some zero paddings or something like this). Can anyone suggest a better approach? Thanks
","Code:

import numpy as np
from scipy import signal

j5 = np.ones((5,5))
j8 = np.ones((8,8))

c58 = signal.convolve2d(j5, j8, boundary='fill')  # by default filled with 0, which is correct for your case


Results in:

print(c58)
[[ 1.  2.  3.  4.  5.  5.  5.  5.  4.  3.  2.  1.]
 [ 2.  4.  6.  8. 10. 10. 10. 10.  8.  6.  4.  2.]
 [ 3.  6.  9. 12. 15. 15. 15. 15. 12.  9.  6.  3.]
 [ 4.  8. 12. 16. 20. 20. 20. 20. 16. 12.  8.  4.]
 [ 5. 10. 15. 20. 25. 25. 25. 25. 20. 15. 10.  5.]
 [ 5. 10. 15. 20. 25. 25. 25. 25. 20. 15. 10.  5.]
 [ 5. 10. 15. 20. 25. 25. 25. 25. 20. 15. 10.  5.]
 [ 5. 10. 15. 20. 25. 25. 25. 25. 20. 15. 10.  5.]
 [ 4.  8. 12. 16. 20. 20. 20. 20. 16. 12.  8.  4.]
 [ 3.  6.  9. 12. 15. 15. 15. 15. 12.  9.  6.  3.]
 [ 2.  4.  6.  8. 10. 10. 10. 10.  8.  6.  4.  2.]
 [ 1.  2.  3.  4.  5.  5.  5.  5.  4.  3.  2.  1.]]


Reference to see other options when you'd need them:
https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.convolve2d.html
",<machine-learning><python><convolution><backpropagation><numpy>
"I have a NumPy array of strings: 'n', 'y', wanna convert it into integer array of 0, 1, how to convert it?

imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
X = imp.fit_transform(X)

X

array([['n', 'y', 'n', ..., 'y', 'n', 'y'],
       ['n', 'y', 'n', ..., 'y', 'n', 'y'],
       ['n', 'y', 'y', ..., 'y', 'n', 'n'],
       ...,
       ['n', 'y', 'n', ..., 'y', 'n', 'y'],
       ['n', 'n', 'n', ..., 'y', 'n', 'y'],
       ['n', 'y', 'n', ..., 'y', 'n', 'n']], dtype=object)

","It is quite trivial

(X=='y').astype(int)


Should do the trick. It simply converts your array to True or False according to your requirements and then astype will impose the required datatype. By default int will give you 1 for True and 0 for False.
",<python><numpy>
"I have a NumPy array of strings: 'n', 'y', wanna convert it into integer array of 0, 1, how to convert it?

imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
X = imp.fit_transform(X)

X

array([['n', 'y', 'n', ..., 'y', 'n', 'y'],
       ['n', 'y', 'n', ..., 'y', 'n', 'y'],
       ['n', 'y', 'y', ..., 'y', 'n', 'n'],
       ...,
       ['n', 'y', 'n', ..., 'y', 'n', 'y'],
       ['n', 'n', 'n', ..., 'y', 'n', 'y'],
       ['n', 'y', 'n', ..., 'y', 'n', 'n']], dtype=object)

","You could use the following code:

X[X=='y'] = 1
X[X=='n'] = 0


This replaces the indexes of 'y' with 1 and of 'n' with 0.
Generally the X=='y' returns a Boolean array which contains True where the 'y' and False everywhere else and so on.
",<python><numpy>
"I have a NumPy array of strings: 'n', 'y', wanna convert it into integer array of 0, 1, how to convert it?

imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
X = imp.fit_transform(X)

X

array([['n', 'y', 'n', ..., 'y', 'n', 'y'],
       ['n', 'y', 'n', ..., 'y', 'n', 'y'],
       ['n', 'y', 'y', ..., 'y', 'n', 'n'],
       ...,
       ['n', 'y', 'n', ..., 'y', 'n', 'y'],
       ['n', 'n', 'n', ..., 'y', 'n', 'y'],
       ['n', 'y', 'n', ..., 'y', 'n', 'n']], dtype=object)

","You can use np.vectorize as this answer explains:

In [1]: import numpy as np

In [2]: a = np.array([['n', 'y', 'n', 'n', 'y', 'n', 'y'],
   ...:        ['n', 'y', 'n', 'n', 'y', 'n', 'y'],
   ...:        ['n', 'y', 'y', 'y', 'y', 'n', 'n']])
   ...:        

In [3]: my_map = {'n': 0, 'y': 1}

In [4]: np.vectorize(my_map.get)(a)
Out[4]: 
array([[0, 1, 0, 0, 1, 0, 1],
       [0, 1, 0, 0, 1, 0, 1],
       [0, 1, 1, 1, 1, 0, 0]])

",<python><numpy>
"I have a NumPy array of strings: 'n', 'y', wanna convert it into integer array of 0, 1, how to convert it?

imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
X = imp.fit_transform(X)

X

array([['n', 'y', 'n', ..., 'y', 'n', 'y'],
       ['n', 'y', 'n', ..., 'y', 'n', 'y'],
       ['n', 'y', 'y', ..., 'y', 'n', 'n'],
       ...,
       ['n', 'y', 'n', ..., 'y', 'n', 'y'],
       ['n', 'n', 'n', ..., 'y', 'n', 'y'],
       ['n', 'y', 'n', ..., 'y', 'n', 'n']], dtype=object)

","you can use np.where to replace 'n' and 'y'. Please go through the documentation here,

https://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html

Also, please try to be more elaborate in your questions such that right solutions can be reached.
",<python><numpy>
"From Machine Learning for Absolute Beginners: A Plain English Introduction:


  Contained in each column is a feature. A feature is also known as variable, a dimension or an attribute - but they all mean the same thing.


From here (the supplement file for this book): 


In NumPy, each dimension is called an axis.
The number of axes is called the rank.


For example, the above 3x4 matrix is an array of rank 2 (it is 2-dimensional).
The first axis has length 3, the second has length 4.

An array's list of axis lengths is called the shape of the array.


For example, the above matrix's shape is (3, 4).
The rank is equal to the shape's length.

The size of an array is the total number of elements, which is the product of all axis lengths (eg. 3*4=12)


Question: Is the dataframe dimension completely different not related to the NumPy dimension (just same word but describing different concept)?

I am learning Python and Machine learning but familial with R and R dataframe from statistical perspective
","The dataframe case refers to the linear algebraic notion of a dimension.  In the NumPy context, it just means the number of axes or rank.
",<machine-learning><python><r><numpy>
"I'm facing the below error while importing NumPy library. I have tried all of the suggestions advised by Python in its error log, but no luck.

Strangely, when I use Python's OS library and print the ""PATH"" environment variable, it appends the below automatically, which I don't find in the Windows Environment Variable GUI.
Afraid if it's a bug causing this.

;;C:\Users\test-apple\AppData\Local\Programs\Python\Python37\lib\site-packages\numpy\.libs


Error log --

Python 3.7.6 (tags/v3.7.6:43364a7ae0, Dec 19 2019, 00:42:30) [MSC v.1916 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license()"" for more information.

&gt;&gt;&gt; import os
&gt;&gt;&gt; import numpy

Traceback (most recent call last):
  File ""C:\Users\test-apple\AppData\Local\Programs\Python\Python37\lib\site-packages\numpy\core\__init__.py"", line 24, in &lt;module&gt;
    from . import multiarray
  File ""C:\Users\test-apple\AppData\Local\Programs\Python\Python37\lib\site-packages\numpy\core\multiarray.py"", line 14, in &lt;module&gt;
    from . import overrides
  File ""C:\Users\test-apple\AppData\Local\Programs\Python\Python37\lib\site-packages\numpy\core\overrides.py"", line 7, in &lt;module&gt;
    from numpy.core._multiarray_umath import (
ImportError: dynamic module does not define module export function (PyInit__multiarray_umath)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""&lt;pyshell#1&gt;"", line 1, in &lt;module&gt;
    import numpy
  File ""C:\Users\test-apple\AppData\Local\Programs\Python\Python37\lib\site-packages\numpy\__init__.py"", line 142, in &lt;module&gt;
    from . import core
  File ""C:\Users\test-apple\AppData\Local\Programs\Python\Python37\lib\site-packages\numpy\core\__init__.py"", line 54, in &lt;module&gt;
    raise ImportError(msg)
ImportError: 

IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!

Importing the numpy c-extensions failed.
- Try uninstalling and reinstalling numpy.
- If you have already done that, then:
  1. Check that you expected to use Python3.7 from ""C:\Users\test-apple\AppData\Local\Programs\Python\Python37\pythonw.exe"",
     and that you have no directories in your PATH or PYTHONPATH that can
     interfere with the Python and numpy version ""1.18.0"" you're trying to use.
  2. If (1) looks fine, you can open a new issue at
     https://github.com/numpy/numpy/issues.  Please include details on:
     - how you installed Python
     - how you installed numpy
     - your operating system
     - whether or not you have multiple versions of Python installed
     - if you built from source, your compiler versions and ideally a build log

- If you're working with a numpy git repository, try `git clean -xdf`
  (removes all files not under version control) and rebuild numpy.

Note: this error has many possible causes, so please don't comment on
an existing issue about this - open a new one instead.

Original error was: dynamic module does not define module export function (PyInit__multiarray_umath)

&gt;&gt;&gt; 
&gt;&gt;&gt;


Please advise if any prior experiencing this.
","I couldn't diagnose the actual reason behind this problem, but circumvented it by installing Anaconda - which comes with huge number of packages pre-installed.

If anything is missing, can be added using Conda or Pip installer. Anaconda has an excellent package manager.

So, if anyone in future (unfortunately) lands on this question, please have a look into Anaconda if it works for you.

Happy learning!
",<python><numpy><python-3.x>
"I currently have a pandas dataframe with the following format

    Model      Metric                     Value
--------------------------------------------------------------
0     Ours     Accuracy  [0.79, 0.79, 0.82]
1   Theirs     Accuracy  [0.68, 0.56, 0.64]
2     Ours  Sensitivity  [0.64, 0.55, 0.55]
3   Theirs  Sensitivity  [0.82, 0.82, 0.78]
4     Ours  Specificity  [0.68, 0.48, 0.6] 
5   Theirs  Specificity  [0.68, 0.48, 0.6]


In the evaluation script I am writing I want to be able to take into account situations where the training of a model is repeated multiple times, the results are stored in an numpy array (Value column). For visualization with seaborn, i believe i need a long form where it looks something like this:

    Model      Metric                     Value
--------------------------------------------------------------
0     Ours     Accuracy                   0.79
1     Ours     Accuracy                   0.79
2     Ours     Accuracy                   0.82
3     Theirs   Accuracy                   0.68
4     Theirs   Accuracy                   0.56
5     Theirs   Accuracy                   0.64
6     Ours     Sensitivity                0.64
...   ....     ...                        ...


I cannot figure out how to do this.
","# create the df
df = pd.DataFrame({'model':['our','theirs'],'metric':['acc','sen'],'value':[np.array([1,2,3]),np.array([4,5,6])]})
# unzip the array into three columns
df[['ex1','ex2','ex3']]=pd.DataFrame(df['value'].to_list(),index=df.index)
# melt df to long df
df.melt(id_vars=['model','metric'],value_vars=['ex1','ex2','ex3'])


You may need to sort the dataframe as you like.
",<python><pandas><numpy>
"I'm trying to calculate the covariance matrix for a dummy dataset using the following formula, but it's not matching with the actual result.



Let's say the dummy dataset contains three features, #rooms, sqft and #crimes. Each column is a feature vector, and we have 5 data points. I'm creating this dataset using the following code:

matrix = np.array([[2., 3., 5., 1., 4.], 
                   [500., 700., 1800., 300., 1200.], 
                   [2., 1., 2., 3., 2.]])


Let's normalize the data, so the mean becomes zero.

D = matrix.shape[0]
for row in range(D):
    mean, stddev = np.mean(matrix[row,:]), np.std(matrix[row,:])   
    matrix[row,:] = (matrix[row,:] - mean)/stddev


Now, I can write a naive covariance calculator that looks at all possible pairs of features, and that works perfectly.

def cov_naive(X):
    """"""Compute the covariance for a dataset of size (D,N) 
    where D is the dimension and N is the number of data points""""""
    D, N = X.shape
    covariance = np.zeros((D, D))

    for i in range(D):
        for j in range(i, D):                      
            x = X[i, :]
            y = X[j, :]
            sum_xy = np.dot(x, y) / N
            if i == j:
                covariance[i, j] = sum_xy
            else:                        
                covariance[i, j] = covariance[j, i] = sum_xy
    return covariance


But, if I try to implement the formula mentioned in the beginning, the result is incorrect. The method I am trying out is as follows:

def cov_naive_2(X):
    """"""Compute the covariance for a dataset of size (D,N) 
    where D is the dimension and N is the number of data points""""""
    D, N = X.shape
    covariance = np.zeros((D, D))

    for i in range(N):                     
        x = X[:, i]
        covariance += x @ x.T
    return covariance / N


What am I doing wrong here?

Expected output:

array([[ 1.        ,  0.96833426, -0.4472136 ],
       [ 0.96833426,  1.        , -0.23408229],
       [-0.4472136 , -0.23408229,  1.        ]])


Actual output from cov_naive_2

array([[3., 3., 3.],
       [3., 3., 3.],
       [3., 3., 3.]])

","Your approach is mathematically right, your problem comes from the fact that numpy matrix multiplication defaults to inner product when providing vectors, independently from them being transposed to row or column.

Try modifying the penultimate line like this to force outer product.

def cov_naive_2(X):
    """"""Compute the covariance for a dataset of size (D,N) 
    where D is the dimension and N is the number of data points""""""
    D, N = X.shape
    covariance = np.zeros((D, D))

    for i in range(N):                     
        x = X[:, i]
        covariance += np.outer(x, x)  # &lt;---- here
    return covariance / N
```

",<python><statistics><numpy>
"I wrote a vectorized Gradient descent implementation of the linear regression model. The Dataset looks something like:

It's Not Working properly as I am getting negative R Squared error I don't understand why ?? Should I decrease Alpha or No. of Iterations or is there any problem in my implementation what should I do?

My Regression plot looks something like below I don't know why I am getting such a line.


My Cost Function Error plot with respect to the number of iterations in Gradient descent looks something like below


R Squared Error is: -3.744682246118262

My Code Snippet:

import numpy as np
from sklearn.model_selection import train_test_split
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd


def CostFunction(Theta,DataMatrix):
    Size = DataMatrix.shape[0]
    Error = 0
    for i in range(0,Size):
        Feature = np.vstack(([1],np.array(DataMatrix[i][:-1]).reshape(-1,1)))
        Error += (np.transpose(Theta).dot(Feature) - DataMatrix[i][-1]) ** 2
    return (1/(2*Size))*Error

def GradientDescent(Theta,Alpha,DataMatrix,Iterations):
    Progress = []
    Iterate = 0
    Size = DataMatrix.shape[0]
    Error = np.zeros((DataMatrix.shape[1],1))
    while(Iterations):
        for i in range(0,Size):
            Feature = np.vstack(([1],np.array(DataMatrix[i][:-1]).reshape(-1,1)))        #Last Entry is Label Thats Why
            Error += (np.transpose(Theta).dot(Feature) - DataMatrix[i][-1])*Feature
        Theta -= Alpha*(1/Size)*Error
        if(Iterations % 10 == 0):
            Progress.append([Iterate,CostFunction(Theta,DataMatrix)])
            Iterate += 10
        Iterations -= 1
    return [Theta,Progress]

def ProgressCurve(Progress):
    Progress = [[i[0],i[1].ravel()[0]] for i in Progress]
    sns.lineplot(x = np.array(Progress)[:,0],y =  np.array(Progress)[:,1],marker = '*')
    plt.show()

def Prediction(Theta,Test):
    Predicted = []
    for i in range(0,Test.size):
        Feature = np.vstack(([1],np.array(Test[i]).reshape(-1,1)))
        Predicted.append(np.transpose(Theta).dot(Feature))
    return Predicted

def Error_Metric(Actual,Predicted):
    Actual = np.array(Actual,dtype = 'float64').reshape(-1,1)
    Predicted = np.array(Predicted,dtype = 'float64').reshape(-1,1)
    Error = (Actual - Predicted) ** 2
    Variance = (Actual - np.mean(Actual)*np.ones((Actual.shape[0],1))) ** 2
    return (1 - np.sum(Error)/np.sum(Variance))

def RegressionLine(X,Y,Orig_X,Orig_Y):
    Y = [i[0].ravel()[0] for i in Y]
    sns.scatterplot(x = Orig_X,y = Orig_Y,color = ""blue"")
    sns.lineplot(x = X,y = Y,color = ""red"")
    plt.show()

X = 2*np.random.rand(1000)
Y = 4 + 3*X + np.random.randn(1000)
X_Train,X_Test,Y_Train,Y_Test = train_test_split(X,Y,test_size = 0.3,random_state = 0)
DataFrame = pd.DataFrame()
DataFrame['X'] = X_Train
DataFrame['Y'] = Y_Train
DataMatrix = DataFrame.as_matrix()
ThetaParams = np.random.randn(2,1)
Theta,Progress = GradientDescent(ThetaParams,0.001,DataMatrix,50)
Prediction_Out = Prediction(Theta,np.array(X_Test))
Error = Error_Metric(Y_Test,Prediction_Out)
ProgressCurve(Progress)
RegressionLine(X_Test,Prediction_Out,X,Y)
print(Error)

","It seems like number of iterations is very small compare to learning rate. (50 vs 0.001). The optimizer can't converge when we look at the fitted line plot. 

Try to increase number of iterations, it will help you. You can check this beautiful work and my another answer about the problem. 
",<linear-regression><gradient-descent><numpy><seaborn><python-3.x>
"I am trying to generate a sample of random numbers from a custom distribution 

$$ p(x) = x^{n}e^{-xtn}. $$

After reading the tutorial on scipy's website, I wrote a subclass which I called kbayes:

class kbayes(rv_continuous):
    def _pdf(self, x, t, n):
        p = x**n * np.exp(-t*n*x)
        s = np.sum(p)
        return p/s


The line s=np.sum(p) is there to normalize the distribution.

The pdf seems to be ok when I check it on some numbers: running the following code

ks = np.logspace(-5, -2.3, 1000)
p = kb._pdf(ks, 500, 12)
hist, _ = np.histogram(p, ks)
plt.plot(ks, p)


results in



which is what I expect.

But when I try to generate a sample, via e.g.

p_test = kb.rvs(size=1000, t=500, n=12)


I get the following error:

/home/[redacted]/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in double_scalars
  """"""
/home/[redacted]/anaconda3/lib/python3.6/site-packages/scipy/integrate/quadpack.py:385: IntegrationWarning: The occurrence of roundoff error is detected, which prevents 
  the requested tolerance from being achieved.  The error may be 
  underestimated.
  warnings.warn(msg, IntegrationWarning)


I understand that this distribution has a vert small practical range (where the probability is higher than $10^{-10}$), but I expected the rv_continuous class to be able to cope with that.

I would appreciate any input on how to make this work, or any other way to draw a sample from this distribution using python. 
","That is a warning which can be ignored. Below is a cleaned-up example that generates samples and ignores that warning.
from   warnings import filterwarnings

import matplotlib.pyplot as plt
import numpy as np
from   scipy.stats import rv_continuous

filterwarnings('ignore')

class kbayes_gen(rv_continuous):
    def _pdf(self, x, t, n):
        p = x**n * np.exp(-t*n*x)
        s = np.sum(p)
        return p/s

kb = kbayes_gen()    
ks = np.logspace(-5, -2.3, 1000)
p = kb._pdf(x=ks, t=500, n=12)
plt.plot(ks, p);

samples = kb.rvs(size=1000, t=500, n=12)

",<python><statistics><numpy><scipy>
"

In the above sample data, I have empty fields and now the task is to fill the fields with previous values.
my columns are dates and the values are a number of items present for that particular article for the specific date.
which would be a faster way to interpolate the missing fields. Any suggestions to build the function is appreciated. 
","You can use fillna function with ffill method:

df.fillna(method='ffill', inplace=True)

",<python><time-series><pandas><numpy><interpolation>
"I wrote an algorithm for generating node embeddings based on the graph's topology. Most of the explanation is done in the readme file and the examples.

The question is:
Am I reinventing the wheel?
Does this approach have any practical advantages over existing solutions for embeddings generation?

Yes, I'm aware there are many algorithms for this based on random walks, but this one is pure deterministic linear algebra and it is quite simple, from my perspective.

In short, the algorithm is heavily inspired by PageRank.
Every node is described by its proximity vector which contains the node's closeness number to every other node or some selected subset of the nodes.
The ""closeness"" is not just the simple shortest distance.

Here's brief explanation from the repo's readme for directed/undirected unweighted graph (the idea generalizes to weighted graphs pretty intuitively):


Every node is assigned a vector. For node i the vector's j-th element is a number representing its closeness to node j (you can think of it as signal strength).
The closeness of the node i to the node j (the ""central"" node the signal flows from) is defined as a sum of the signal strengths of the neighboring nodes multiplied by the damping factor parameter. (The dumping factor works like a kind of distance penalty)
Signal a node ""emits"" to other connected nodes equal to the given signal's strength in the node itself divided by the number of edges it is able to emit to.
Initially, the node j is assigned a closeness (to itself) equal to 1.


The whole thing is computed by solving sparse linear systems.

""Signal"" strengths from node 4 to other nodes. Note that the signal here travel in direction opposite to edge direction (like ""follow"" relation on Instagram)


","So I think that it is important to realize that pagerank utilizes the eigenvalues of the nodes to speed up computation. The thing is that turns out to be equivalent to a random walk. Based on what you mentioned in your question it sounds like you are describing a random walk in your 4 part procedure. The fact that you are solving it using linear algebra rather than simulating the random walk is somewhat irrelevant. It doesn't seem to me that there is anything particularly unique doing this via linear algebra. In fact, it would surprise me if anyone is actually simulating the random walks vs. doing the equivalent linear algebra like you are doing, because it is much more efficient to do the linear algebra.
",<python><numpy><graphs><embeddings><representation>
"I wrote an algorithm for generating node embeddings based on the graph's topology. Most of the explanation is done in the readme file and the examples.

The question is:
Am I reinventing the wheel?
Does this approach have any practical advantages over existing solutions for embeddings generation?

Yes, I'm aware there are many algorithms for this based on random walks, but this one is pure deterministic linear algebra and it is quite simple, from my perspective.

In short, the algorithm is heavily inspired by PageRank.
Every node is described by its proximity vector which contains the node's closeness number to every other node or some selected subset of the nodes.
The ""closeness"" is not just the simple shortest distance.

Here's brief explanation from the repo's readme for directed/undirected unweighted graph (the idea generalizes to weighted graphs pretty intuitively):


Every node is assigned a vector. For node i the vector's j-th element is a number representing its closeness to node j (you can think of it as signal strength).
The closeness of the node i to the node j (the ""central"" node the signal flows from) is defined as a sum of the signal strengths of the neighboring nodes multiplied by the damping factor parameter. (The dumping factor works like a kind of distance penalty)
Signal a node ""emits"" to other connected nodes equal to the given signal's strength in the node itself divided by the number of edges it is able to emit to.
Initially, the node j is assigned a closeness (to itself) equal to 1.


The whole thing is computed by solving sparse linear systems.

""Signal"" strengths from node 4 to other nodes. Note that the signal here travel in direction opposite to edge direction (like ""follow"" relation on Instagram)


","I would not be so skeptical. Yes, there is lots of research on random walk based recommender/ranking systems, including those similar to PageRank. Yes, spectral methods and linear algebra are researched for such problems very well. And a sparse matrix is a natural data structure for this problem, so you'd find many implementations that exploit it.
But if this particular type of random walks has not been tried before (although it does look from your description that it might be the usual random walk that just selects an outgoing edge at random) and/or especially if it works practically to solve some real problem or improves on some benchmark, your algorithm might still be interesting. But to get any serious people to have a look at it you should analyze it, link to existing research and compare with other existing variations and present in a paper or a blog post.
A random set of references to start from, see also the papers they cite or the papers they are cited by:
Linyuan Lü and Tao Zhou, Link prediction in complex networks: A survey, Physica A: statistical mechanics and its applications 390 (2011), 1150-1170.
Chapter 14 of David Easley and Jon Kleinberg, Networks, crowds, and markets, Cambridge university press (2010).
Maurizio Ferrari Dacrema, Paolo Cremonesi and Dietmar Jannach, Are we really making much progress? A worrying analysis of recent neural recommendation approaches, In Proceedings of the 13th ACM Conference on Recommender Systems, 101-109 (2019).
",<python><numpy><graphs><embeddings><representation>
"I have a high number of npy files (448 files) each consisting of around 12k frames (150x150 RGB images) which together make the input to my neural network (X). However, since it is impossible to load all of the files into a single array, and the fact that it is necessary to shuffle all of the samples to avoid bias, how do I create the input and feed it to the network? Someone suggested creating a dummy array to represent indices, shuffle that, create chunks based on the array size and the indices and then feed the chunks to the neural network. However, I was wondering if there is another simpler method.
So in a word, I would like to do this step but with a high number of large npy files:

X_train_filenames, X_val_filenames, y_train, y_val = train_test_split(...)


Note1: Some suggested using TFRecords but I could not find out how to convert and use them.
","All Deep Learning libraries have data loading APIS that can lazily way to load data.
You mention TFRecords so I assume you are using TensorFlow. You can use TensorFlow's data API.
",<python><deep-learning><tensorflow><preprocessing><numpy>
"Below is the numpy array. I need to perform two operations on this array.


Add one column with value [column 1] - [column 3].
Add another column with value [column 1] - [previous value of column1].


I can do this using normal list operations, but is it possible to use numpy or pandas? If so, how can it be done?

Input data:

[['78' '3412' '98' '3441']
 ['106' '3412' '127' '3434']
 ['139' '3411' '160' '3434']
 ['170' '3411' '191' '3442']
]

","These types of operations can easily be done using both numpy or pandas. However, in this case I would recommend pandas since it is more intuitive. Using the example array we can create a pandas dataframe:

arr = np.array([[78, 3412, 98, 3441], [106, 3412, 127, 3434], [139, 3411, 160, 3434], [170, 3411, 191, 3442]])
df = pd.DataFrame(arr, columns=['a', 'b', 'c', 'd'])


The two new columns can now be added as follows:

df['e'] = df['a'] - df['c']
df['f'] = df['a'].diff(1)




Directly using numpy, one possible way would be to do:

arr = np.c_[ arr, arr[:,0] - arr[:,2], np.append(np.NaN, arr[1:, 0] - arr[:-1, 0]) ] 

",<python><pandas><numpy>
"I'm trying to compute the theta for a regression linear exercice.

x = x * 1.0
y = y * 1.0

# add ones
X = np.ones((201, 2))
X[:, 1] = x

# convert to matrix
Y = y[:,np.newaxis]

# compute teta
p1 = (X.dot(X.T))**-1
p2 = (X.T).dot(Y)

theta = p1.dot(p2)


The last line failed with error : 


  ValueError: shapes (201,201) and (2,1) not aligned: 201 (dim 1) != 2
  (dim 0)


I don't understand why. I'm simply trying to implement this:



here my x

[ 69.  95.  21.  20.  33.  13.  17.  27.  32.  26.  25.  31.  18.  24.
  25.  25.  27.  37.  28.  39.  31.  25.  25.  29.  38.  16.  20.  50.
  37.  33.  40.  46.  45.  19.  45.  56.  60.  23.  49.  51.  48.  51.
  41.  47.  45.  47.  37.  52.  45.  43.  26.  49.  49.  39.  60.  65.
  60.  60.  47.  57.  65.  33.  56.  69.  61.  60.  47.  49.  60.  72.
  70.  59.  30.  39.  25.  68.  63.  78.  50.  70.  75.  60.  70.  28.
  62.  90.  78.  80.  72.  78.  62.  80.  80.  75.  68.  76.  81.  92.
  75.  82.  80.  95.  85.  58.  33.  94. 100.  77.  80.  80.  92.  92.
  99.  98.  90.  96.  92.  86.  73.  80.  96.  72.  91.  53.  60.  95.
  97. 103. 105. 112. 110. 107.  65.  97. 110. 102. 101.  98. 109. 120.
 107. 125. 120. 108. 130. 112. 121. 118. 107.  87. 114. 110. 118. 126.
 112.  93.  80. 116. 145.  95. 100.  98.  94. 110. 100. 110. 109. 131.
 133.  86.  84. 145. 113. 130. 108.  94. 136. 140. 125. 156.  91. 130.
 102. 130. 142.  88. 178. 150. 185. 164. 214. 191. 156. 145. 175. 150.
 129. 160. 201. 240. 145.]


And my y :

[1810. 2945.  685.  720.  830.  850.  850.  855.  875.  890.  890.  900.
  900.  900.  920.  930.  950.  955.  960.  970.  980.  980.  980. 1000.
 1015. 1040. 1060. 1100. 1130. 1160. 1200. 1210. 1235. 1250. 1310. 1315.
 1320. 1350. 1370. 1385. 1390. 1400. 1400. 1410. 1410. 1415. 1420. 1445.
 1450. 1450. 1470. 1480. 1490. 1530. 1530. 1580. 1590. 1590. 1595. 1630.
 1640. 1650. 1660. 1690. 1690. 1690. 1700. 1700. 1700. 1715. 1730. 1750.
 1750. 1750. 1780. 1790. 1790. 1790. 1800. 1810. 1830. 1840. 1840. 1850.
 1860. 1870. 1920. 1930. 1940. 1950. 1980. 1990. 2000. 2030. 2040. 2060.
 2080. 2085. 2090. 2130. 2130. 2145. 2160. 2160. 2170. 2190. 2250. 2270.
 2270. 2290. 2320. 2335. 2335. 2358. 2360. 2380. 2380. 2390. 2400. 2400.
 2403. 2410. 2420. 2425. 2490. 2500. 2530. 2550. 2550. 2550. 2560. 2570.
 2570. 2590. 2625. 2635. 2675. 2700. 2710. 2710. 2720. 2725. 2750. 2805.
 2820. 2825. 2830. 2840. 2840. 2850. 2850. 2870. 2875. 2900. 2915. 2945.
 2950. 3050. 3050. 3080. 3090. 3090. 3100. 3150. 3160. 3180. 3220. 3300.
 3300. 3350. 3400. 3450. 3490. 3500. 3525. 3570. 3765. 3765. 3790. 3930.
 3950. 3965. 4061. 4200. 4260. 4310. 4760. 4800. 4900. 5160. 5200. 5229.
 5250. 5383. 5460. 5500. 5560. 5775. 6200. 6700. 7383.]

","p1 = (X.dot(X.T))**-1 needs to be p1 = np.linalg.inv((X.T.dot(X))) 

X**-1 doesn't invert a matrix, this is equivalent to 1/X (elementwise) and you need to transpose the first not the second matrix.
",<linear-regression><numpy>
"I'm new to programming with Python, and so far it's been a headache to create a build environment- and need your support and expertise in this area.

Background I'm running a MacBook, and using Sublime Text 2, and needing to learn python. I'd like to finish this tutorial on youtube on Data Science (manipulating excel files really ""aggregating data on excel""- i'll post the link below and need to import pandas- but apparently I can't.

Also, worth mentioning; I'll also be using Numpy, Xlrd, Matplotlib in the future, and not sure if these modules are available on Sublime Text 2.

The Challenge:

When I run this line, I get an error.

import pandas as pd

I have researched the problem, and it seems that I don't have package control on my sublime text and found this site with the package control addition- so here's the website that I found with a long code to download the package.

https://packagecontrol.io/installation#st2

1) Is this a legit site to download this software?

2) Can I even add these modules to Sublime Text 2 to do Data Science &amp; view excel documents

3) If I add this installation ""package control"", will this package allow me to download pandas, and other modules into Sublime Text 2

4) If Yes, where can I find instructions for installing the Pandas/Numpy/Xlrd modules?

https://www.youtube.com/watch?v=4_BPNnKEMn8
","Sublime Text is a text editor - it is independent of your Python setup. I would suggest following a tutorial on setting up Anaconda on your machine, then follow a second tutorial on making sublime text find the Anaconda Python executable.


Here is a tutorial for setting up Python with Anaconda on a Mac.
Here is something I found with a simple search (disclaimer: I don't know anything about sublime text!)


Once you have anaconda installed, you would need to run this command in terminal to install the packages you need:

conda install numpy pandas xlrd matplotlib

",<machine-learning><python><pandas><numpy>
"I’m facing some (RAM) memory issues to train a neural network.

I have an input array consisting of grayscale images encoded as the type numpy.uint8 (therefore the whole range 0-255 can be covered). When feeding the data to the network, I’m supposed to normalize the values into the 0-1 range, making them now floating points (numpy.float64 or 32). However, this conversion makes my data 8x(or 4x) bigger, which my RAM memory can’t handle.

Would there be a way I could overcome this issue?   

Thanks !
","After normalization, cast the numbers to float16 or bfloat16. Those are the smallest floats available in Keras.
",<neural-network><keras><numpy>
"I have column 'ABC' which has 5000 rows. Currently, dtype of column is object. Mostly it has string values but some values dtype is not string, I want to find all those rows and modify those rows. Column is as following:

1 abc
2 def
3 ghi
4 23
5 mno
6 null
7 qwe
8 12-11-2019
...
...
...
4900 ert
5000 tyu


In above case, I can use for loop to find out rows which do not have desired dtype. I just wanted to know, is their better way to solve this issue.

Note: I am using Pandas.
","You can get the type of the entries of your column with map:

df['ABC'].map(type)


So to filter on all values, which are not stored as str, you can use:

df['ABC'].map(type) != str


If however you just want to check if some of the rows contain a string, that has a special format (like a date), you can check this with a regex like:

df['ABC'].str.match('[0-9]{4}-[0-9]{2}-[0-9]{2}')


But of course, that is no exact date check. E.g. it would also return True for values like 0000-13-91, but this was only meant to give you an idea anyways.
",<python><pandas><data-cleaning><numpy>
"I tried to implement a Deep fully connected neural network for binary classification using python and numpy and used Gradient Descent as optimization algorithm. 

Turns out my model is heavily under fitting, even after 1000 epochs. My loss never improves beyond 0.69321, i tried checking my weight derivatives and instantly realized they're very small ( as small as 1e-7 ), such small gradients are causing my model to never have bigger gradient descent updates and never reach the global minima. I will detail out the math/pseudo code for forward and backward propagation's, please let me know if I'm on the right track. I will follow the naming convention used in DeepLearning.ai By Andrew Ng.

Say we have 4 layer neural network with only one node at the  output layer to classify between 0/1.

X -&gt; Z1 - &gt; A1 - &gt; Z2 - &gt; A2 - &gt; Z3 - &gt; A3 - &gt; Z4 - &gt; A4

Forward propagation

Z1 = W1 dot_product X + B1
A1 = tanh_activation(Z1)

Z2 = W2 dot_product A1 + B2
A2 = tanh_activation(Z2)

Z3 = W3 dot_product A2 + B3
A3 = tanh_activation(Z3)

Z4 = W4 dot_product A3 + B4
A4 = sigmoid_activation(Z4)


Backward Propagation

DA4 = -( Y / A4 + (1 - Y /  1 - A4 ) ) ( derivative of output activations or logits w.r.t to loss function )

DZ4 = DA4 * derivative_tanh(Z4) ( derivative of tanh activation, which I assume is ( 1 - (Z4 ) ^ 2 ) )
Dw4 = ( dZ4 dot_produt A3.T ) / total_number_of_samples
Db4 = np.sum(DZ4, axis = 1, keepdims = True ... ) / total_number_of_samples
DA3 = W4.T dot_product(DZ4)


DZ3 = DA3 * derivative_tanh( Z3 )
DW3 = ( DZ3 dot_product A2.T ) / total_number_of_samples
DB3 = np.sum( DZ3, .. ) / total_number_of_samples
DA2 = W3.T dot_product(DZ3)


DZ2 = DA2 * derivative_tanh( Z2 )
DW2 = ( DZ2 dot_product A1.T ) / total_number_of_samples
DB2 = np.sum( DZ2, .. ) / total_number_of_samples
DA1 = W2.T dot_product(DZ2)



DZ1 = DA1 * derivative_tanh( Z1 )
DW1 = ( DZ1 dot_product X.T ) / total_number_of_samples
DB1 = np.sum( DZ1, .. ) / total_number_of_samples



After the above back propagation steps I updated the weights and biases using gradient descent with their respective derivatives. But, no matter how many times I run the algorithm, the model never improves it's loss beyond 0.69 and the derivatives of output weights ( in my case dW4 ) is pretty low 1e-7. I'm assuming that either my derivative_tanh function  or my calculations of dZ is really off, which is causing bad loss values to propagate back to the network. Please share your thoughts whether my implementation of backprop is valid or not. TIA. I went through 
back propagation gradient descent calculus

and 

how to optimize weights of neural network .. and many other blogs, but couldn't find for what I was looking for.
","The problem faced with back propagation with activation functions that has a max limit used in a Deep Neural Network (i.e. more then 2 layer network) is that the deeper the network is the quicker your back propagation will degrade.

Take for example your tanh activation function


If say you backpropagate from the output layer to the hidden layer and your tanh derivative is 0.25 then the next layer will only be limited by 0.25 from your tanh function your derived. Since you compute the next layer's derivative which in this case can be 0.5 of the total input and the previous layer's derivative totals to 0.25 then your derivative would be a total of 0.125. By the time the back propagation reaches the input layer, the weights would change by such a small fraction that it does not even matter. 

This is why the ReLu function was such a big breakthrough as it does not have a max limit. There should be methods to counter this effect but ReLu is way more effective. 
",<python><neural-network><deep-learning><gradient-descent><numpy>
"I tried to implement a Deep fully connected neural network for binary classification using python and numpy and used Gradient Descent as optimization algorithm. 

Turns out my model is heavily under fitting, even after 1000 epochs. My loss never improves beyond 0.69321, i tried checking my weight derivatives and instantly realized they're very small ( as small as 1e-7 ), such small gradients are causing my model to never have bigger gradient descent updates and never reach the global minima. I will detail out the math/pseudo code for forward and backward propagation's, please let me know if I'm on the right track. I will follow the naming convention used in DeepLearning.ai By Andrew Ng.

Say we have 4 layer neural network with only one node at the  output layer to classify between 0/1.

X -&gt; Z1 - &gt; A1 - &gt; Z2 - &gt; A2 - &gt; Z3 - &gt; A3 - &gt; Z4 - &gt; A4

Forward propagation

Z1 = W1 dot_product X + B1
A1 = tanh_activation(Z1)

Z2 = W2 dot_product A1 + B2
A2 = tanh_activation(Z2)

Z3 = W3 dot_product A2 + B3
A3 = tanh_activation(Z3)

Z4 = W4 dot_product A3 + B4
A4 = sigmoid_activation(Z4)


Backward Propagation

DA4 = -( Y / A4 + (1 - Y /  1 - A4 ) ) ( derivative of output activations or logits w.r.t to loss function )

DZ4 = DA4 * derivative_tanh(Z4) ( derivative of tanh activation, which I assume is ( 1 - (Z4 ) ^ 2 ) )
Dw4 = ( dZ4 dot_produt A3.T ) / total_number_of_samples
Db4 = np.sum(DZ4, axis = 1, keepdims = True ... ) / total_number_of_samples
DA3 = W4.T dot_product(DZ4)


DZ3 = DA3 * derivative_tanh( Z3 )
DW3 = ( DZ3 dot_product A2.T ) / total_number_of_samples
DB3 = np.sum( DZ3, .. ) / total_number_of_samples
DA2 = W3.T dot_product(DZ3)


DZ2 = DA2 * derivative_tanh( Z2 )
DW2 = ( DZ2 dot_product A1.T ) / total_number_of_samples
DB2 = np.sum( DZ2, .. ) / total_number_of_samples
DA1 = W2.T dot_product(DZ2)



DZ1 = DA1 * derivative_tanh( Z1 )
DW1 = ( DZ1 dot_product X.T ) / total_number_of_samples
DB1 = np.sum( DZ1, .. ) / total_number_of_samples



After the above back propagation steps I updated the weights and biases using gradient descent with their respective derivatives. But, no matter how many times I run the algorithm, the model never improves it's loss beyond 0.69 and the derivatives of output weights ( in my case dW4 ) is pretty low 1e-7. I'm assuming that either my derivative_tanh function  or my calculations of dZ is really off, which is causing bad loss values to propagate back to the network. Please share your thoughts whether my implementation of backprop is valid or not. TIA. I went through 
back propagation gradient descent calculus

and 

how to optimize weights of neural network .. and many other blogs, but couldn't find for what I was looking for.
","@SandMan  Thank you for all the suggestions, I did check the Loss, DZ4 and DW4 values before posting this question, as you suspected the problem start's much earlier i.e during my calculation of DZ4, my implementation sigmoid_derivative seemed to be not quite to the point, which caused very low values to propagate back through the network and my gradient descent was never able to take bigger steps towards global minima. I was able to fix it and achieve 100% accuracy with 0.2% loss, I was about to post an answer with my corrections. I will detail out the pseudo code here:

Forward Propagation

Z1 = W1 dot_product X + B1
A1 = relu_activation(Z1)

Z2 = W2 dot_product A1 + B2
A2 = relu_activation(Z2)

Z3 = W3 dot_product A2 + B3
A3 = relu_activation(Z3)

Z4 = W4 dot_product A3 + B4
A4 = sigmoid_activation(Z4)



Backward Propagation

DA4 = -( Y / A4 + (1 - Y /  1 - A4 ) ) ( derivative of output activations or logits w.r.t to loss function )

DZ4 = DA4 * sigmoid_derivative(Z4) ( derivative of sigmoid activation, which I missed initially )
Dw4 = ( dZ4 dot_produt A3.T ) / total_number_of_samples
Db4 = np.sum(DZ4, axis = 1, keepdims = True ... ) / total_number_of_samples
DA3 = W4.T dot_product(DZ4)


DZ3 = derivative_relu(DA3, Z3 )
DW3 = ( DZ3 dot_product A2.T ) / total_number_of_samples
DB3 = np.sum( DZ3, .. ) / total_number_of_samples
DA2 = W3.T dot_product(DZ3)


DZ2 = derivative_relu( DA2, Z2 )
DW2 = ( DZ2 dot_product A1.T ) / total_number_of_samples
DB2 = np.sum( DZ2, .. ) / total_number_of_samples
DA1 = W2.T dot_product(DZ2)



DZ1 = derivative_relu(DA1, Z1 )
DW1 = ( DZ1 dot_product X.T ) / total_number_of_samples
DB1 = np.sum( DZ1, .. ) / total_number_of_samples



My Sigmoid derivative before

def sigmoid_derivative(x):
 return x * ( 1 - x )


My Sigmoid derivative after

def simoid_derivative(x):
 return np.multiply(sigmoid(x), (1 - sigmoid(x) ) 


Thanks once again for your comments.
",<python><neural-network><deep-learning><gradient-descent><numpy>
"Am trying to develop a python script which reads a large CSV file (approx 1.2 GB historical data) in chunks and performs following steps:


Take backup of the file
Extract new records of previous day transactions,append to original/base CSV file and store the data in dataframe.
Perform mathematical operations on the big dataframe
Convert big dataframe to CSV and store in same location for nex day processing.


Similar process runs next day as well and so on...

I'm getting memory exception error while processing the step 3 above (probably step 1 &amp; 2 would have consumed most of the memory max-3GB limited space). Even if I extend the space in the server, I foresee a problem as my input file size is going to be increase daily.

I need to dip through historical data daily for mathematical operations along with daily transactions so can't avoid storing/accessing of base CSV file which contains historical data and can't afford to buy space in cloud as well.

I have used pd.read_csv for reading records from CSV file in chunks, for mathematical operations pandas and numpy has been used.

The script runs without any issue on my local machine, however I have the memory issue while processing on the server. So, as far as the code goes, it seems to be good.

I believe that if I can change the processing in steps 1 to 4 to be more efficient, the memory exception can be solved and with limited space my script can be executed.

Can someone suggest the best way to handle the above steps?
","I had also faced similar issues when my python script was processing my 12GB and 35GB files. It used take weeks for processing and many times failed.  

You need to check your mathematical operation, if it can work on subset of data. In my case, I finally was able to split the file in multiple pieces and run the mathematical operation on individual pieces. It helped improving the speed as well as better exceptional handling.  

If you cant split your file, then check if your code can work in batches. Then you can read your file in chunks (in pd.read_csv you can supply chunk size) and then feed to your formula piece by piece.
",<python><pandas><numpy>
"Since these libraries can turn CPU arrays into GPU tensors, could you parallelize (and therefore accelerate) the calculations for a decision tree? I am considering making a decision tree class written in Tensorflow/Pytorch for a school project, but I want to be certain that it makes sense. 
","Not directly, mostly because the structure of a decision tree doesn't lend itself to GPU parallelisation, and is better suited to CPUs. Even the most established decision tree algorithms use CPUs.

GPUs can only perform a small subset of operations at high performance, the most important being matrix multiplication, which is the fundamental building block of a neural network.
Decision trees don't train by gradient descent, but an iterative process of partitioning the dataset. As such, they need access to the entire dataset in memory (rather than batches).
Decision trees are not easily differentiable without amendment.

The Deep Neural Decision Forest paper does address these issues in a simple and clever way:
https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/ICCV15_DeepNDF_main.pdf
This is interesting as it does allow trees to be learnt with gradient descent, but it's unlikely to be faster than existing CPU bound tree algorithms.
",<tensorflow><decision-trees><pytorch><numpy><parallel>
"Here's a code I wrote for pre-processing a data set. It works
import numpy as np
import pandas as pd
from sklearn import svm

%matplotlib inline
import matplotlib.pyplot as plt


from sklearn.impute import SimpleImputer
import seaborn as sns; sns.set(font_scale=1.2)

stock=pd.read_csv(&quot;C:/Users/Dulangi/Downloads/winequality-red.csv&quot;)
stock.head()
X= stock.iloc[:,0:5].values
y= stock.iloc[:,5].values

g=sns.lmplot('alcohol','quality',data=stock,height=7, truncate=True, scatter_kws={&quot;s&quot;:100})
imputer = SimpleImputer( strategy = &quot;mean&quot;)

imputer = imputer.fit(X[:,1:2])
imputer.fit_transform(X[:,1:2])

imputer = imputer.fit(X[:,4:5])
imputer.fit_transform(X[:,4:5])

I want to know what if i have both strings and numeric data in one column, how to pre-process such data to have all numeric data?
","Normally pandas should identify numeric types automatically. If it doesn't in your case, there seems to be a formatting issue. 

Instead of reading everything into string, I'd rather try to enable pandas to read the types directly in the correct form. First I'd try to pass the column types in a dict to dtype, as in:

pd.read_csv(file_name, dtype={'int_column_name': 'int32', 'float_column_name': 'float32'})


If that is not applicable, because the format is not recognized by pandas automatically, you might want to try some other options. 

E.g. if it's just the decimal point or the thousand separator that is different in your data, you can set it over the corresponding keywords in read_csv (thousands and/or decimal). 

If that isn't sufficient because you have some special formatting etc., you can also pass your own converter to parse the string into the data type you require for the column, like this as an example of how you could parse numeric values with a currency unit:

import re
mon_re=re.compile('(?P&lt;value&gt;[0-9.]*)([^0-9].*)?')
def strip_off_currency(currency_string):
    m=mon_re.match(currency_string)
    if m is not None:
        return np.float32(m.group('value'))
    else:
        return np.NaN

pd.read_csv(file_name, dtype={'int_column_name': 'int32', 'float_column_name': 'float32'}, converters={'monetary_ammount': strip_off_currency})


The reason why I would prefer this way over reading everything into memory and processing it there is, that it needs less memory and probably also is faster if you process the values just once (at least if you don't need to pass converters).
",<python><scikit-learn><pandas><numpy>
"I have two matrix V_r of shape(19, 300) and vecs of shape(100000, 300). I would like to subtract rows of V_r from from rows of vecs. Currently, I am achieving this with the following code. Is there a way to do it using broadcasting?

    list=[]
for v in V_r:
    a=np.linalg.norm(vecs-v,axis=1)
    list.append(a)
M=np.vstack(list)

","This worked for me:

np.linalg.norm(vecs[:, :, None] - V_r[None, :, :].transpose(0, 2, 1), axis=1)

",<python><numpy>
"I have two matrix V_r of shape(19, 300) and vecs of shape(100000, 300). I would like to subtract rows of V_r from from rows of vecs. Currently, I am achieving this with the following code. Is there a way to do it using broadcasting?

    list=[]
for v in V_r:
    a=np.linalg.norm(vecs-v,axis=1)
    list.append(a)
M=np.vstack(list)

","This is what you're looking for :  np.linalg.norm(vecs-V_r[:,np.newaxis], axis = 2)
",<python><numpy>
"In order to install some libraries, we just use:

conda install pandas


While in other cases we use:

conda install -c conda-forge html5lib


What is the difference between them?
","conda-forge is just an alternative channel where you can upload to or download packages from. Usually it makes no difference where you download the package from but sometimes conda-forge has the latest version.
",<machine-learning><python><pandas><numpy><anaconda>
"I have a 3*3 Adjacency matrix and I'm trying to sum the elements of each column and divide each column element by that sum to get the transition matrix. Can you please help me code this part? Thanks in advance. 
","Considering a is your adjacency matrix 2D numpy array :

a / a.sum(axis=0)


Should do the trick (divide all elements by columns sum)
",<numpy>
"I cannot find anything in the documentation but it was used in some starter code for a class I am taking at school.
Upon testing per below, it seems to reverse the order of the dimensions of an numpy array.

pic = np.ones((3,4,5))
print(pic.shape,""\n"", pic)
#new_shape is the reverse of pic.shape
new_shape = pic.shape[::-1]
print(new_shape)


This outputs:

(3, 4, 5) 
 [[[1. 1. 1. 1. 1.]
  [1. 1. 1. 1. 1.]
  [1. 1. 1. 1. 1.]
  [1. 1. 1. 1. 1.]]

 [[1. 1. 1. 1. 1.]
  [1. 1. 1. 1. 1.]
  [1. 1. 1. 1. 1.]
  [1. 1. 1. 1. 1.]]

 [[1. 1. 1. 1. 1.]
  [1. 1. 1. 1. 1.]
  [1. 1. 1. 1. 1.]
  [1. 1. 1. 1. 1.]]]
(5, 4, 3)


In the starter code it was used in this way:

res_img = cv2.resize(img, sub_img.shape[::-1])


There are some other tricks that are planted in the starter code (to show us that they are there I'm guessing).  Here is another example:

channels = blue, green, red = np.moveaxis(color_img, 2, 0) #move channels to pos 0


The np.moveaxis() moved channels to the first dim position but it's assigned to blue, green and red at the same time via Python multiple assignment?  When I looked up multiple assignment, I only saw stuff like var1, var2 = 1, 2.  Also np.moveaxis() returns an array with the new shape so I have to stretch my imagination to assume the first axis can be assigned to variables.  Is this correct? 

My instructor just replied that np.split() does the splitting but the documentation does not say anything about this being implicitly called when np.moveaxis() is called.  

Does anyone know what is really happening?  
","Note that when you do:

pic.shape[::-1]


You only reverse the elements of the shape are of pic. So pic.shape is (3,4,5] and reversed, it's (5, 4, 3).

What happens with the multiple assignment is just that you can assign the elements along the first index to individual variables. For example:

pic = np.array( [['x1','x2','x3'],['y1','y2','y3']]) 


The first index can be 0 or 1, the second index can be 0, 1 or 2. I can assign the x-vector to a and the y-vector to be like this:

a, b = pic

In [18]: a
Out[18]: array(['x1', 'x2', 'x3'], dtype='&lt;U2')
In [19]: b
Out[19]: array(['y1', 'y2', 'y3'], dtype='&lt;U2')


The call to moveaxis reformats the image, by moving axis 0 to position 2 in the array of axes. My pic doesn't have a position 2 (since it only has 2 dimensions), but I can swap axis 0 with axis 1:

In [11]: np.moveaxis(pic, 1, 0)
Out[11]: 
array([['x1', 'y1'],
   ['x2', 'y2'],
   ['x3', 'y3']], dtype='&lt;U2')


And now I can assign (x1, y1) to a, (x2, y2) to b and (x3, y3) to c:

a, b, c = np.moveaxis(pic, 1, 0)

In [14]: a
Out[14]: array(['x1', 'y1'], dtype='&lt;U2')


There is no call to split anywhere here. I think what confused you is just the multiple assignment from a numpy array, which allows the direct assignment of columns to individual variables. In combination with moveaxis this would allow you to split the red, green and blue images in a single command.
",<numpy>
"I want to have the same weights for layer initializations in all my networks, so that when I'm comparing their first epoch loss they all start from the same value. Is there a way in keras to do this? 

I have set the random seed for the numpy and tensorflow, but still I get different results in initializations.
","You need to specify the seed in the initializer, e.g:

from keras.initializers import RandomUniform

seed = 0

model.add(Dense(64, kernel_initializer = RandomUniform(minval = -0.05, 
                                                       maxval =  0.05, 
                                                       seed = seed)))

",<keras><tensorflow><numpy><weight-initialization>
"I have this dataset. I want to create a dataframe from the dataset which starts with a particular integers.

This is a sample:


box = {'colors': ['Green','Yellow','Green','Blue','Blue','Red','Red','Red','Green','Blue', 'Red', 'Yellow','Green',
                  'Green','Green','Yellow','Green','Green','Green','Green','Green'],
      'shapes': ['Rectangle','Circle','Rectangle','Square','Rectangle','Rectangle','Square','Rectangle','Square',
                 'Square','Rectangle','Circle','Square','Square','Square','Rectangle', 'Rectangle','Square','Rectangle',
                 'Square','Circle'],
      'Terminal_ID': [5061739764654, 90908743645343, 50614354347865, 50617343645343, 90908746543363, 50617489752564, 
                      50617443645343, 50614464234764, 9764654, 50614343645343, 50614446543363, 50614389752564, 
                      50614443645343, 64234764,5061439764654, 90908743645343, 50614346543363, 87500089752564, 50617443645343,
                      50614364234764, 8750009764654]}

df2 = pd.DataFrame(box, columns = ['Terminal_ID','colors', 'shapes'])


**NOTE: ** The objective is to form a dataframe from this dataset whose Terminal_ID starts with $506143$ and $506173$
","To find out these numbers in the data frame coloumn, you could try the following steps:


Split each row of the data frame column in a array of strings [1];
Merge the six first strings of each array [2];
Compare the numbers you are looking for with the array of merged numbers [3];
Select the positions of the rows that begins with the numbers desired [4].


import pandas as pd

box = {'colors': ['Green','Yellow','Green','Blue','Blue','Red','Red','Red','Green','Blue', 'Red', 'Yellow','Green',
              'Green','Green','Yellow','Green','Green','Green','Green','Green'],
  'shapes': ['Rectangle','Circle','Rectangle','Square','Rectangle','Rectangle','Square','Rectangle','Square',
             'Square','Rectangle','Circle','Square','Square','Square','Rectangle', 'Rectangle','Square','Rectangle',
             'Square','Circle'],
  'Terminal_ID': [5061739764654, 90908743645343, 50614354347865, 50617343645343, 90908746543363, 50617489752564, 
                  50617443645343, 50614464234764, 9764654, 50614343645343, 50614446543363, 50614389752564, 
                  50614443645343, 64234764,5061439764654, 90908743645343, 50614346543363, 87500089752564, 50617443645343,
                  50614364234764, 8750009764654]}

df2 = pd.DataFrame(box, columns = ['Terminal_ID','colors', 'shapes'])

num1 = 506143
num2 = 506173

# 1
def digits(number, base=10):
    assert number &gt;= 0
    if number == 0:
        return [0]
    l = []
    while number &gt; 0:
        l.append(str(number % base))
        number = number // base
    l.reverse()
    return l

# 2
digit = [''.join(digits(x)[0:6]) for x in df2['Terminal_ID']]

# 3
pos = [i for i, j in enumerate(digit) if j in (str(num1), str(num2))]

# 4
df3 = df2.loc[pos, ['colors', 'shapes','Terminal_ID']]
print(df3)
#     colors     shapes     Terminal_ID
# 0    Green  Rectangle   5061739764654
# 2    Green  Rectangle  50614354347865
# 3     Blue     Square  50617343645343
# 9     Blue     Square  50614343645343
# 11  Yellow     Circle  50614389752564
# 14   Green     Square   5061439764654
# 16   Green  Rectangle  50614346543363
# 19   Green     Square  50614364234764


",<python><pandas><numpy>
"I have this dataset. I want to create a dataframe from the dataset which starts with a particular integers.

This is a sample:


box = {'colors': ['Green','Yellow','Green','Blue','Blue','Red','Red','Red','Green','Blue', 'Red', 'Yellow','Green',
                  'Green','Green','Yellow','Green','Green','Green','Green','Green'],
      'shapes': ['Rectangle','Circle','Rectangle','Square','Rectangle','Rectangle','Square','Rectangle','Square',
                 'Square','Rectangle','Circle','Square','Square','Square','Rectangle', 'Rectangle','Square','Rectangle',
                 'Square','Circle'],
      'Terminal_ID': [5061739764654, 90908743645343, 50614354347865, 50617343645343, 90908746543363, 50617489752564, 
                      50617443645343, 50614464234764, 9764654, 50614343645343, 50614446543363, 50614389752564, 
                      50614443645343, 64234764,5061439764654, 90908743645343, 50614346543363, 87500089752564, 50617443645343,
                      50614364234764, 8750009764654]}

df2 = pd.DataFrame(box, columns = ['Terminal_ID','colors', 'shapes'])


**NOTE: ** The objective is to form a dataframe from this dataset whose Terminal_ID starts with $506143$ and $506173$
","Do this

 df2['Terminal_ID'] = df2['Terminal_ID'].astype('str')
 df2[df2['Terminal_ID'].str.contains(""506143| 506173"", na=False)]

",<python><pandas><numpy>
"I am new to neural network. I'm trying to train word embeddings without using word2vec package. 

Using titles from reddit worldnews dataset I'm have done some CBOW representation. 

For window size three here is some of my outputs:

0 Context : ['scores', 'killed', 'pakistan'] ---&gt; Target: clashes
1 Context : ['japan', 'resumes', 'refuelling'] ---&gt; Target: mission
2 Context : ['us', 'presses', 'egypt'] ---&gt; Target: gaza
3 Context : ['presses', 'egypt', 'gaza'] ---&gt; Target: border


For Vocab size = 513, I've collected 369 of target words against 369 of 3-grams context words. Each context word is one-hot codded of length 513. 

Therefore, my dataset length becomes:
X.shape = (369, 3, 1, 513)
Y.shape = (369, 1, 513)

Now I'm having trouble in fitting the data in neural network. My Neural Network model is constructed with keras.

# create model
model = Sequential()
model.add(Dense(100, input_dim=1, init=369 'uniform' , activation= 'sigmoid' ))
model.add(Dense(1, init= 'uniform' , activation= 'sigmoid' ))
model.compile(loss= 'binary_crossentropy' , optimizer= 'sgd' , metrics=['accuracy'])

#train
history = model.fit(X, Y, nb_epoch=100)


Raised Error:

ValueError: Error when checking input: expected dense_9_input to have 2 dimensions, but got array with shape (369, 3, 1, 513)

","This shows you did not properly flatten your output before the final dense layer. Your first model.add should be an embedding layer since that represents word vectors that'll be trained. The last layer should be the dense one. Hard to tell on the spot which hidden layers to consider, but try GlobalAveragePooling and Dense with the relu activation.
",<neural-network><keras><word2vec><numpy>
"I am trying to multiply two array in python 3.7 using numpy by using the following syntax: 

array1 = np.array([{1,2,3,4},{5,6,7,8}]) 
print (array1)  
array2=array1*array1 
print(array2)  


but this error arises

TypeError: unsupported operand type(s) for *: 'set' and 'set'

","You are simply defining your array so that it is made of python sets. That is a different data structure which is not able to be  multiplied, unlike an array.

Just change your code to this:

array1 = np.array([[1,2,3,4],[5,6,7,8]])


The only difference is using square brackets instead of curly ones. These are python list objects (or standard arrays).

array2=array1*array1  
print(array2)

[[ 1  4  9 16]
 [25 36 49 64]]

",<python><numpy>
"

This image shows my data in .yaml format. I want to export in column
","Something like:

import yaml
import pandas as pd

data = """"""
thermal_properties:
- temp: 0.0
  free_energy: 10

- temp: 1.0
  free_energy: 11

- temp: 2.0
  free_energy: 12
""""""
x = yaml.load(data)

pd.DataFrame(x['thermal_properties']).to_excel('file.xlsx')


should do the job.
",<python><numpy>
"TSV(Tab separated Value) extension file can't be uploaded to google colab using pandas

Used this to upload my file

import io
df2 = pd.read_csv(io.BytesIO(uploaded['Filename.csv']))

import io
stk = pd.read_csv(io.BytesIO(uploaded['train.tsv']))


What i want is a tsv file should be uploaded and read into the dataframe stk
","You first need to upload your file. The io.BytesIO only reads from the uploaded. So first run:

from google.colab import files
uploaded = files.upload()


and select the file you would like to upload.

Also, when you load it into your pandas, you need the sep='\t':

tsk = pd.read_csv(io.BytesIO(uploaded['train.tsv']), sep='\t')

",<machine-learning><pandas><numpy><jupyter><colab>
"I am doing some supervised learning using neural networks, and i have a Targets array containing 1906 samples, which contain 664 unique values. min. count of each unique value==2, by design. Is there a smarter way to split this dataset into train and test, using a leaveoneout scheme to pick randomly 1 sample from each class and put it in the test set and use the rest for training, before i get down to explicitly iterating over all my values? I am using python, numpy, sklearn and pytorch btw! 
","Never mind! Found it! 

just doing a,indices,counts=np.unique(y) will return the starting indices of each unique value in indices, and then you can just slice this off as x_test=X[indices], and remember to delete it using np.delete(X,indices)
",<python><neural-network><scikit-learn><cross-validation><numpy>
"I am fitting a regression model on randomly generated X1,x2 and Y be the sum of x1, x2 but I am getting this error


  ValueError: Found input variables with inconsistent numbers of samples: [2, 10000000]


Note:- I am doing this only for learning purposes

My code:-

X = np.random.random_integers(100000000,size=(2,10000000))
X=(X-(100000000/2))/(100000000/2) # Scaling [-1,1]
Y = X[0]+X[1]

regr = linear_model.LinearRegression()
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, 
random_state=0)
regr.fit(X_train,Y_train)

","You need the shape of your training data to be (num_samples, num_features) or in your case (10000000, 2). An easy way to fix this is to transpose X before feeding it to the train test split:

X_train, X_test, y_train, y_test = train_test_split(X.T, Y, test_size=0.2, random_state=0)
regr.fit(X_train, y_train)  # &lt;-- also 'y_train' not 'Y_train' here

",<machine-learning><python><scikit-learn><numpy>
"This  issue that  I am facing is very simple yet weird and has troubled me to no end.

I have a dataframe as follows : 

df['datetime'] = df['datetime'].dt.tz_convert('US/Pacific')
df.head()

        vehicle_id  trip_id                                 datetime    
6760612 1000500 4f874888ce404720a203e36f1cf5b716    2017-01-01 10:00:00-08:00       
6760613 1000500 4f874888ce404720a203e36f1cf5b716    2017-01-01 10:00:01-08:00    
6760614 1000500 4f874888ce404720a203e36f1cf5b716    2017-01-01 10:00:02-08:00      
6760615 1000500 4f874888ce404720a203e36f1cf5b716    2017-01-01 10:00:03-08:00       
6760616 1000500 4f874888ce404720a203e36f1cf5b716    2017-01-01 10:00:04-08:00   


I am trying to find out the datatime difference as follows ( in two different ways) :

df['datetime_diff'] = df['datetime'].diff()

df['time_diff'] = (df['datetime'] - df['datetime'].shift(1)).astype('timedelta64[s]')


For a particular trip_id, I have the results as follows : 

df[trip_frame['trip_id'] == '4f874888ce404720a203e36f1cf5b716'][['datetime','datetime_diff','time_diff']].head()

                   datetime         datetime_diff   time_diff
6760612 2017-01-01 10:00:00-08:00   NaT             NaN
6760613 2017-01-01 10:00:01-08:00   00:00:01        1.0
6760614 2017-01-01 10:00:02-08:00   00:00:01        1.0
6760615 2017-01-01 10:00:03-08:00   00:00:01        1.0
6760616 2017-01-01 10:00:04-08:00   00:00:01        1.0


But for some other trip_ids like the below, you can observe that I am having the datetime difference as zero (for both the columns) when it is actually not.There is a time difference in seconds.

df[trip_frame['trip_id'] == '01b8a24510cd4e4684d67b96369286e0'][['datetime','datetime_diff','time_diff']].head(4)

         datetime            datetime_diff  time_diff
3236107 2017-01-28 03:00:00-08:00   0 days  0.0
3236108 2017-01-28 03:00:01-08:00   0 days  0.0
3236109 2017-01-28 03:00:02-08:00   0 days  0.0
3236110 2017-01-28 03:00:03-08:00   0 days  0.0

df[df['trip_id'] == '01c2a70c25e5428bb33811ca5eb19270'][['datetime','datetime_diff','time_diff']].head(4)

        datetime             datetime_diff  time_diff
8915474 2017-01-21 10:00:00-08:00   0 days  0.0
8915475 2017-01-21 10:00:01-08:00   0 days  0.0
8915476 2017-01-21 10:00:02-08:00   0 days  0.0
8915477 2017-01-21 10:00:03-08:00   0 days  0.0


Any leads as to what the actual issue is ? I will be very grateful.

Update - I tried out @n1k31t4's approach and got the following results for those problematic rows. Did not even do the time-zone conversion. It is still so weird and surprising.

                        datetime    timestamps    timestamp_diffs    date_diffs
3236107 2017-01-28 11:00:00+00:00   1485601200000000    0.0          0 days
3236108 2017-01-28 11:00:01+00:00   1485601201000000    0.0          0 days
3236109 2017-01-28 11:00:02+00:00   1485601202000000    0.0          0 days
3236110 2017-01-28 11:00:03+00:00   1485601203000000    0.0          0 days
3236111 2017-01-28 11:00:04+00:00   1485601204000000    0.0          0 days

","It is hard to see where the problem is with your code; given your comments, my guess is a problem in the timezone conversion.

I cannot see a problem exactly, buy my suggestion to help debug your situation would be to convert all time to timestamps - by default that will be seconds since the epoch (January 1st 1970). These are then simply normal float64 values (I convert to integer microseconds below). If your differences are still returning zero, subtracting simple number, then the problem cannot be in the differences.

Here is a minimal working example. A simple dateframe with timestamps with millisecond frequency:

import pandas as pd
from datetime import datetime

In [1]: df = pd.DataFrame(pd.date_range(start=datetime(2016,1,1,0,0,1), 
    ...:     end=datetime(2016,1,1,0,0,2), freq='ms'), columns=[""dates""]).head(10)    # just take first 10 rows for simplicity


Make a new column, converting the dates into microseconds since the epoch, as an integer:

In [2]: df[""timestamps""] = df.dates.apply(lambda x: int(datetime.timestamp(x) * 1e6))                              

In [3]: df                                                                                                         
Out[3]: 
                    dates        timestamps
0 2016-01-01 00:00:01.000  1451602801000000
1 2016-01-01 00:00:01.001  1451602801001000
2 2016-01-01 00:00:01.002  1451602801002000
3 2016-01-01 00:00:01.003  1451602801003000
4 2016-01-01 00:00:01.004  1451602801004000
5 2016-01-01 00:00:01.005  1451602801005000
6 2016-01-01 00:00:01.006  1451602801006000
7 2016-01-01 00:00:01.007  1451602801007000
8 2016-01-01 00:00:01.008  1451602801008000
9 2016-01-01 00:00:01.009  1451602801009000


You could at this point check that there are no duplicates in any of your columns, using:

In [4]: df.T.duplicated()                                                                                          
Out[4]: 
dates         False
timestamps    False
dtype: bool


If there are duplicates, that could be the cause of differences equal to zero.

Now compute the differences, in my case all 1-millisecond differences (1000 microseconds):

In [5]: df[[""date_diffs"", ""timestamp_diffs""]] = df.diff(1)                                                        

In [6]: df                                                                                                        
Out[6]: 
                    dates        timestamps      date_diffs  timestamp_diffs
0 2016-01-01 00:00:01.000  1451602801000000             NaT              NaN
1 2016-01-01 00:00:01.001  1451602801001000 00:00:00.001000           1000.0
2 2016-01-01 00:00:01.002  1451602801002000 00:00:00.001000           1000.0
3 2016-01-01 00:00:01.003  1451602801003000 00:00:00.001000           1000.0
4 2016-01-01 00:00:01.004  1451602801004000 00:00:00.001000           1000.0
5 2016-01-01 00:00:01.005  1451602801005000 00:00:00.001000           1000.0
6 2016-01-01 00:00:01.006  1451602801006000 00:00:00.001000           1000.0
7 2016-01-01 00:00:01.007  1451602801007000 00:00:00.001000           1000.0
8 2016-01-01 00:00:01.008  1451602801008000 00:00:00.001000           1000.0
9 2016-01-01 00:00:01.009  1451602801009000 00:00:00.001000           1000.0


Add a sample zero difference and retrieve the index of zero differences:

In [7]: df.iloc[3, 3] = 0.0                                                                                       

In [8]: np.where(df == 0)                                                                                         
Out[8]: (array([3]), array([3]))


Hopefully that will be enough to find where there could actually be zero difference. If all of that works out without zero differences, I would either look into your timezone conversion (maybe they do some rounding there?) or report a bug to the pandas issues



EDIT

After your update trying my debugging method, and seeing this:



I believe there must be either a bug or an inherent limitation to your system.

Pandas Bug

A bug might be in your specific version of Pandas and its df.diff() method. Check your version of Pandas with pd.__version__ and look on the issues page I linked above for any clues ... maybe just try the latest stable version anyway.

32-bit system

Another possible solution would be that you are running on a 32-bit system and so could actually lose the precision required for my example above.
A 32-bit integer can only retain precision for 10 digits. My timestamps above require 12 digits. You can find out on like this, on Linux or Windows or Mac.

Additionally, you could retry my example, but only look at seconds instead of microseconds, just to be sure.
",<python><pandas><numpy><dataframe>
"I'm working on an Airbnb dataset in order to predict the nightly price--where each example is one Airbnb listing (ie a listing for someone's house, room, apartment, etc). Each feature is a characteristic of the listing (ie # bedrooms, # bathrooms, average review score etc).

One of the features is 'amenities', which are the amenities offered for the listing (ie wifi, tv, kitchen, etc). Each example contains a string of its own amenities like so:

{TV,""Cable TV"",Wifi,""Air conditioning"",Kitchen,Elevator,Heating,Washer,Dryer,""Smoke detector"",""Fire extinguisher"",Essentials,Shampoo,Hangers,""Hair dryer"",Iron,""Laptop friendly workspace"",""Self check-in"",Keypad,""Private living room"",""Hot water"",""Bed linens"",Microwave,""Coffee maker"",Refrigerator,Dishwasher,""Dishes and silverware"",""Cooking basics"",Oven,Stove,""Long term stays allowed""}


Dtype is a string ('O' in pandas).

I plan to remove the {} on the ends and the "" quote signs using pd.Series.str.replace() for each character--unless those are useful to keep.

But past that I'm not sure of the best way to go about handling the feature itself.

My questions is:

Should I create individual binary columns for each amenity (ie 1 in column ""TV"" if TV is an amenity, 0 in column ""wifi"" if wifi isn't offered)? I'm hesitant because that would add a few dozen columns. If I should do this, how can I convert the current format to binary columns with Pandas?
","This might Pandas convert a column of list to dummies.
You can try using the MultiLabelBinarizer class from sklearn.
",<python><pandas><numpy>
"I want my index which is mixed up (i just sorted out ratings from highest to lowest) to be in ascending (sorted) order corresponding to my sorted ratings. How can I do that?
","After sorting, try:

df = df.reset_index(drop = True)

",<pandas><numpy>
"I am new to ML and as I take courses for the area DL, I am wondering, by our choice of activation function for the last layer, whether we take sigmoid, relu or softmax, would the formula for calculation of cost function change? 

I am grateful for every good reply I can get, have a nice day! :)
","The cost function doesn't change the activation function but is limits the activation function you can use on the output layer.
For example for a classification problem you will want to output a probability will which is between 0 and 1 so you will take a softmax as the output layer activation function, if you are looking at a regression problem then you will use linear activation function etc 
",<deep-learning><classification><numpy><activation-function><cost-function>
"I am new to ML and as I take courses for the area DL, I am wondering, by our choice of activation function for the last layer, whether we take sigmoid, relu or softmax, would the formula for calculation of cost function change? 

I am grateful for every good reply I can get, have a nice day! :)
","Activation functions are just used to squeeze (not numpy's) the output of a layer and cost functions are a way to measure the magnitude discrepancy between predicted output and the original output of net. Cost functions just need to be differentiable and continuous and don't really depend on the activation function.

But in keras, if your last activation function is sigmoid then you must be using their binary classification function and also account for regressions problems, else you are good to go.
",<deep-learning><classification><numpy><activation-function><cost-function>
"I am new to ML and as I take courses for the area DL, I am wondering, by our choice of activation function for the last layer, whether we take sigmoid, relu or softmax, would the formula for calculation of cost function change? 

I am grateful for every good reply I can get, have a nice day! :)
","You need to discriminate between two types of neural networks. If your output variable is continous you can use linear, ReLU, tanh, logistic-sigmoid,... as activation functions, because these functions map continous inputs to continous outputs. If your output is discrete / categorical you can use the signum (binary) or softmax activation (multiclass) function as activation function for the output layer.  

The cost function is often a function that is comparing the real outputs $y_n$ and the predicted outputs $\hat{y}(x_n)$ for the input $x_n$ for all $n=1,...,N$. Let us introduce the comparison function $D(y_n,\hat{y}(x_n))$. The comparison function has a low value if the predicted output is almost equal to the real output and high if the outputs are not similar. Assuming all the observations are equivalently important, we could sum the values of comparison function applied on all observations and obtain the integrated loss

$$J=\sum_{n=1}^ND(y_n,\hat{y}(x_n))$$

for the whole data set. 

In order to see the influence of the activation function $g$ in the last layer we summarize the transfer function from the input to the last layer as $f(x_n)$.  Then the predicted output $\hat{y}(x_n)$ can be written as 

$$\hat{y}(x_n)=g(f(x_n)).$$

Hence, the activation function at the output has an effect on the integrated loss $J$. For example if you choose the $\tanh$ as output activation you will bound your outputs in the intervall $(-1,1)$ which will be a bad choice if your outputs can be from $\mathbb{R}$ and your cost function will probably have a very high value while training. A better choice would be a linear activation function at the output layer. 
",<deep-learning><classification><numpy><activation-function><cost-function>
"I am trying to multiply a sparse matrix with itself using numpy and scipy.sparse.csr_matrix. The size of matrix is 128x256. Its 93% values are 0. Ironically the multiplication using numpy is faster than scipy.sparse. I do not know why? The code I am using is:

import numpy,time
W=numpy.random.choice([0, 1], size=(128,256), p=[0.93,0.07])
start=time.time()
W1=numpy.matmul(W,numpy.transpose(W))
end=time.time()
print(end-start)

from scipy.sparse import csr_matrix
start=time.time()
W1=csr_matrix(W).dot(csr_matrix(W).transpose())
end=time.time()
print(end-start)


Numpy gives time 0.0006 and scipy gives 0.004. Why. Comparing times for dense matrix, numpy gives smaller time on dense matrix as well and scipy takes more time. Why is the time for scipy.sparse not less than numpy for sparse matrix
","Here, you do not time only the time taken to make the matrix multiplication but also the time taken to convert your matrix from dense to sparse. If you convert your matrix before the timing starts, you will see that multiplication with scipy is indeed more than twice faster

import time, numpy, scipy
from scipy.sparse import csr_matrix
import numpy as np

W = np.random.binomial(n=1, p=0.01, size=(100, 100))
start=time.time()
numpy.matmul(W,numpy.transpose(W))
end=time.time()
dt_dense = end - start 
print ('time taken for the dense matrix {}'.format(end - start))

sparse_W = csr_matrix(W)
start=time.time()
sparse_W.dot(sparse_W.transpose())
end=time.time()
dt_sparse = end - start
print ('time taken for the sparse matrix {}'.format(end - start))
dt_dense/dt_sparse

",<python><numpy><scipy>
"This is my stock market csv data:

Date,Open,High,Low,Close,Adj Close,Volume
43283,511,514.950012,503.5,512.599976,512.599976,261839
43284,512.599976,520,509.700012,512,512,332619
43285,512,515.950012,507.950012,514.299988,514.299988,173621
43286,515.549988,517.5,509.399994,510.899994,510.899994,117474
43287,510.049988,516.5,510.049988,514.25,514.25,82106
43290,514.200012,528.5,514.200012,523.650024,523.650024,322861
43291,530,534.900024,522.099976,532.549988,532.549988,404132
43292,533.400024,541.75,531,536.599976,536.599976,267510
43293,539.450012,545,535.25,537.25,537.25,254942
43294,540,540.799988,520.5,523.900024,523.900024,240378
43297,524,529.75,518.549988,523.099976,523.099976,191192
43298,523,540,519.799988,538.049988,538.049988,213308
43299,542.349976,542.799988,515.849976,524.200012,524.200012,557333
43300,528,536.900024,518.849976,527.299988,527.299988,201716
43301,527.599976,536.450012,524.950012,534.450012,534.450012,156703
43304,534.5,544.950012,531.049988,540.799988,540.799988,209083
43305,542.950012,549,538.450012,546,546,216217
43306,547,547.5,529.450012,531.849976,531.849976,145508
43307,537,543.900024,527,541.650024,541.650024,547093
43308,545,555,538,553.650024,553.650024,540695
43311,555,570,551.099976,568.450012,568.450012,564010
43312,582,584.950012,548,550.099976,550.099976,942588
43313,552.450012,555.549988,538.650024,544.900024,544.900024,440881


I am trying to load stock market data csv file in a jupyter note book using 

import numpy as np

np.loadtxt(r""C:\Users\Souro\Downloads\Data.csv"",delimiter="","")


but it shows the following error after compiling:

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-54-6552d575b229&gt; in &lt;module&gt;
----&gt; 1 np.loadtxt(r""C:\Users\Souro\Downloads\Data.csv"",delimiter="","")

c:\python3.7.2\lib\site-packages\numpy\lib\npyio.py in loadtxt(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows)
   1139         # converting the data
   1140         X = None
-&gt; 1141         for x in read_data(_loadtxt_chunksize):
   1142             if X is None:
   1143                 X = np.array(x, dtype)

c:\python3.7.2\lib\site-packages\numpy\lib\npyio.py in read_data(chunk_size)
   1066 
   1067             # Convert each value according to its column and store
-&gt; 1068             items = [conv(val) for (conv, val) in zip(converters, vals)]
   1069 
   1070             # Then pack it according to the dtype's nesting

c:\python3.7.2\lib\site-packages\numpy\lib\npyio.py in &lt;listcomp&gt;(.0)
   1066 
   1067             # Convert each value according to its column and store
-&gt; 1068             items = [conv(val) for (conv, val) in zip(converters, vals)]
   1069 
   1070             # Then pack it according to the dtype's nesting

c:\python3.7.2\lib\site-packages\numpy\lib\npyio.py in floatconv(x)
    773         if '0x' in x:
    774             return float.fromhex(x)
--&gt; 775         return float(x)
    776 
    777     typ = dtype.type

ValueError: could not convert string to float: 'ï»¿""Date""'


How can I get rid of this error?
","The problem might arise because of the meta-text in the .csv or .txt file that is not really written there but is copied when its content is loaded somewhere. 

I think it is better to first import your text in an array or a string and then split it and save into the dataframe specifically when your data is not too large.

import csv 
arrays = []
path = ""C:\\Users\\Souro\\Downloads\\AXISBANK.csv""
with open(path, 'r') as f: 
   reader = csv.reader(f) 
   for row in reader: 
       row = str(row).replace('\\', '') #deleting backslash
       arrays.append(row)


Then take a look at arrays[:10] to find where the meta data ends and delete the unwanted data (meta data) 
and then converting the 'arrays' array into the dataframe.
for instance: 

arrays = arrays[9:]
df = pd.DataFrame(arrays[1:], columns=arrays[0]) #arrays[0] is the columns names


about your comments:

if you look at the text in each row (print each row), you would find out that a backslash is at the end of each row, so by replace('\',' ') we are substituting each backslash with nothing(''). why two \? It is the way that we declare backslash, otherwise, it won't be recognized.

row=str(row).replace('\\',' ') 


and about 

open('text.txt','r')


It opens the file 'text.txt' in reading mode (r). 
",<numpy><jupyter><csv><ipython>
"Let's say that I have image data with shape $(32, 32, 3)$ and $50000$


If I would like to reshape it to $(50000, 3, 32, 32)$ what should I do? 


I tried np.transpose(0, 3, 1, 2) but it failed.


If I would like to print the number $3$ from $(50000, 3, 32, 32)$ what should I do?

","If image is a numpy object:

image = image.reshape((50000,3,32,32))


and then print:

print(image[:,3,:,:])

",<numpy><image-preprocessing><reshape>
"I have data of a metric grouped date wise. I have plotted the data, now, how do I remove the values outside the range of the boxplot (outliers)?

All the ['AVG'] data is in a single column,
I need it for time series modelling.


","If you need to remove outliers and you need it to work with grouped data, without extra complications, just add showfliers argument as False in the function call. It's inherited from matplotlib.
df.boxplot(..., showfliers=False)

",<time-series><pandas><numpy><outlier><seaborn>
"I have data of a metric grouped date wise. I have plotted the data, now, how do I remove the values outside the range of the boxplot (outliers)?

All the ['AVG'] data is in a single column,
I need it for time series modelling.


","Seaborn uses inter-quartile range to detect the outliers. What you need to do is to reproduce the same function in the column you want to drop the outliers. It's quite easy to do in Pandas.

If we assume that your dataframe is called df and the column you want to filter based AVG, then

Q1 = df['AVG'].quantile(0.25)
Q3 = df['AVG'].quantile(0.75)
IQR = Q3 - Q1    #IQR is interquartile range. 

filter = (df['AVG'] &gt;= Q1 - 1.5 * IQR) &amp; (df['AVG'] &lt;= Q3 + 1.5 *IQR)
df.loc[filter]  

",<time-series><pandas><numpy><outlier><seaborn>
"I have data of a metric grouped date wise. I have plotted the data, now, how do I remove the values outside the range of the boxplot (outliers)?

All the ['AVG'] data is in a single column,
I need it for time series modelling.


","You can simply turn showfliers = False in seaborn.
",<time-series><pandas><numpy><outlier><seaborn>
"I have a column named BsmntQual that gives a ranking on the height of the basement per each house.  These are all of the unique values in this column:

print(train['BsmtQual'].unique().tolist())

&gt;&gt;&gt; ['Gd', 'TA', 'Ex', nan, 'Fa']


This is the legend of this particular column:

BsmtQual: Evaluates the height of the basement

       Ex   Excellent (100+ inches) 
       Gd   Good (90-99 inches)
       TA   Typical (80-89 inches)
       Fa   Fair (70-79 inches)
       Po   Poor (&lt;70 inches
       NA   No Basement &lt; Not to be confused with the nan value above


This is what I did for my other ranked columns but this one did not have NaN values:

train['ExterQual'] = train['ExterQual'].replace(['Ex', 'Gd', 'TA', 'Fa'], [4, 3, 2, 1])  # Exterior Quality


For numerical values, a common way is to fill all NaN values with the mean of the column.  But what is a good way of replacing the NaN values for columns such as these?

Here is the full dataset
","Your legend clearly states that missing values mean that there is no basement. You could fill the missing values with ’NoBase’ to make that point clearer (train[‘BsmtQual’].fillna(‘NoBase’, inplace=True)). 
When you rank them then, you just add another ranking for ‘NoBase’, maybe 0, based on your example rating that gives a higher value to more quality. 
",<machine-learning><python><scikit-learn><pandas><numpy>
"I have a column named BsmntQual that gives a ranking on the height of the basement per each house.  These are all of the unique values in this column:

print(train['BsmtQual'].unique().tolist())

&gt;&gt;&gt; ['Gd', 'TA', 'Ex', nan, 'Fa']


This is the legend of this particular column:

BsmtQual: Evaluates the height of the basement

       Ex   Excellent (100+ inches) 
       Gd   Good (90-99 inches)
       TA   Typical (80-89 inches)
       Fa   Fair (70-79 inches)
       Po   Poor (&lt;70 inches
       NA   No Basement &lt; Not to be confused with the nan value above


This is what I did for my other ranked columns but this one did not have NaN values:

train['ExterQual'] = train['ExterQual'].replace(['Ex', 'Gd', 'TA', 'Fa'], [4, 3, 2, 1])  # Exterior Quality


For numerical values, a common way is to fill all NaN values with the mean of the column.  But what is a good way of replacing the NaN values for columns such as these?

Here is the full dataset
","method : {‘backfill’, ‘bfill’, ‘pad’, ‘ffill’, None}, default None
Method to use for filling holes in reindexed Series pad / ffill: propagate last valid observation forward to next valid backfill / bfill: use NEXT valid observation to fill gap

from pandas fillna, these methods can be used or fill in with the highest frequency category.
",<machine-learning><python><scikit-learn><pandas><numpy>
"I am learning SVD by following this MIT course.  In this video, the lecturer is finding the SVD for 

$$
\begin{pmatrix} 5 &amp; 5 \\ -1 &amp; 7 \end{pmatrix},
$$

which involves finding the eigenvalues for

$$
C^T C = 
\begin{pmatrix}
26 &amp; 18 \\ 18 &amp; 74
\end{pmatrix}.
$$

In the example (at the time in the link above), the lecturer finds eigenvalues 

$$\begin{pmatrix}-3/\sqrt{10} \\ 1/\sqrt{10} \end{pmatrix}, 
\begin{pmatrix} 1/\sqrt{10} \\ 3/\sqrt{10} \end{pmatrix}.$$

But np.linalg.eig produces the opposite vector to the second one:

w, v = np.linalg.eig(C.T*C)
v
matrix([[-0.9486833 , -0.31622777],
        [ 0.31622777, -0.9486833 ]])


Why?
","Any scalar multiple of an eigenvector is also an eigenvector.  LAPACK (which np.linalg.eig uses under the hood) chooses to return unit-length eigenvectors (good for SVD!), but this still leaves two choices, and there doesn't seem to be a convention for which one to return; it's up to the underlying algorithm (which in turn may depend on the input data).

https://stackoverflow.com/questions/17998228/sign-of-eigenvectors-change-depending-on-specification-of-the-symmetric-argument
",<machine-learning><python><data-mining><numpy><linear-algebra>
"I am learning SVD by following this MIT course

The lecturer is trying to normalize a vector

$${\begin{bmatrix}
-3\\
1\\
\end{bmatrix}}$$

to

$${\begin{bmatrix}
\dfrac{-3}{\sqrt{10}}\\
\dfrac{1}{\sqrt{10}}\\
\end{bmatrix}}$$

I tried this with Python NumPy

np.linalg.norm(v1,ord=2,axis=1,keepdims=True)


and got

array([[3.],
       [1.]])


I would like to get something like this

[[-0.9486833 ],
[ 0.31622777]]


is there a way with Python (for instance, NumPy) to do the job? any other 3rd party library is also appreciated.
","You have already computed that, but you've not bound the output to a variable, also called name in python. Try the following snippet:

result = np.linalg.norm(v1,ord=2,axis=1,keepdims=True)
print(result)



Based on the edit, I update the answer. As you may find answers to your question, a typical way to find what you need is something like the following function:

def normalize(v):
    norm = np.linalg.norm(v)
    if norm == 0: 
       return v
    return v / norm


Equivalently, there is a function called normalize in sklearn.preprocessing which can be employed for your task. 
",<machine-learning><python><data-mining><numpy><normalization>
"import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import SelectFromModel
from scipy.io import arff

data = arff.loadarff(""C:\\Users\\manib\\Desktop\\Python Job\\Project Work\\Breast\\Breast.arff"")
df = pd.DataFrame(data[0])

df.head()

df[""Class""].value_counts()

X = df.iloc[:,:24481].values
y = df.iloc[:, -1].values

from sklearn import preprocessing
label_encoder = preprocessing.LabelEncoder()

y=y.astype('str')

y= label_encoder.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)

sel = SelectFromModel(RandomForestClassifier(n_estimators = 100))
sel.fit(X_train, y_train)

sel.get_support()

selected_feat= X_train.columns[(sel.get_support())]
len(selected_feat)

print(selected_feat)

","because this :

X = df.iloc[:,:24481].values
y = df.iloc[:, -1].values


you should remove .values or make extra X_col, y_col like that 

X_col = df.iloc[:,:24481]
y_col = df.iloc[:, -1]

",<scikit-learn><pandas><numpy>
"import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import SelectFromModel
from scipy.io import arff

data = arff.loadarff(""C:\\Users\\manib\\Desktop\\Python Job\\Project Work\\Breast\\Breast.arff"")
df = pd.DataFrame(data[0])

df.head()

df[""Class""].value_counts()

X = df.iloc[:,:24481].values
y = df.iloc[:, -1].values

from sklearn import preprocessing
label_encoder = preprocessing.LabelEncoder()

y=y.astype('str')

y= label_encoder.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)

sel = SelectFromModel(RandomForestClassifier(n_estimators = 100))
sel.fit(X_train, y_train)

sel.get_support()

selected_feat= X_train.columns[(sel.get_support())]
len(selected_feat)

print(selected_feat)

","The problem is that train_test_split(X, y, ...) returns numpy arrays and not pandas dataframes. Numpy arrays have no attribute named columns 

If you want to see what features SelectFromModel kept, you need to substitute X_train (which is a numpy.array) with X which is a pandas.DataFrame.

selected_feat= X.columns[(sel.get_support())]


This will return a list of the columns kept by the feature selector.

If you wanted to see how many features were kept you can just run this:

sel.get_support().sum()  # by default this will count 'True' as 1 and 'False' as 0

",<scikit-learn><pandas><numpy>
"I am having difficulty finding where my error is while building deep learning models, but I typically have issues when setting the input layer input shape. 

This is my model: 

model = Sequential([
Dense(32, activation='relu', input_shape=(1461, 75)),
Dense(32, activation='relu'),
Dense(ytrain.size),])


It is returning the following error:

 ValueError: Error when checking input: expected dense_1_input to have 3

 dimensions, but got array with shape (1461, 75)


The array is the training set from the kaggle housing price competition and my dataset has 75 columns and 1461 rows. My array is 2 dimensional, so why are 3 dimensions expected? I have tried adding a redundant 3rd dimension of 1 or flattening the array before the first dense layer but the error simply becomes:

ValueError: Input 0 is incompatible with layer flatten_1: expected 

min_ndim=3, found ndim=2


How do you determine what the input size should be and why do the dimensions it expects seem so arbitrary?

For reference, I attached the rest of my code:

xtrain = pd.read_csv(""pricetrain.csv"")
test = pd.read_csv(""pricetest.csv"")
xtrain.fillna(xtrain.mean(), inplace=True)
xtrain.drop([""Alley""], axis=1, inplace=True)
xtrain.drop([""PoolQC""], axis=1, inplace=True)
xtrain.drop([""Fence""], axis=1, inplace=True)
xtrain.drop([""MiscFeature""], axis=1, inplace=True)
xtrain.drop([""PoolArea""], axis=1, inplace=True)
columns = list(xtrain)
for i in columns:
    if xtrain[i].dtypes == 'object':
        xtrain[i] = pd.Categorical(pd.factorize(xtrain[i])[0])
from sklearn import preprocessing

le = preprocessing.LabelEncoder()
for i in columns:
    if xtrain[i].dtypes == 'object':
        xtrain[i] = le.fit_transform(xtrain[i])
ytrain = xtrain[""SalePrice""]
xtrain.drop([""SalePrice""], axis=1, inplace=True)
ytrain = ytrain.values
xtrain = xtrain.values
ytrain.astype(""float32"")

size = xtrain.size
print(ytrain)
model = Sequential(
    [Flatten(),
     Dense(32, activation='relu', input_shape=(109575,)),
     Dense(32, activation='relu'),
     Dense(ytrain.size),
     ])
model.compile(loss='mse', optimizer='adam')
model.fit(xtrain, ytrain, epochs=10, verbose=1)


Any advice would be incredibly helpful!

Thank you.
","The number of rows in your training data is not part of the input shape of the network because the training process feeds the network one sample per batch (or, more precisely, batch_size samples per batch). And in input_shape, the batch dimension is not included for the first layer. You can read more on this here.

So, the input shape for your problem will be:

input_shape=(75, )

",<python><deep-learning><keras><numpy>
"I am having difficulty finding where my error is while building deep learning models, but I typically have issues when setting the input layer input shape. 

This is my model: 

model = Sequential([
Dense(32, activation='relu', input_shape=(1461, 75)),
Dense(32, activation='relu'),
Dense(ytrain.size),])


It is returning the following error:

 ValueError: Error when checking input: expected dense_1_input to have 3

 dimensions, but got array with shape (1461, 75)


The array is the training set from the kaggle housing price competition and my dataset has 75 columns and 1461 rows. My array is 2 dimensional, so why are 3 dimensions expected? I have tried adding a redundant 3rd dimension of 1 or flattening the array before the first dense layer but the error simply becomes:

ValueError: Input 0 is incompatible with layer flatten_1: expected 

min_ndim=3, found ndim=2


How do you determine what the input size should be and why do the dimensions it expects seem so arbitrary?

For reference, I attached the rest of my code:

xtrain = pd.read_csv(""pricetrain.csv"")
test = pd.read_csv(""pricetest.csv"")
xtrain.fillna(xtrain.mean(), inplace=True)
xtrain.drop([""Alley""], axis=1, inplace=True)
xtrain.drop([""PoolQC""], axis=1, inplace=True)
xtrain.drop([""Fence""], axis=1, inplace=True)
xtrain.drop([""MiscFeature""], axis=1, inplace=True)
xtrain.drop([""PoolArea""], axis=1, inplace=True)
columns = list(xtrain)
for i in columns:
    if xtrain[i].dtypes == 'object':
        xtrain[i] = pd.Categorical(pd.factorize(xtrain[i])[0])
from sklearn import preprocessing

le = preprocessing.LabelEncoder()
for i in columns:
    if xtrain[i].dtypes == 'object':
        xtrain[i] = le.fit_transform(xtrain[i])
ytrain = xtrain[""SalePrice""]
xtrain.drop([""SalePrice""], axis=1, inplace=True)
ytrain = ytrain.values
xtrain = xtrain.values
ytrain.astype(""float32"")

size = xtrain.size
print(ytrain)
model = Sequential(
    [Flatten(),
     Dense(32, activation='relu', input_shape=(109575,)),
     Dense(32, activation='relu'),
     Dense(ytrain.size),
     ])
model.compile(loss='mse', optimizer='adam')
model.fit(xtrain, ytrain, epochs=10, verbose=1)


Any advice would be incredibly helpful!

Thank you.
","Try this bunch of code:

x_train=x_train.reshape(-1,75,1)


but before you train(fit) model 

Negative one (-1)  in reshape(-1,75,1) simply mean, that you don't know how much should be in first dimension, but you know that second one should be equals 75 and last one 1.
",<python><deep-learning><keras><numpy>
"I was reading this code, for implemnting linear regression from scratch:

# convert from data frames to numpy matrices
X = np.matrix(X.values)
y = np.matrix(y.values)
theta = np.matrix(np.array([0,0]))


When I came accross this line :  np.matrix(np.array([0,0]))

I was wondering why didn't the person just write np.matrix([0,0]).

I ran both in jupyter notebook and got the same output:

theta = np.matrix([0,0])
theta2 = np.matrix(np.array([0,0]))
print(theta,theta2,type(theta),type(theta2))


Output:[[0 0]] [[0 0]] &lt;class 'numpy.matrix'&gt; &lt;class 'numpy.matrix'&gt;

Is there a difference between the two? Does the extra np.array somehow part add to the functionality of theta? Will the final code function properly if I replace the former with the latter?

Thanks.

Edit: Is this the right place to ask this question? I am new here...
","As you definitely know the type of $np.array([0,0])$ is numpy.ndarray and the type of $[0,0]$ is list. 

And using numpy ndarrays instead of lists is way faster. Further, numpy arrays consume smaller memory. Hence, due to a higher speed and memory usage, etc. (functionality), it is better to use numpy arrays instead of lists.
",<linear-regression><numpy><matrix>
"I was reading this code, for implemnting linear regression from scratch:

# convert from data frames to numpy matrices
X = np.matrix(X.values)
y = np.matrix(y.values)
theta = np.matrix(np.array([0,0]))


When I came accross this line :  np.matrix(np.array([0,0]))

I was wondering why didn't the person just write np.matrix([0,0]).

I ran both in jupyter notebook and got the same output:

theta = np.matrix([0,0])
theta2 = np.matrix(np.array([0,0]))
print(theta,theta2,type(theta),type(theta2))


Output:[[0 0]] [[0 0]] &lt;class 'numpy.matrix'&gt; &lt;class 'numpy.matrix'&gt;

Is there a difference between the two? Does the extra np.array somehow part add to the functionality of theta? Will the final code function properly if I replace the former with the latter?

Thanks.

Edit: Is this the right place to ask this question? I am new here...
","No, they're absolutely the same.

In this case there is absolutely no difference apart from perhaps a trivial amount of processing time.  This is all open source code so we can just read it:

The relevant part of numpy for us here is the matrix constructor (yes, np.matrix is a python class below the hood).  In a summary from the NumPy code we see:

class matrix(N.ndarray):
# ...
    def __new__(subtype, data, dtype=None, copy=True):
    # ...
        if isinstance(data, N.ndarray):
            if dtype is None:
                intype = data.dtype
            else:
                intype = N.dtype(dtype)
            new = data.view(subtype)
            if intype != data.dtype:
                return new.astype(intype)
            if copy: return new.copy()
            else: return new
        # ...
        arr = N.array(data, dtype=dtype, copy=copy)
        ndim = arr.ndim
        shape = arr.shape
        # some extra checks
        ret = N.ndarray.__new__(subtype, shape, arr.dtype,
                                buffer=arr,
                                order=order)
        return ret


What we give as the data argument is literally what we give to to np.matrix().  Therefore we can draw for the two cases:

np.matrix([0, 0])


The python interpreter builds two integers: 0 and 0.
The python interpreter builds a list from the pointers to the two integers.
The python interpreter evaluates the matrix constructor with the list as data.
The if in the constructor is not executed, instead an np.array is build from the list.
Inside the array constructor data types are checked.
The final array is returned (the second array constructor perform much less work because it is passed buffer=)


np.matrix(np.array([0, 0]))


The python interpreter builds two integers: 0 and 0.
The python interpreter builds a list from the pointers to the two integers.
The python interpreter evaluates the array constructor.
The resulting array is passed as data to the matrix constructor, and the if is executed.
Within the if the data type is taken from the existing array.
The array is copied an returned.




Both ways execute pretty much the same number of constructors and lines of code.  One could argue that copying the array (the copy= argument) may be a slow operation.  Yet, given the fact that to have enough data for array.copy() to be slow one would first need to construct a full python list of that size, the copy() time is negligible compared to the list construction.  In other words, both methods need to construct the list - because python will always evaluate arguments before passing them - which is the slowest part of the execution of this code.

As for the return value, they're absolutely and completely the same.  Most of the code within the constructor summarized (and linked) above is to make sure that you get the same return if you give equivalent input.



P.S. (Unrelated Note) If one starts reading data from a file (or any other external source) the picture changes.  If one reads directly into an array without going through the python list phase, that method is bound to be much faster.  The processing bottleneck is the python list, if one can avoid that things will go faster.
",<linear-regression><numpy><matrix>
"I'm doing a small POC in which I've trained my Machine Learning model (Naive Bayes) and saved in &quot;.pkl&quot; (pickle) format. Now my next task is to develop a web application which asks the user to enter the Text for the Text classification analysis. This newly taken (from the user) &quot;TEXT&quot; will be the testing dataset which can be fed to the Naive Bayes model that I built in the earlier stage and make prediction.
Is there any way to convert the text (taken from the user) into numpy array and then transform this numpy array using CountVectorizer() (which was used for training dataset) and then feed to the saved model for predictions?
I want to make an Angular application like this where the input is an image and this image can be converted into pixels.
In my case, it is plain text which needs to be transformed into NumPy array in an Angular application before I feed it to the Trained Naive Bayes Model.
","When deploying a model into production, similar data transformation steps have to be taken during training and prediction.
Scikit-learn has a Pipeline class that makes it more straightforward to do that.
Something like:
import pickle

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes             import GaussianNB
from sklearn.pipeline                import Pipeline

pipeline = Pipeline(
    [
        (&quot;vect&quot;, CountVectorizer()),
        (&quot;clf&quot;, GaussianNB()),
    ]
)

# Training
pipe.fit(X_train, y_train)
saved_model = pickle.dumps(clf)

# Predicion in app
pipe = pickle.loads(saved_model)
pipe.predict(X)

",<machine-learning><python><classification><numpy><nlp>
"I intend on perfroming some numerical approximations for a problem in physics. 

The main gist of the program will be to perform a svd on large sparse matrices and also calculating the trace of a large matrix. 

I used numpy/scipy and multiprocessing modules in python to do this, but it is not fast enough.

I also implemented the same code using tensorflow on some gpus. Tensorflow does the calulations much faster, but it takes a long time to send the data into the gpus and bring it out. Each instance of data is sent individually as float values.  Is there any way to send all the data at once into the gpu?

Is it a good or bad idea to use tensorflow for numerical calculations? 

Below is a snippet of my code....

def Expectation_Value(density,N):
   expec=0
   for i in np.arange(1,N+1):
       a=tf.linalg.trace(tf.matmul(Sigma(i,N,Sz),density))
       expec+=a
return expec/N

def main():
   df=pd.read_csv('.....')
   sess=tf.Session()

   for l in range(len(df)):
   Delta=df['Delta'].iloc[l]
   Omega=df['Omega'].iloc[l]
   Gamma=df['Gamma'].iloc[l]
   J=df['J'].iloc[l]


    Sz_tens=tf.zeros(len(df))
    Expec=Expectation_Value(DMT,N)
    Sz_tens[l]=Expec
Sz_arr=Sz_tens.eval(session=sess)
df['Sz']=Sz_arr
sess.close()
df.head()

","TensorFlow is a library that supports dataflow paradigm which makes it a good candidate for numerical calculations.

In general, TensorFlow data loading performance can be optimized using tf.data module. In particular, there is a concept called GPUs starvation, where GPUs are waiting for data to be loaded. GPU starvation can be minimized in TensorFlow with Pipelining, overlapping the preprocessing and model execution of a training step.

In particular, your code can be optimized by refactoring to Expectation_Value operate on vectors instead of scalars. Right now N and i are scalars and there is a for-loop that processes elements one at a time. tf.linalg.trace can take a tensor as an input so that N and i could be vectors. The function will be much faster once it operates on vectors and no longer uses a for-loop.
",<python><tensorflow><numpy>
"Can anyone please explain to me the difference between them in terms of any operation or computation and values stored in them?
","From the documentation:


  empty, unlike zeros, does not set the array values to zero, and may
  therefore be marginally faster. On the other hand, it requires the
  user to manually set all the values in the array, and should be used
  with caution.


np.zeros

Return a new array setting values to zero.

&gt;&gt;&gt; np.zeros((2, 2))
array([[0., 0.],
       [0., 0.]])


np.empty

Return a new uninitialized array.

&gt;&gt;&gt; np.empty((2, 2))
array([[1.35807735e-312, 1.35807731e-312],
       [1.99637364e-310, 8.69169476e-311]])

",<machine-learning><python><pandas><data-science-model><numpy>
"I am currently working through Kaggle's titanic competition and I'm trying to figure out the correlation between the Survived column and other columns.  I am using numpy.corrcoef() to matrix the correlation between the columns and here is what I have:

The correlation between pClass &amp; Survived is: [[ 1.         -0.33848104]
 [-0.33848104  1.        ]]

The correlation between Sex &amp; Survived is: [[ 1.         -0.54335138]
 [-0.54335138  1.        ]]

The correlation between Age &amp; Survived is:[[ 1.         -0.07065723]
 [-0.07065723  1.        ]]

The correlation between Fare &amp; Survived is: [[1.         0.25730652]
 [0.25730652 1.        ]]

The correlation between Parent-Children &amp; Survived is: [[1.         0.08162941]
 [0.08162941 1.        ]]

The correlation between Sibling-Spouse &amp; Survived is: [[ 1.        -0.0353225]
 [-0.0353225  1.       ]]

The correlation between Embarked &amp; Survived is: [[ 1.         -0.16767531]
 [-0.16767531  1.        ]]


There should be higher correlation between Survived and [pClass, sex, Sibling-Spouse] and yet the values are really low.  I'm new to this so I understand that a simple method is not the best way to find correlations but at the moment, this doesn't add up.

This is my full code (without the printf() calls):

import pandas as pd
import numpy as np

train = pd.read_csv(""https://raw.githubusercontent.com/oo92/Titanic-Kaggle/master/train.csv"")
test = pd.read_csv(""https://raw.githubusercontent.com/oo92/Titanic-Kaggle/master/test.csv"")

survived = train['Survived']
pClass = train['Pclass']
sex = train['Sex'].replace(['female', 'male'], [0, 1])
age = train['Age'].fillna(round(float(np.mean(train['Age'].dropna()))))
fare = train['Fare']
parch = train['Parch']
sibSp = train['SibSp']
embarked = train['Embarked'].replace(['C', 'Q', 'S'], [1, 2, 3])

","You probably encoded Women as 0 and men as 1 that's why you get a negative correlation of -0.54, because Survived is 0 for No and 1 for Yes. Your calculation actually show what you've expected. The negative correlation is only about the direction depending on your encoding, the relationship between Women and Survived is 0.54.

Similarly pClass is correlated negatively with -0.33 because the highest class (1st class) is encoded as 1 and the lowest as 3, thus the direction is negative.

You could make the relations more intuitive if you make new columns for men and women where you put 0 and 1 depending on the sex, then the correlations will have the intuitive direction (sign). The same holds for pClass.
",<machine-learning><python><feature-selection><numpy><kaggle>
"I am currently working through Kaggle's titanic competition and I'm trying to figure out the correlation between the Survived column and other columns.  I am using numpy.corrcoef() to matrix the correlation between the columns and here is what I have:

The correlation between pClass &amp; Survived is: [[ 1.         -0.33848104]
 [-0.33848104  1.        ]]

The correlation between Sex &amp; Survived is: [[ 1.         -0.54335138]
 [-0.54335138  1.        ]]

The correlation between Age &amp; Survived is:[[ 1.         -0.07065723]
 [-0.07065723  1.        ]]

The correlation between Fare &amp; Survived is: [[1.         0.25730652]
 [0.25730652 1.        ]]

The correlation between Parent-Children &amp; Survived is: [[1.         0.08162941]
 [0.08162941 1.        ]]

The correlation between Sibling-Spouse &amp; Survived is: [[ 1.        -0.0353225]
 [-0.0353225  1.       ]]

The correlation between Embarked &amp; Survived is: [[ 1.         -0.16767531]
 [-0.16767531  1.        ]]


There should be higher correlation between Survived and [pClass, sex, Sibling-Spouse] and yet the values are really low.  I'm new to this so I understand that a simple method is not the best way to find correlations but at the moment, this doesn't add up.

This is my full code (without the printf() calls):

import pandas as pd
import numpy as np

train = pd.read_csv(""https://raw.githubusercontent.com/oo92/Titanic-Kaggle/master/train.csv"")
test = pd.read_csv(""https://raw.githubusercontent.com/oo92/Titanic-Kaggle/master/test.csv"")

survived = train['Survived']
pClass = train['Pclass']
sex = train['Sex'].replace(['female', 'male'], [0, 1])
age = train['Age'].fillna(round(float(np.mean(train['Age'].dropna()))))
fare = train['Fare']
parch = train['Parch']
sibSp = train['SibSp']
embarked = train['Embarked'].replace(['C', 'Q', 'S'], [1, 2, 3])

","On a side note, I don't think correlation is the correct measure of relation for you to be using, since Survived is technically a binary categorical variable.

""Correlation"" measures used should depend on the type of variables being investigated:


continuous variable v continuous variable: use ""traditional"" correlation - e.g. Spearman's rank correlation or Pearson's linear correlation.
continuous variable v categorical variable: use an ANOVA F-test / difference of means
categorical variable v categorical variable: use Chi-square / Cramer's V

",<machine-learning><python><feature-selection><numpy><kaggle>
"I just want to know if my ipython is properly installed. I have both Python and Anaconda installed on my Windows. I entered the ""pip3 install ipython"" command on the Command Prompt and it downloaded and installed the ipython package. However, whenever I try to access it, I see this:



Notice the icon on the top left hand corner:



And whenever I execute a code and will like to continue working in that file, it will take forever for iPython to take me to the next line of code (or in some cases, may not even do so). For example, in the case below, I have not been able to enter any code after the ""In [18]: plt.show()"" since ""In [19]:"" would not just come up. Please see this:



I believe this is because the package is not properly installed.

I will be grateful if you guys can just show me what I can do about this?
Thanks.
","It looks fine to me :)  the only problem is that your plot (resulting from In [18]) is being displayed on your computer in a separate window somewhere - maybe you have to find it. Once you close that window, your iPython prompt woill return to In [19]. You could alternatively press Ctrl-C in the iPython session, but this will end the session.

If the problem persists (and you cannot even find/close the plot from plt.show()) - maybe have a look at this post, which discusses the various backends used for matplotlib.

I assume you do this to begin with:

import matplotlib.pyplot as plt


plt is a module (as the error message says), so you cannot use it directly like that, but rather the functions that are contained in that module, like scatter() (which worked for you) and then others like plot(), hist() and so on.

Have a look at the relevant documentation with lots of exmaples.

Here is a full example:

import matplotlib.pyplot as plt
import numpy as np

# evenly sampled time at 200ms intervals
t = np.arange(0., 5., 0.2)

# red dashes, blue squares and green triangles
plt.plot(t, t, 'r--', t, t**2, 'bs', t, t**3, 'g^')
plt.show()



",<data-science-model><numpy><plotting><matplotlib><ipython>
"I am new to data science. I use Anaconda on windows 7.
I plotted a sine curve by doing the following on iPython:

import numpy as np
import matplotlib.pyplot as plt
x = np.linspace(0, 10, 1000)
y = np.sin(x)
plt.plot(x, y)


And I got this:

But when I got ready to name the axes, the curve disappeared. I wrote this code:

plt.xlabel(""Time"")


And I got this:

I also wrote this:

plt.ylabel(""Speed"")


And got this:


So my question is, how can I plot a curve with labelled axes? (In other words, I will like the x-axis to be Time, the y-axis to be Speed and the curve intact)
","When you use matplotlib's plot function, it holds an object behind the scenes for you. You can change this object with more calls to plt and then only once everything has been done should you plt.show() the graph.

Here is a simply example that does what you want:

In [1]: import matplotlib.pyplot as plt
In [2]: import numpy as np                                                           

In [3]: x = np.linspace(0, 10, 100)                                                  
In [4]: y = x ** 2                                   


The following lines change the plot object, adding the axes labels - but we don't show it until all are complete...

In [5]: plt.plot(x, y)                                                               
Out[5]: [&lt;matplotlib.lines.Line2D at 0x7f3c896bb9b0&gt;]

In [6]: plt.xlabel(""Time"")                                                           
Out[6]: Text(0.5, 0, 'Time')

In [7]: plt.ylabel(""Speed"")  
Out[7]: Text(0, 0.5, 'Speed')


Now we are done, so show it:

In [8]: plt.show()                                                                  




Have a look here for a more thorough demo, which also shows the object explicitly and lets you better understand what is going on.
",<numpy><plotting><matplotlib>
"I searched for numpy.numpy() and tried replacing .numpy() with .np() because numpy is already imported as np here:  Tensorflow tutorial 

But using `.np() returns an error.  

In the section, ""Creating training examples and targets"" there is:

# Create training examples / targets
char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int) #slices text_as_int into elements for dataset

print(type(char_dataset))

for i in char_dataset.take(5): #from 0 to 4
  print(i, i.numpy())
  print(idx2char[i.numpy()])


That outputs:

&lt;class 'tensorflow.python.data.ops.dataset_ops.TensorSliceDataset'&gt;
tf.Tensor(18, shape=(), dtype=int64) 18
F
tf.Tensor(47, shape=(), dtype=int64) 47
i
tf.Tensor(56, shape=(), dtype=int64) 56
r
tf.Tensor(57, shape=(), dtype=int64) 57
s
tf.Tensor(58, shape=(), dtype=int64) 58
t


So i is a tensor and .numpy() seems to convert that into the int representing the character in the text.  However, I was looking for a more formal explanation.  
","That is part of TensorFlow's eager execution:


  Tensors can be explicitly converted to NumPy ndarrays by invoking the
  .numpy() method on them.


There is no such thing as numpy.numpy(). There is no numpy function inside of the NumPy package. The numpy function is only in the TensorFlow package.

The NumPy package is frequently imported with alias:

import numpy as np

After importing the NumPy package, you have access to NumPy's modules and functions:

np.random.random_sample()
",<numpy>
"My data set has a total of 200 columns, where each column corresponds to the same pixel in all of my images. In total, I have 48,500 rows. The labels for the data range from 0-9.

The data looks something like this:

raw_0   raw_1   raw_2   raw_3   raw_4
0   120.0   133.0   96.0    155.0   66.0
1   159.0   167.0   163.0   185.0   160.0
2   45.0    239.0   66.0    252.0   NaN
3   126.0   239.0   137.0   NaN 120.0
4   226.0   222.0   153.0   235.0   171.0
5   169.0   81.0    100.0   44.0    104.0
6   154.0   145.0   76.0    134.0   175.0
7   77.0    35.0    105.0   108.0   112.0
8   104.0   55.0    113.0   90.0    107.0
9   97.0    253.0   255.0   251.0   141.0
10  224.0   227.0   84.0    214.0   57.0
11  NaN 13.0    51.0    50.0    NaN
12  82.0    213.0   61.0    98.0    59.0
13  NaN 40.0    84.0    7.0 39.0
14  129.0   103.0   65.0    159.0   NaN
15  123.0   128.0   116.0   198.0   111.0


Each column has around 5% missing values and I want to fill in these NaN
values with something meaningful. However, I'm not sure how to go about this. Any suggestions would be welcome.

Thank you!
","If adjacent rows are adjacent pixels that I'd just use the average value of the adjacent pixels. That seems like it would make sense for an image, and would certainly be hard for the human eye to see.
",<machine-learning><python><pandas><numpy><image-preprocessing>
"My data set has a total of 200 columns, where each column corresponds to the same pixel in all of my images. In total, I have 48,500 rows. The labels for the data range from 0-9.

The data looks something like this:

raw_0   raw_1   raw_2   raw_3   raw_4
0   120.0   133.0   96.0    155.0   66.0
1   159.0   167.0   163.0   185.0   160.0
2   45.0    239.0   66.0    252.0   NaN
3   126.0   239.0   137.0   NaN 120.0
4   226.0   222.0   153.0   235.0   171.0
5   169.0   81.0    100.0   44.0    104.0
6   154.0   145.0   76.0    134.0   175.0
7   77.0    35.0    105.0   108.0   112.0
8   104.0   55.0    113.0   90.0    107.0
9   97.0    253.0   255.0   251.0   141.0
10  224.0   227.0   84.0    214.0   57.0
11  NaN 13.0    51.0    50.0    NaN
12  82.0    213.0   61.0    98.0    59.0
13  NaN 40.0    84.0    7.0 39.0
14  129.0   103.0   65.0    159.0   NaN
15  123.0   128.0   116.0   198.0   111.0


Each column has around 5% missing values and I want to fill in these NaN
values with something meaningful. However, I'm not sure how to go about this. Any suggestions would be welcome.

Thank you!
","There are multiple ways to go after this. You can do mean imputation, median imputation, mode imputation or most common value imputation. Calculate one of the above value for either rows or columns depending on how your data is structured. One of the simplest ways to fill Nan's are df.fillna in pandas
",<machine-learning><python><pandas><numpy><image-preprocessing>
"My data set has a total of 200 columns, where each column corresponds to the same pixel in all of my images. In total, I have 48,500 rows. The labels for the data range from 0-9.

The data looks something like this:

raw_0   raw_1   raw_2   raw_3   raw_4
0   120.0   133.0   96.0    155.0   66.0
1   159.0   167.0   163.0   185.0   160.0
2   45.0    239.0   66.0    252.0   NaN
3   126.0   239.0   137.0   NaN 120.0
4   226.0   222.0   153.0   235.0   171.0
5   169.0   81.0    100.0   44.0    104.0
6   154.0   145.0   76.0    134.0   175.0
7   77.0    35.0    105.0   108.0   112.0
8   104.0   55.0    113.0   90.0    107.0
9   97.0    253.0   255.0   251.0   141.0
10  224.0   227.0   84.0    214.0   57.0
11  NaN 13.0    51.0    50.0    NaN
12  82.0    213.0   61.0    98.0    59.0
13  NaN 40.0    84.0    7.0 39.0
14  129.0   103.0   65.0    159.0   NaN
15  123.0   128.0   116.0   198.0   111.0


Each column has around 5% missing values and I want to fill in these NaN
values with something meaningful. However, I'm not sure how to go about this. Any suggestions would be welcome.

Thank you!
","Given you have images stretched out as columns in a table with ~48,500 rows, I am assuming you have the raw images that are 220x220 in dimension.
You can use a function available via OpenCV called inpaint, which will restore missing pixel values (for example black pixels of degraded photos).
Here is an image example. Top-left shows the image with missing values (in black). Top-right shows just the missing values (the mask). Bottom-left and bottom-right are the final output, comparing two different algorithms for filling the images.

I would suggest trying both methods on your images to see what looks best.
Have a look at the Documentation for more details on the algorithms themselves. Here is the documentation of the actual function.
As for code, it will look something like this:
import opencv as cv    # you will need to install OpenCV

dst = cv.inpaint(img, mask, 3, cv.INPAINT_TELEA)


the first argument is your image with missing values
the second is the mask, with locations of where missing pixels are, i.e. which pixels should be filled/interpolated.
third is the radius around missing pixels to fill
fourth is the flag for the algorithm to use (see link above for two alternatives)

For each image, you can generate the mask with something like this:
mask = image[np.isnan(image)]

Note: '==' doesn't work with np.nan
",<machine-learning><python><pandas><numpy><image-preprocessing>
"My data set has a total of 200 columns, where each column corresponds to the same pixel in all of my images. In total, I have 48,500 rows. The labels for the data range from 0-9.

The data looks something like this:

raw_0   raw_1   raw_2   raw_3   raw_4
0   120.0   133.0   96.0    155.0   66.0
1   159.0   167.0   163.0   185.0   160.0
2   45.0    239.0   66.0    252.0   NaN
3   126.0   239.0   137.0   NaN 120.0
4   226.0   222.0   153.0   235.0   171.0
5   169.0   81.0    100.0   44.0    104.0
6   154.0   145.0   76.0    134.0   175.0
7   77.0    35.0    105.0   108.0   112.0
8   104.0   55.0    113.0   90.0    107.0
9   97.0    253.0   255.0   251.0   141.0
10  224.0   227.0   84.0    214.0   57.0
11  NaN 13.0    51.0    50.0    NaN
12  82.0    213.0   61.0    98.0    59.0
13  NaN 40.0    84.0    7.0 39.0
14  129.0   103.0   65.0    159.0   NaN
15  123.0   128.0   116.0   198.0   111.0


Each column has around 5% missing values and I want to fill in these NaN
values with something meaningful. However, I'm not sure how to go about this. Any suggestions would be welcome.

Thank you!
","for any (x,y) if NAN you can impute to average of surrounding pixels as:

if((x==0  &amp; y==0):
 return (x+1)+(y+1))/2 

else if(x==x_max &amp; y==y_max):
 return (x-1)+(y-1))/2

else if(x==0 &amp; y==y_max):
 return (x+1)+(y-1))/2

else if(x==x_max &amp; y==0):
 return (x-1)+(y+1))/2

else if(x==0):
 return ((x+1)+(y-1)+(y+1))/3

else if(x==x_max):
 return ((x-1)+(y-1)+(y+1))/3

else if(y==0):
 return ((x+1)+(x-1)+(y+1))/3

else if(y==y_max):
 return ((x-1)+(x+1)+(y-1))/3

else :
  return  ((x-1)+(x+1)+(y-1)+(y+1))/4 

",<machine-learning><python><pandas><numpy><image-preprocessing>
"Suppose I have data with two independent variable $X_1$, $X_2$ and one dependent variable say $y$, as follows:

$X_1$: $x_{1,1}$, $x_{1,2}$ , $x_{1,3}$

$X_2$: $x_{2,1}$, $x_{2,2}$, $x_{2,3}$

$y$: $y_1$, $y_2$, $y_3$

I built some Machine learning model which is good .

Now I want to generate predictions not just for test data but for all possible combinations of test data for example, if our test data looks like

$X_1$: $a$, $b$, $c$

$X_2$: $p$, $q$, $r$

then I want predictions for pairs $(a,p)$$(a,q)$,$(a,r)$,$(b,p)$,$(b,q)$....etc

I have tried np.ravel, Meshgrid kind of commands but find it difficult.
","It is possible.

It is called Multi-output regression. Allows you to predict more that one variable:

Multioutput regression
",<python><predictive-modeling><numpy>
"I am trying to calculate a cosine similarity using Python in order to find similar users basing on ratings they have given to movies. As it can be expected there are a lot of NaN values.
I am using movie dataset from Kaggle.

When I use np.dot() on two nd.arrays the outcome is:'nan'.
I have checked with np.nansum() that there are some other than 'nan' values.

I do not want to change all 'nan' values to '0' as it would mean that users have given 0 rating to the movies which lead to 'false' similarity between users.

Please, give me some advice regarding how to proceed with this problem.

Thanks in advance.
","def cosine_sim(df1, df2):

    df1na = df1.isna()
    df1clean = df1[~df1na]
    df2clean = df2[~df1na]

    df2na = df2clean.isna()
    df1clean = df1clean[~df2na]
    df2clean = df2clean[~df2na]


    # Compute cosine similarity
    distance = cosine(df1clean, df2clean)
    sim = 1 - distance

    return sim

",<python><recommender-system><numpy><cosine-distance>
"I am trying to calculate a cosine similarity using Python in order to find similar users basing on ratings they have given to movies. As it can be expected there are a lot of NaN values.
I am using movie dataset from Kaggle.

When I use np.dot() on two nd.arrays the outcome is:'nan'.
I have checked with np.nansum() that there are some other than 'nan' values.

I do not want to change all 'nan' values to '0' as it would mean that users have given 0 rating to the movies which lead to 'false' similarity between users.

Please, give me some advice regarding how to proceed with this problem.

Thanks in advance.
","I think it's rarely meaningful to consider cosine similarity on sparse data like this, not just because of sparsity (because it's only defined for dense data), but because it's not obvious the cosine similarity is meaningful. For example a user that rates 10 movies all 5s has perfect similarity with a user that rates those 10 all as 1. Magnitude doesn't matter in cosine similarity, but it matters in your domain.

It's much more likely that it's meaningful on some dense embedding of users and items, such as what you get from ALS. 

To answer the question, you either need to impute the missing ratings (don't assume 0, but a mean value or similar), or ignore dimensions that aren't defined in both.
",<python><recommender-system><numpy><cosine-distance>
"I am trying to calculate a cosine similarity using Python in order to find similar users basing on ratings they have given to movies. As it can be expected there are a lot of NaN values.
I am using movie dataset from Kaggle.

When I use np.dot() on two nd.arrays the outcome is:'nan'.
I have checked with np.nansum() that there are some other than 'nan' values.

I do not want to change all 'nan' values to '0' as it would mean that users have given 0 rating to the movies which lead to 'false' similarity between users.

Please, give me some advice regarding how to proceed with this problem.

Thanks in advance.
","Have you couple of types na's handling. Its sample of code:

def na_handling(df, name_of_strategy):

        #list of stategies -&gt; mean, mode, 0, spefic_value, next_row, previous_row

        if name_of_strategy==""previous_row"":
            df.fillna(method=""backfill"", inplace=True)
            return df
        elif name_of_strategy==""next_row"":
            df.fillna(method=""ffill"", inplace=True)
            return df
        elif name_of_strategy==""0"":
            df.fillna(0, inplace=True)
            return df

        elif name_of_strategy==""mean"":
            df.fillna(df.mean(), inplace=True)
            return df
        elif name_of_strategy==""mode"":
            df.fillna(df.mode(), inplace=True)
            return df
        else:
            print(""Wrong specified strategy"")


vec1 = na_handling(old_vec, ""next_row"")

",<python><recommender-system><numpy><cosine-distance>
"I have an image (for example (7x7x3) and a filter (3x3x3)). I convolved the image with the filter and it became a (3x3) output. If I want to do the inverse operation and want it to become the image from the output and the filter. How can I implement this operation in Python with Numpy?

I don't know which operation I should use with the filter (inverse or transpose)?

Here is my code for the Deconvolution:

import numpy as np

def deConv(Z, cashe):

'''

deConv calculate the transpoe Convoultion between the output of the ConvNet and the filter

Arguments:
    Z-- Output of the ConvNet Layer, an array of the shape()
'''
# Retrieve information from ""cache""
(X_prev, W, b, s, p) = cashe

# Retrieve dimensions from X_prev's shape
(m, n_H_prev, n_W_prev, n_C_prev) = X_prev.shape 

# Retrieve dimensions from W's shape
(f, f, n_C_prev, n_C) = W.shape

# Retrieve dimensions from Z's shape
(m, n_H, n_W, n_C) = Z.shape

#create initial array for the output of the Deconvolution
X_curr = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))

#loop over the Training examples
for i in range (m):

    #loop over the vertical of the output
    for h in range(n_H):

        #loop over the horizontal of the output
        for w in range(n_W):

            #loop over the 
            for c in range (n_C):

                #loop over the color channels
                for x in range(n_C_prev):

                    #inverse_W = np.linalg.pinv(W[:, :, x, c])
                    transpose_W = np.transpose(W[:,:,x,c]) 
                    #X_curr[i, h*s:h*s+f, w*s:w*s+f, x] +=  Z[i, h, w, c] * inverse_W
                    X_curr[i, h*s:h*s+f, w*s:w*s+f, x] +=  Z[i, h, w, c] * transpose_W
                X_curr[i, h*s:h*s+f, w*s:w*s+f, :] += b[:,:,:,c]

X_curr = relu(X_curr)

return X_curr

","This is probably irreversible operation unless the pre-convolution data was not full rank. But note that you reduced the dimensions of your signal, so the convolution was probably cropped to ""valid"" information (which doesn't need padding)

If you have at least the image or the filter, recovery might be possible, since convolution can be inverted using deconvolution but note that:


Convolution is defined as
$$ f(x) \circledast g(x) =  h(x)  = \int_{-\infty}^{\infty}f(x-t)g(t)dt $$
The some integral transforms (such as Fourier and Laplace) have the property that:


$$ T\{f(x) \circledast g(x)\}(s) = T\{h(x)\}(s) = T\{f(x)\}(s) \times T\{g(x)\}(s) $$


This is true for the Discrete Fourier Transform and Discrete Convolution as well, so to find your image $i$ from the filtered image $j$ by a filter $h$ given $I$,$J$ and $H$ as the Discrete Fourier transform of $i$,$j$ and $h$, respectively. Let $F\{.\}$ denote the discrete fourier transform and $F^{-1}\{.\}$ denote its inverse transformation:


$$ i = F^{-1}\{I\} = F^{-1}\{\frac{J}{H}\} $$


The discrete fourier transform is implemented efficiently by SciPy(signal module), FFTW, NumPy(fft module) and probably Theano. Having a lot of wrappers arround.


Note 1: Is important to notice that you will need at least an estimation of your filter, there are a lot of algorithms to do so.

Note 2: Deconvolution is very sensitive to noise, you can check on this class on Digital Image Processing to understand image filtering, mainly the part on Wiener filters.

Note 3: Image Deconvolution is implemented on scikit-image (e.g. Unsupervised Wiener) and on OpenCV using many algorithms (also on matlab in Image Processing Toolbox).
",<python><deep-learning><convolution><numpy>
"Problem

fit_resample(X,y)


is taking too long to complete execution for 2million rows.

Dataset specifications

I have a labeled dataset about network features, where the X(features) and Y(labels) are of shape (2M, 24) and (2M,11) respectively.

i.e. there are over 2million rows in the dataset. there are 24 features, and 11 different classes/labels.

Both X and Y are numpy arrays of float dtype.

Motivation for using SVM SMOTE

Due to class imbalance, I realized SVM SMOTE is a good technique balance it, thereby, giving better classification.

Testing with smaller sub-datasets

To test the performance of my classifier, I started small. I made small subdatasets out of the big 2million row dataset.

It took the following code:-

%%time
sm = SVMSMOTE(random_state=42)
X_res, Y_res = sm.fit_resample(X, Y)


1st dataset contains only 7.5k rows. It took about 800ms to run the cell.
2nd dataset contains 115k rows. It took 20min to execute the cell.

Solution Attempts

My system crashes after running continuously for more than 48hrs, running out of memory.

I've tried some ideas, such as 

1. splitting it to run on multiple CPU cores using %%px

No improvement in quicker execution

2. using NVIDIA GPU's

Same as above. Which is more understandable since the _smote.py library functions aren't built with parallel programming for CUDA in mind. 

I'm pretty frustrated by the lack of results, and a warm PC. What should I do?
","This is expected and is not related to SMOTE sampling.

The computational complexity of non-linear SVM is on the order of $O(n^2)$ to $O(n^3)$ where $n$ is the number of samples. This means that if it takes 0.8 seconds for 7.5K data points, it should take [3, 48] minutes for 115K, $$[(115/7.5)^{2} \times 0.8, (115/7.5)^{3} \times 0.8]s=[3,48]m,$$and from 16 hours to 175 days, 11 days for $O(n^{2.5})$, for 2M data points.

You should continue using sample sizes on the order of 100K or less. Also, it is fruitful to track the accuracy (or any other score) as a function of samples for 1K, 10K, 50K, and 100K samples. It is possible that SVM accuracy stops improving well before 100K, therefore, there will be not much to lose by limiting the samples to 100K or less.
",<python><preprocessing><numpy><sampling><smote>
"I have in mind a program for analyzing short fragments of music, categorizing them as ""good"" or ""bad"". This would be part of a larger program that searches for larger good phrases and whole pieces. 

The idea now is to take a fragment of music, X, and compare it to known good sample fragments G_1, G_2, ... etc. and get a rank of similarity to each one. Then compare it to known bad fragments B_1, B_2, B_3, .. etc. 

""Good"" music is subjective of course, but this program would work with G and B samples that I hand-optimized according to my own tastes. 

Good music is then music that resembles at least one of the G's, while resembling none of the B's. A fragment that has strong similarity to both G's and B's is probably bad: The B's have veto power.

So, how to determine similarity? Musical fragments can be represented by image-like matrices of pixels. The vertical axis is pitch and the horizontal axis is time. If a note of pitch P_1 occurs between times T_beg and T_end, then that's like drawing a little line between (T_beg, P_1) and (T_end, P_1). 

An sample X to be classified can be convolved, in a sense, with a known sample K. It can be transposed up or down (vertical shifting) or moved left or right in time (or stretched in time, or other alterations) and each transposition would be superimposed on the G or B sample. I'm not too familiar with convolution but I think that overlapping pixels are multiplied and the sum of all is taken. The transposition with the brightest result pixel is a good indication of how similar X is to the K sample: it's magnitude becomes the measure of similarity.

Dark pixels don't matter much. A preponderance of dark pixels doesn't make music bad. It just means the real pattern isn't found in those locations. A bright match to a known bad fragment is what makes music bad.

I'd like to perform these computations with NumPy or a similar language optimized for matrix or image computations. 

Can I get some idea whether there is a name for this kind of operation, and where to look for efficient implementations of it? Boosting speed with a GPU would be a bonus.
","There are two high level approaches (Approach 2 was a better fit for a music-classification problem that I worked on) :


Signal processing + CNN  : Output of signal processing is saved as image. Models use the image as input. 




This paper is a good intro to the approach : https://arxiv.org/ftp/arxiv/papers/1712/1712.02898.pdf 



Couple of articles on this : https://www.codementor.io/vishnu_ks/audio-classification-using-image-classification-techniques-hx63anbx1 , https://medium.com/datadriveninvestor/audio-and-image-features-used-for-cnn-4f307defcc2f


Raw audio  + recurrent networks : https://deepmind.com/blog/wavenet-generative-model-raw-audio/  ,  https://arxiv.org/pdf/1606.04930.pdf , https://arxiv.org/pdf/1612.04928.pdf , https://gist.github.com/naotokui/12df40fa0ea315de53391ddc3e9dc0b9


GPU will make the project easier, but is not a requirement. 
",<machine-learning><classification><numpy><gpu>
"I'm trying to find the intersection of lines $y=a_1x+b_1$ and $y=a_2x+b_2$ using numpy.linalg.solve(). What I can't get my head around is how to correctly make $A$ a square matrix for solve() to work. I'm familiar with solving linear equation systems, but there's something here I don't get.

What I'd like to do is:

def meeting_lines(a1, b1, a2, b2):
    a = np.array([[a1], [a2]])
    b = np.array([b1, b2])
    return np.linalg.solve(a, b)

def main():
    a1=1
    b1=4
    a2=3
    b2=2

    y, x = meeting_lines(a1, b1, a2, b2)


Where I expect $y=-3$ and $x=1$. However, this fails with numpy.linalg.LinAlgError: Last 2 dimensions of the array must be square.

Thank you very much for your help, trying to figure this out has messed up my day already!
","to avoid numpy.linalg.LinAlgError: Singular matrix (determinant=0) when using other vars - Esmailian's codelines can be corrected with try: .. except: .. Error_handler - something like this:
import numpy as np

def meeting_lines(arr):
##    print(&quot;arr:\n&quot;,arr)
    a1, b1, a2, b2 = arr
    try:
        a = np.array([[a1, -1], [a2, -1]])
        b = np.array([-b1, -b2])
        res= np.linalg.solve(a, b)
    except: 
        # 1. The determinant of a singular matrix (for squared matrices) is zero. A non-invertible matrix is referred to as singular matrix 
        # 2. &quot;never explicitly invert matrix on a computer because of inaccuracy in floating point representation error&quot; - use linalg pckg with Basic Linear Algebra Subprograms (BLAS)
        # 3. in scipy to deal with floats can use: scipy.allclose(det, 0)

        res = np.Inf # perfect correlation (=1)
    return res

# USED for a1, b1, a2, b2 packed into array
print(meeting_lines(np.array([3., 1., 3., 7.], dtype=float)))

",<numpy><linear-algebra>
"I'm trying to find the intersection of lines $y=a_1x+b_1$ and $y=a_2x+b_2$ using numpy.linalg.solve(). What I can't get my head around is how to correctly make $A$ a square matrix for solve() to work. I'm familiar with solving linear equation systems, but there's something here I don't get.

What I'd like to do is:

def meeting_lines(a1, b1, a2, b2):
    a = np.array([[a1], [a2]])
    b = np.array([b1, b2])
    return np.linalg.solve(a, b)

def main():
    a1=1
    b1=4
    a2=3
    b2=2

    y, x = meeting_lines(a1, b1, a2, b2)


Where I expect $y=-3$ and $x=1$. However, this fails with numpy.linalg.LinAlgError: Last 2 dimensions of the array must be square.

Thank you very much for your help, trying to figure this out has messed up my day already!
","You should formulate your lines as follows to have $(x, y)$ as unknowns:
$$\begin{align}
\left.\begin{matrix}
a_1x-y=-b_1\\
a_2x-y=-b_2
\end{matrix}\right\}
\rightarrow
\overbrace{
\begin{bmatrix}
 a_1&amp; -1\\ 
 a_2&amp; -1
\end{bmatrix}
}^{\boldsymbol{a}}
\overbrace{
\begin{bmatrix}
 x\\ 
 y
\end{bmatrix}
}^{\boldsymbol{x}}
=
\overbrace{
\begin{bmatrix}
 -b_1\\ 
 -b_2
\end{bmatrix}
}^{\boldsymbol{b}}
\end{align}$$
Therefore, the code should be:

import numpy as np

def meeting_lines(a1, b1, a2, b2):
    a = np.array([[a1, -1], [a2, -1]])
    b = np.array([-b1, -b2])
    return np.linalg.solve(a, b)

a1=1
b1=4
a2=3
b2=2
x, y = meeting_lines(a1, b1, a2, b2)
print(x, y)


which outputs:

1.0 5.0

",<numpy><linear-algebra>
"According to the convolution theorem, convolution operation changes to pointwise multiplication in fourier domain - here I have 'fft_x' of shape (batchsize, height, width, in_channels) which is the fft of input data and similarly 'fft_kernel' of shape (height, width, in_channels, out_channels) which is fft of the kernel after being padded to image size. To get pointwise multiplication of these in efficient way, I was using einsum in the following way -

...
    print(fft_x)
    print(fft_kernel)
    output = 0
    n=int(self.no_of_kernels/2)+1      # n = out_channels here
    for i in range(n):
        output += np.einsum('ijkl,jkl-&gt;ijk', fft_x, fft_kernel[i])
    return output 
...


It gives the following output and error-

Tensor(""input_11:0"", shape=(?, 28, 28, 1), dtype=complex64)
Tensor(""fourier__conv2d_11/transpose:0"", shape=(28, 28, 1, 17), dtype=complex64)
...
...
ValueError: einstein sum subscripts string contains too many subscripts for operand 0


Could anyone please explain why this error is arising? Thanks in advance for any help.
","import tensorflow as tf
output  = tf.einsum('ijkl,jklo-&gt;ijko', fft_x, fft_kernel)


I used tf.einsum instead of np.einsum, and it worked. 
Also as seen in above, I removed the loop and changed the equation in einsum. 
",<python><tensorflow><numpy>
"I have a matrix $b$ with elements:
$$b =
\begin{pmatrix}
 0.01 &amp; 0.02 &amp; \cdots &amp; 1 \\
 0.01 &amp; 0.02 &amp; \cdots &amp; 1 \\
 \vdots&amp; \vdots  &amp; \ddots  &amp; \vdots \\
 0.01 &amp; 0.02 &amp; \cdots &amp; 1 \\    
 \end{pmatrix}
$$For which through a series of calculation which is vectorised, $b$ is used to calculate $a$ which is another matrix that has the same dimension/shape as $b$.
$$a =
\begin{pmatrix}
 3 &amp; 5 &amp; \cdots &amp; 17 \\
 2 &amp; 6 &amp; \cdots &amp; 23 \\
 \vdots&amp; \vdots  &amp; \ddots  &amp; \vdots \\
 4 &amp; 3 &amp; \cdots &amp; 19 \\    
 \end{pmatrix}
$$
At this point it is important to note that the elements of $a$ and $b$ have a one to one correspondence. The different row values(let's call it $\sigma$) $0.01, 0.02...$ are different parameters for a series of simulations that I'm running. Hence for a fixed value of say $\sigma = 0.01$, the length of its column values correspond to the total number of ""simulations"" I'm running for that particular parameter. If you know python vectorisation then you'll start to understand what I'm doing.

It is known that higher the $\sigma$, the more the number of simulations for that particular sigma will have a value higher than 5 i.e. more of the matrix element along a column will have value bigger than 5. Essentially what I'm doing is vectorising $N$(columns) different simulations for $M$(rows) different parameters. Now I wish to find out the value of $\sigma$ for which the total number simulation that's bigger than 5, is bigger than 95% of the total simulation.

To put it more concisely, for a $\sigma$ of 0.02, each simulation would have results of $$5, 6, ..., 3$$ with say a total simulation of $N$. So let $$\kappa = \sum{ (\text{all the simulations that have values bigger than 5})},$$I wish to find out the FIRST $\sigma$ for which
$$\frac{\kappa}{N} &gt; 0.95*N$$
i.e. the FIRST $\sigma$ for which the proportion of total experiment for which its value $&gt;5$ is bigger than 95% of the total number of experiment.

The code that I have written is:

# say 10000 simulations for a particular sigma 
SIMULATION = 10000

# say 100 different values of sigma ranging from 0.01 to 1
# this is equivalent to matrix b in mathjax above
SIGMA = np.ones((EXPERIMENTS,100))*np.linspace(0.01, 1, 100)

def return_sigma(matrix, simulation, sigma):
    """"""
    My idea here is I put in sigma and matrix and total number of simulation. 
    Each time using np.ndenumerate looping over i and j to compare if the 
    element values are greater than 5. If yes then I add 1 to counter, if no 
    then continue. If the number of experiments with result bigger than 5 is 
    bigger than 95% of total number of experiment then I return that particular 
    sigma.
    """"""
    counter = 0
    for (i, j), value in np.ndenumerate(matrix):
        if value[i, j] &gt; 5:
            counter+=1
        if counter/experiments &gt; 0.95*simulation:
            break
        return sigma[0, j] # sigma[:, j] should all be the same anyway
""""""Now this can be ran by:""""""
print(return_sigma(a, SIMULATION, SIGMA))


which doesn't seem to quite work as I'm not well-versed with 2D slicing comprehension so this is quite a challenging problem for me. Thanks in advance.

EDIT
I apologise on not giving away my calculation as it's sort of a coursework of mine. I have generated a for 15 different values of $\sigma$ with 15 simulations each, and here they are:

array([[ 6,  2, 12, 12, 14, 14, 11, 11,  9, 23, 15,  3, 10, 12, 10],
       [ 7,  7,  6,  9, 13,  8, 11, 17, 13,  8, 10, 16, 11, 16,  8],
       [14,  6,  4,  8, 10,  9, 11, 14, 12, 14,  5,  8, 18, 29, 22],
       [ 4, 12, 12,  3,  7,  8,  5, 13, 13, 10, 14, 16, 22, 15, 22],
       [ 9,  8,  7, 12, 12,  6,  4, 13, 12, 12, 18, 20, 18, 14, 23],
       [ 8,  6,  8,  6, 12, 11, 11,  4,  9,  9, 13, 19, 13, 11, 20],
       [12,  8,  7, 17,  3,  9, 11,  5, 12, 24, 11, 12, 17,  9, 16],
       [ 4,  8,  7,  5,  6, 10,  9,  6,  4, 13, 13, 14, 18, 20, 23],
       [ 5, 10,  5,  6,  8,  4,  7,  7, 10, 11,  9, 22, 14, 30, 17],
       [ 6,  4,  5,  9,  8,  8,  4, 21, 14, 18, 21, 13, 14, 22, 10],
       [ 6,  2,  7,  7,  8,  3,  7, 19, 14,  7, 13, 12, 18,  8, 12],
       [ 5,  7,  6,  4, 13,  9,  4,  3, 20, 11, 11,  8, 12, 29, 14],
       [ 6,  3, 13,  6, 12, 10, 17,  6,  9, 15, 12, 12, 16, 12, 15],
       [ 2,  9,  8, 15,  5,  4,  5,  7, 16, 13, 20, 18, 14, 18, 14],
       [14, 10,  7, 11,  8, 13, 14, 13, 12, 19,  9, 10, 11, 17, 13]])


As you can see as $\sigma$ gets higher the number of matrix elements in each column for which it is bigger than 5 is higher.

EDIT 2
So now condition is giving me the right thing, which is an array of booleans.

array([[False, False, False, False, False, False, False, False, True, True],
       ....................................................................,
       [False, False, False, False, False, False, False, True,  True, True]])


So now the last row is the important thing here as it corresponds to the parameters, in this case,

array([[0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.5],
       ...........................................................,
       [0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5]])


Now the last row of condition is telling me that first True happens at $\sigma$=0.4 i.e. for which all the > 95% of the total simulations for that $\sigma$ have simulation result of > 5. So now I need to return the index of condition where the first True in the last row appeared i.e. [i, j]. Now doing b[i, j] should give me the parameter I want.(which I'm not sure if your next few line of codes are doing that.)
","I think I have understood your problem (mostly from the comments added in your function).

I'll show step by step what the logic is, building upon each previous step to get the final solution.

First we want to find all position where the matrix is larger than 5:

a &gt; 5    # returns a boolean array with true/false in each position


Now we want to check each row to count if the proportion of of matches (> 5) has reached a certain threshold; $N * 0.95$. We can divide by the number of simulations (number of columns) to essentially normalise by the number of simulations:

(a &gt; 5) / SIMULATION    # returns the value of one match


These values are required to sum to your threshold for an experiment to be valid.

Now we cumulatively sum across each row. As the True/False array is ones and zeros, we now have a running total of the numbers of matches for each experiment (each row).

np.cumsum((a &gt; 5) / SIMULATION, axis=1)     # still same shape as b


Now we just need to find out where (in each row) the sum of matches reaches your threshold. We can use np.where:

## EDIT: we only need to check the cumsum is greater than 0.95 and not (0.95 * SUMLATION)
## because we already ""normalised"" the values within the cumsum.
condition = np.cumsum((a &gt; 5) / SIMULATION, axis=0) &gt; 0.95
mask = np.where(condition)


I broke it down now as the expressions are getting long.

That gave us the i and j coordinates of places where the condition was True. We just want to find the place where we first breached the threshold, so we want to find the indices for the first time in each row:

valid_rows = np.unique(mask[0], return_index=True)[1]    # [1] gets the indices themselves


Now we can simply use these indices to get the first index in each valid row, where the threshold was breached:

valid_cols = mask[1][valid_rows]


So now you can get the corresponding values from the parameter matrix using these valid rows/columns:

params = b[valid_rows, valid_cols]




If this is correct, it should be significantly faster than your solution because it avoids looping over the 2d array and instead utilises NumPy's vectorised method and ufuncs.
",<python><dataset><data><numpy>
"I have a matrix $b$ with elements:
$$b =
\begin{pmatrix}
 0.01 &amp; 0.02 &amp; \cdots &amp; 1 \\
 0.01 &amp; 0.02 &amp; \cdots &amp; 1 \\
 \vdots&amp; \vdots  &amp; \ddots  &amp; \vdots \\
 0.01 &amp; 0.02 &amp; \cdots &amp; 1 \\    
 \end{pmatrix}
$$For which through a series of calculation which is vectorised, $b$ is used to calculate $a$ which is another matrix that has the same dimension/shape as $b$.
$$a =
\begin{pmatrix}
 3 &amp; 5 &amp; \cdots &amp; 17 \\
 2 &amp; 6 &amp; \cdots &amp; 23 \\
 \vdots&amp; \vdots  &amp; \ddots  &amp; \vdots \\
 4 &amp; 3 &amp; \cdots &amp; 19 \\    
 \end{pmatrix}
$$
At this point it is important to note that the elements of $a$ and $b$ have a one to one correspondence. The different row values(let's call it $\sigma$) $0.01, 0.02...$ are different parameters for a series of simulations that I'm running. Hence for a fixed value of say $\sigma = 0.01$, the length of its column values correspond to the total number of ""simulations"" I'm running for that particular parameter. If you know python vectorisation then you'll start to understand what I'm doing.

It is known that higher the $\sigma$, the more the number of simulations for that particular sigma will have a value higher than 5 i.e. more of the matrix element along a column will have value bigger than 5. Essentially what I'm doing is vectorising $N$(columns) different simulations for $M$(rows) different parameters. Now I wish to find out the value of $\sigma$ for which the total number simulation that's bigger than 5, is bigger than 95% of the total simulation.

To put it more concisely, for a $\sigma$ of 0.02, each simulation would have results of $$5, 6, ..., 3$$ with say a total simulation of $N$. So let $$\kappa = \sum{ (\text{all the simulations that have values bigger than 5})},$$I wish to find out the FIRST $\sigma$ for which
$$\frac{\kappa}{N} &gt; 0.95*N$$
i.e. the FIRST $\sigma$ for which the proportion of total experiment for which its value $&gt;5$ is bigger than 95% of the total number of experiment.

The code that I have written is:

# say 10000 simulations for a particular sigma 
SIMULATION = 10000

# say 100 different values of sigma ranging from 0.01 to 1
# this is equivalent to matrix b in mathjax above
SIGMA = np.ones((EXPERIMENTS,100))*np.linspace(0.01, 1, 100)

def return_sigma(matrix, simulation, sigma):
    """"""
    My idea here is I put in sigma and matrix and total number of simulation. 
    Each time using np.ndenumerate looping over i and j to compare if the 
    element values are greater than 5. If yes then I add 1 to counter, if no 
    then continue. If the number of experiments with result bigger than 5 is 
    bigger than 95% of total number of experiment then I return that particular 
    sigma.
    """"""
    counter = 0
    for (i, j), value in np.ndenumerate(matrix):
        if value[i, j] &gt; 5:
            counter+=1
        if counter/experiments &gt; 0.95*simulation:
            break
        return sigma[0, j] # sigma[:, j] should all be the same anyway
""""""Now this can be ran by:""""""
print(return_sigma(a, SIMULATION, SIGMA))


which doesn't seem to quite work as I'm not well-versed with 2D slicing comprehension so this is quite a challenging problem for me. Thanks in advance.

EDIT
I apologise on not giving away my calculation as it's sort of a coursework of mine. I have generated a for 15 different values of $\sigma$ with 15 simulations each, and here they are:

array([[ 6,  2, 12, 12, 14, 14, 11, 11,  9, 23, 15,  3, 10, 12, 10],
       [ 7,  7,  6,  9, 13,  8, 11, 17, 13,  8, 10, 16, 11, 16,  8],
       [14,  6,  4,  8, 10,  9, 11, 14, 12, 14,  5,  8, 18, 29, 22],
       [ 4, 12, 12,  3,  7,  8,  5, 13, 13, 10, 14, 16, 22, 15, 22],
       [ 9,  8,  7, 12, 12,  6,  4, 13, 12, 12, 18, 20, 18, 14, 23],
       [ 8,  6,  8,  6, 12, 11, 11,  4,  9,  9, 13, 19, 13, 11, 20],
       [12,  8,  7, 17,  3,  9, 11,  5, 12, 24, 11, 12, 17,  9, 16],
       [ 4,  8,  7,  5,  6, 10,  9,  6,  4, 13, 13, 14, 18, 20, 23],
       [ 5, 10,  5,  6,  8,  4,  7,  7, 10, 11,  9, 22, 14, 30, 17],
       [ 6,  4,  5,  9,  8,  8,  4, 21, 14, 18, 21, 13, 14, 22, 10],
       [ 6,  2,  7,  7,  8,  3,  7, 19, 14,  7, 13, 12, 18,  8, 12],
       [ 5,  7,  6,  4, 13,  9,  4,  3, 20, 11, 11,  8, 12, 29, 14],
       [ 6,  3, 13,  6, 12, 10, 17,  6,  9, 15, 12, 12, 16, 12, 15],
       [ 2,  9,  8, 15,  5,  4,  5,  7, 16, 13, 20, 18, 14, 18, 14],
       [14, 10,  7, 11,  8, 13, 14, 13, 12, 19,  9, 10, 11, 17, 13]])


As you can see as $\sigma$ gets higher the number of matrix elements in each column for which it is bigger than 5 is higher.

EDIT 2
So now condition is giving me the right thing, which is an array of booleans.

array([[False, False, False, False, False, False, False, False, True, True],
       ....................................................................,
       [False, False, False, False, False, False, False, True,  True, True]])


So now the last row is the important thing here as it corresponds to the parameters, in this case,

array([[0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.5],
       ...........................................................,
       [0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5]])


Now the last row of condition is telling me that first True happens at $\sigma$=0.4 i.e. for which all the > 95% of the total simulations for that $\sigma$ have simulation result of > 5. So now I need to return the index of condition where the first True in the last row appeared i.e. [i, j]. Now doing b[i, j] should give me the parameter I want.(which I'm not sure if your next few line of codes are doing that.)
","Is this helpful?

import numpy as np, numpy.random as npr
N_sims = 15 # sims per sigma
N_vals = 15 # num sigmas
# Parameters
SIGMA = np.ones( (N_sims, N_vals) ) * np.linspace(0.01, 1, N_vals)
# Generate ""results"" :3 (i.e., the matrix a)
RESULTS = npr.random_integers(low=1, high=10, size=SIGMA.shape)
for i in range(N_vals): 
    RESULTS[:, i] += npr.random_integers(low=0, high=1, size=(N_sims)) + i // 3
print(""SIGMA\n"", SIGMA)
print(""RESULTS\n"", RESULTS)
# Mark the positions &gt; 5
more_than_five = RESULTS &gt; 5
print(""more_than_five\n"", more_than_five)
# Count how many are greater than five, per column (i.e., per sigma)
counts = more_than_five.sum(axis=0)
print('COUNTS\n', counts)
# Compute the proportions (so, 1 if all exps were &gt; 5)
proportions = counts.astype(float) / N_sims
print('Proportions\n', proportions)
# Find the first time it is larger than 0.5
first_index = np.argmax( proportions &gt; 0.95 )
print('---\nFIRST INDEX\n', first_index)

",<python><dataset><data><numpy>
"I've created 50 random x and y points (with slope of y = 2x-1).

First, I used Linear Regression from sklearn to fit the model onto my dataset where I got a slope of 2.0066... and intercept of -0.535...

My Question: is fitting the model to our dataset considered training? For each given x value, since it has a y-value (supervised), does our machine go through each x,y match and create line of best fit based upon that? Thus, is our model trained?

Second, I used stats.linregress(x,y) from scipy to get slope and intercept (which seem really close if not the same to the slope and intercept I've got from using sklearn Linear Regression).

My Question: If both methods give the same result, why not just use scipy to get formula for the best fit line to make predictions? What is the benefit of using machine learning?
","
Yes fitting the data and finding the best fitting line is called training the model. 
If you look at the source code of scikit-learn linear regression you can find the its using scipy linalg.lstsq module for finding the coefficients and intercept (most cases). See the source code for more details . Machine learning is fancy word for Application of  mathematics (on data mostly) using computers (machines)

",<machine-learning><python><scikit-learn><numpy><scipy>
"I calculated the eigenvectors and eigenvalues from a covariance matrix given a data matrix of 3 columns and 2 rows.

I am trying to interpret results but I can't understand on how to interpret them.

Create a 2x3 matrix:

# Create a 2x3 matrix

data = np.around(np.random.uniform(size=(2,3)) * 100)


The data looks as follows:

    [
      [ 4., 65., 77.],
      [68., 12., 89.]
    ]

# Here each row represents one data point 
# and columns represent the features in the data set
# So there are 3 features and 2 data points


Calculate the mean for each feature in the data set.

mean = np.mean(data, axis = 0)


Center the data around origin, by subtracting mean from the data set.

difference = np.subtract(data, mean)


Now, calculate the covariance matrix:

cov = np.dot(difference.T, difference)


The cov matrix looks as follows:

[
    [ 2048. , -1696. ,   384. ],
    [-1696. ,  1404.5,  -318. ],
    [  384. ,  -318. ,    72. ]
]


As I understand about the covariance matrix, it explains the variance between all feature-pairs. Since there are 3 features, it gives out a 3x3 matrix explaining the variance between all possible pairs.

Finally, calculate the eigenvectors and eigenvalues:

val, vec = np.linalg.eigh(cov)


The vec matrix looks as follows:

[
    [ 0.60999981,  0.21639063,  0.76228297],
    [ 0.77451164,  0.040441  , -0.63126559],
    [ 0.16742745, -0.97546892,  0.14292806]
]


How do I interpret the the vector matrix? I understand what are eigenvectors physically. They do not change in position when an object undergoes a transformation but only a scalar change by their eigenvalues.

What are some possible ways, I could use this vec matrix?
","The eigenvectors can be used to transform your data into a coordinate system in which no covariance is there. Assume we have a $p$ dimensional multivariate normal distribution with 

$$\mathcal{N}(\boldsymbol{x}|\boldsymbol{\mu}=\boldsymbol{0},\boldsymbol{\Sigma})=\dfrac{1}{\sqrt{(2\pi)^p\det\boldsymbol{\Sigma}}}\exp\left[-\dfrac{1}{2}\boldsymbol{x}^T\boldsymbol{\Sigma}\boldsymbol{x} \right].\quad (*)$$

As the covariance matrix is real and symmetric we know it is diagonalizable and that we can scale the eigenvectors to represent an orthonormal basis by the set of all eigenvectors. The eigenvalue equation for the covariance matrix is given by

$$\boldsymbol{\Sigma}\boldsymbol{v}_i=\lambda_i\boldsymbol{v}_i \quad \forall i=1,...,p.$$

We can combine all equations into a single matrix equation

$$\boldsymbol{\Sigma}[\boldsymbol{v}_1,\ldots,\boldsymbol{v}_p]=[\boldsymbol{v}_1,\ldots,\boldsymbol{v}_p]\text{diag}\left[\lambda_1,\ldots,\lambda_p \right].$$

If we call $\boldsymbol{V}=[\boldsymbol{v}_1,\ldots,\boldsymbol{v}_p]$ and $\boldsymbol{\Lambda}=\left[\lambda_1,\ldots,\lambda_p \right].$ With these definitions in hand we can write the eigenvalue equation as

$$\boldsymbol{\Sigma}\boldsymbol{V}=\boldsymbol{V}\boldsymbol{\Lambda}$$
$$\implies \boldsymbol{\Sigma}=\boldsymbol{V}\boldsymbol{\Lambda}\boldsymbol{V}^{-1}.$$

as $\boldsymbol{V}$ is orthogonal (consists of orthonormal vectors) we can write $\boldsymbol{V}^{-1}=\boldsymbol{V}^T.$ This implies

$$\boldsymbol{\Sigma}=\boldsymbol{V}\boldsymbol{\Lambda}\boldsymbol{V}^{T}.$$

We plug this into $(*)$ and introduce the new variable $\boldsymbol{z}=\boldsymbol{V}^T\boldsymbol{x}$.

$$\mathcal{N}(\boldsymbol{x}|\boldsymbol{\mu}=\boldsymbol{0},\boldsymbol{\Sigma})=\dfrac{1}{\sqrt{(2\pi)^p\det\boldsymbol{V}\boldsymbol{\Lambda}\boldsymbol{V}^T}}\exp\left[-\dfrac{1}{2}\boldsymbol{x}^T\boldsymbol{V}\boldsymbol{\Lambda}\boldsymbol{V}^{T}\boldsymbol{x} \right]$$
$$=\dfrac{1}{\sqrt{(2\pi)^p\det\boldsymbol{V}\boldsymbol{\Lambda}\boldsymbol{V}^T}}\exp\left[-\dfrac{1}{2}\boldsymbol{z}\boldsymbol{\Lambda}\boldsymbol{z} \right]$$
$$\implies \mathcal{N}(\boldsymbol{z}|\boldsymbol{\mu}=\boldsymbol{0},\boldsymbol{\Lambda})=\dfrac{1}{\sqrt{(2\pi)^p\det\boldsymbol{\Lambda}}}\exp\left[-\dfrac{1}{2}\boldsymbol{z}\boldsymbol{\Lambda}\boldsymbol{z} \right].$$

In the last step I used $\det \boldsymbol{ABC}=\det\boldsymbol{A}\det\boldsymbol{B}\det\boldsymbol{C}$, $\det\boldsymbol{A}=\det\boldsymbol{A}^T$ for orthogonal matrices and $|\det\boldsymbol{A}|=1$ for orthogonal matrices. Hence, we proved that we can use the eigenvectors to linearly transform our variables to obtain a new coordinate system which has a diagonal covariance matrix $\boldsymbol{\Lambda}$ (which is diagonal). This implies that we do not have any covariance anymore.

Remark 1: This diagonalization is what the principal components analysis is also doing under the hood. 

Remark 2: There is one suboptimal part of your code. You should explicitly determine the sample_size = data.shape[0] and then calculate cov = 1 / (sample_size - 1) * np.dot(difference.T, difference).
",<python><pca><numpy><matrix>
"I have the data in the following format:

1: DATA NUMPY ARRAY (trainX)

A numpy array of a set of numpy array of 3d np arrays.
To be more articulate the format is: [[3d data], [3d data], [3d data], [3d data], ...]

2: TARGET NUMPY ARRAY (trainY)

This consists of a numpy array of the corresponding target values for the above array.

The format is [target1, target2, target3]

The numpy array gets quite large, and considering that I'll be using a deep neural network, there will be many parameters that would need fitting into the memory as well.

How can I push the numpy arrays in batches for trainX and trainY
","You should implement a generator and feed it to model.fit_generator().

Your generator may look like this:

def batch_generator(X, Y, batch_size = BATCH_SIZE):
    indices = np.arange(len(X)) 
    batch=[]
    while True:
            # it might be a good idea to shuffle your data before each epoch
            np.random.shuffle(indices) 
            for i in indices:
                batch.append(i)
                if len(batch)==batch_size:
                    yield X[batch], Y[batch]
                    batch=[]



And then, somewhere in your code:

train_generator = batch_generator(trainX, trainY, batch_size = 64)
model.fit_generator(train_generator , ....)


UPD.: 
I order to avoid placing all your data into memory beforehand, you can modify the generator to consume only the identifiers of your data-set and then load your data on-demand:

def batch_generator(ids, batch_size = BATCH_SIZE):
    batch=[]
    while True:
            np.random.shuffle(ids) 
            for i in ids:
                batch.append(i)
                if len(batch)==batch_size:
                    yield load_data(batch)
                    batch=[]



Your loader function may look like this:

def load_data(ids):
   X = []
   Y = []

   for i in ids:
     # read one or more samples from your storage, do pre-processing, etc.
     # for example:
     x = imread(f'image_{i}.jpg')
     ...
     y = targets[i]

     X.append(x)
     Y.append(y)

   return np.array(X), np.array(Y)

",<machine-learning><neural-network><deep-learning><keras><numpy>
"I have the data in the following format:

1: DATA NUMPY ARRAY (trainX)

A numpy array of a set of numpy array of 3d np arrays.
To be more articulate the format is: [[3d data], [3d data], [3d data], [3d data], ...]

2: TARGET NUMPY ARRAY (trainY)

This consists of a numpy array of the corresponding target values for the above array.

The format is [target1, target2, target3]

The numpy array gets quite large, and considering that I'll be using a deep neural network, there will be many parameters that would need fitting into the memory as well.

How can I push the numpy arrays in batches for trainX and trainY
","Another approach using Keras Sequence class:
class DataGenerator(keras.utils.Sequence):
  def __init__(self, x_data, y_data, batch_size):
    self.x, self.y = x_data, y_data
    self.batch_size = batch_size
    self.num_batches = np.ceil(len(x_data) / batch_size)
    self.batch_idx = np.array_split(range(len(x_data)), self.num_batches)

  def __len__(self):
    return len(self.batch_idx)

  def __getitem__(self, idx):
    batch_x = self.x[self.batch_idx[idx]]
    batch_y = self.y[self.batch_idx[idx]]
    return batch_x, batch_y

train_generator = DataGenerator(x_train, y_train, batch_size = 128)
model.fit(train_generator,...)

```

",<machine-learning><neural-network><deep-learning><keras><numpy>
"I'm still a beginner in machine learning and I want to know how to code this situation based on python and machine learning (clustering).

I have data like:

id      Column1    duration(seconde)    column3
1       aaa        20                   bbb
2       ccc        01                   ddd
3       eee        150                  fff
4       ggg        25                   hhh


I want to group my data according to the duration column value and create new column containing a category name based on duration cluster.  I want to get this result:

id      Column1    duration(seconde)    column3      NewColCategorie
1       aaa        20                   bbb          Cat2 
2       ccc        01                   ddd          Cat1
3       eee        150                  fff          Cat3
4       ggg        25                   hhh          Cat2
5       iii        175                  jjj          Cat3

","Here, is another way to use clustering for creating a new feature.
We pass the input_data to fit_predict and store the result in new col_name.
Note: just remember that need to standardize data before performing clustering.
from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=4, max_iter=500, init=&quot;k-means++&quot;, tol=0.001)
X = np.array([[1, 2], [1, 4], [1, 0],
              [10, 2], [10, 4], [10, 0]])

X[&quot;cluster&quot;] = kmeans.fit_predict(X)
X[&quot;cluster&quot;] = X[&quot;cluster&quot;].astype(&quot;category&quot;)
```

",<python><pandas><numpy>
"I'm still a beginner in machine learning and I want to know how to code this situation based on python and machine learning (clustering).

I have data like:

id      Column1    duration(seconde)    column3
1       aaa        20                   bbb
2       ccc        01                   ddd
3       eee        150                  fff
4       ggg        25                   hhh


I want to group my data according to the duration column value and create new column containing a category name based on duration cluster.  I want to get this result:

id      Column1    duration(seconde)    column3      NewColCategorie
1       aaa        20                   bbb          Cat2 
2       ccc        01                   ddd          Cat1
3       eee        150                  fff          Cat3
4       ggg        25                   hhh          Cat2
5       iii        175                  jjj          Cat3

","To do clustering you can use sklearn's KMeans Clustering function - sklearn.cluster.KMeans with n_clusters=3 and other parameters as default. This will give you 3 clusters. After you have trained your model you can use the .labels_ attribute of the trained model to classify every example. You can do this in the following way:

&gt;&gt;&gt; from sklearn.cluster import KMeans
&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; X = np.array([[1, 2], [1, 4], [1, 0],
...               [10, 2], [10, 4], [10, 0]])
&gt;&gt;&gt; kmeans = KMeans(n_clusters=2, random_state=0).fit(X)
&gt;&gt;&gt; kmeans.labels_
array([1, 1, 1, 0, 0, 0], dtype=int32)


To create a new column based on category cluster you can simply add the kmeans.labels_ array as a column to your original dataframe:

&gt;&gt;&gt; df['categories'] = kmeans.labels_

",<python><pandas><numpy>
"I have an understanding of this error, it means that the input that I'm passing to the model is of a different dimension that what was expected. The error also states that the input that I'm passing is of the dimension (1,) while it was expecting (2,)

I have tested the input value dimension by using x.shape and it prints out (2,) still the error exists. As a counter-intuitive move I picked one of the data that was in the training data and printed the shape of the zeroth element x1[0].shape also used that as an input, the error still exists.

model.fit works well, having error with model.predict (tried passing one of the training data hardcoded, still doesn't work)

CODE:

import tensorflow as tf
import numpy as np
from tensorflow import keras
import csv

x1, ys = [], []

with open('./house.csv') as csv_file:
    csv_reader = csv.reader(csv_file, delimiter=',')
    line = 0
    for row in csv_reader:
        if line &gt; 0:
            x1.append([row[1], row[3]])
            ys.append(row[5])
        line += 1


model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[2])])
model.compile(optimizer='sgd', loss='mean_squared_error')
x1 = np.asarray(x1, dtype=float)
ys = np.asarray(ys, dtype=float)
model.fit(x1, ys, epochs=500)

print(x1[0].shape)
while True:
    house_size = float(input('Enter the house size: '))
    house_size = house_size/3000
    bhks = float(input('Enter the BHK: '))
    bhks = bhks/3
    x = np.array([house_size, bhks])
    try:
        value = model.predict(x)
    except Exception as e:
        print(e)
        print(x)
        print(x.shape)
    else:
        value = value[0][0] * 500
        print(value)

","Yoy always need to pass the data for prediction in batches, although this batch is of size one (one sample). Try changing this line:

x = np.array([house_size, bhks])


into this: 

x = np.array([[house_size, bhks]])


This should work.
",<machine-learning><keras><tensorflow><machine-learning-model><numpy>
"m38     m78        alpha

4.4717  4.8745
4.4569  4.6491
4.5101  4.7262
4.4407  4.8234
4.1184  4.3862
3.8448  4.2816
3.7246  4.6183
3.2857  4.6744  

For the above sample data (actually 8000 rows) i need to calculate alpha(column), using the log function.

In: alpha = (np.log([dataset.m38,dataset.m78])/np.math.log(38,78.7))

OUT: array([[1.79754463, 1.90105609],
       [1.7935659 , 1.8442364 ],
       [1.80780671, 1.86397626],
       ...,
       [2.06431358, 2.40416332],
       [2.08820691, 2.41635699],
       [2.09982107, 2.39551918]])

I converted this into dataframe by using the below code:

In: alpha = pd.DataFrame(data = alpha)

OUT: 2 rows × 8046 columns

Now, i used alpha.T to get this in 8046 rows and 2 columns.

Why i am getting 2 columns not one? and how should i convert it into one column?
","You are not getting 2 columns. You are getting 2 rows.

The function alpha = (np.log([dataset.m38,dataset.m78])/np.math.log(38,78.7)) returns a 2-dimensional array. Putting in:

         m38     m78
      0  4.4717  4.8745
      1  4.4569  4.6491
      2  4.5101  4.7262
      3  4.4407  4.8234
      4  4.1184  4.3862


gives us: [[0.41174795 0.41083658 0.4140986  0.40983552 0.38912198]
           [0.43545842 0.42244323 0.42696487 0.43256132 0.40644075]]

This is due to the fact that np.log([dataset.m38,dataset.m78]) returns a 2-dimensional array.

Regarding the flipped columns/ row issue: Once your function doesn't return a 2-dimensional array, it will most likely return a Pandas Series which will have one column and 8000 rows.

If you want to turn that into a DataFrame, you can use 
alpha = pd.DataFrame({'alpha':alpha})

Think about what your formula does. Putting in [dataset.m38,dataset.m78] will do the calculation for both columns seperately resulting in 2 different arrays.
",<pandas><numpy>
"I'm creating a basic application to predict the 'Closing' value of a stock for day n+1, given features of stock n using Python and Scikit-learn
A sample row in my dataframe looks like this (2000 rows)
       Open     Close    High     Low      Volume     
0      537.40   537.10   541.55   530.47   52877.98  

Similar to this video, where he uses 'Dates' and 'Open Price'. In this example, Dates are the features and Open price is the target.
Now in my example, I don't have a 'Dates' value in my dataset, but instead want to use Open, High, Low, Volume data as the features because I thought that would make it more accurate
I was defining my features and targets as so
features = df.loc[:,df.columns != 'Closing']
targets = df.loc[:,df.columns  == 'Closing']

Which would return a  df looking like this
features:
       Open      High      Low      Vol from  
29     670.02    685.11    661.09   92227.36

targets:
       Close
29     674.57

However I realised that the data needs to be in a numpy array, so I now get my features and targets like this
features = df.loc[:,df.columns != 'Closing'].values
targets = df.loc[:,df.columns  == 'Closing'].values

So now my features look like this
[6.70020000e+02 6.85110000e+02 6.61090000e+02 9.22273600e+04
  6.23944806e+07]
 [7.78102000e+03 8.10087000e+03 7.67541000e+03 6.86188500e+04
  5.41391322e+08]

and my targets look like this
[  674.57]
[ 8042.64]

I then split up my data using
X_training, X_testing, y_training, y_testing = train_test_split(features, targets, test_size=0.8)

I tried to follow the Scikit-Learn documentation, which resulted in the following
svr_rbf = svm.SVR(kernel='rbf', C=100.0, gamma=0.0004, epsilon= 0.01 )
svr_rbf.fit(X_training, y_training)
predictions = svr_rbf.predict(X_testing)
print(predictions)

I assumed that this would predict the Y values given the testing features, which I could then plot against the actual y_testing values to see how similar they are. However, the predictions is printing out the same value for each X_testing feature.
[3763.84681818 3763.84681818 3763.84681818 3763.84681818 3763.84681818

I've tried changing the value of epsilon, c and gamma but that doesnt seem to change the fact that the predictions always gives the same value
I know that it might not be accurate to predict stock prices, but I must have done something wrong to get the same value when applying the model to various different test data
","There are a couple of parts that I think changing will help.

First, a general one for all model building: I would suggest you scale your data before putting it into the model. 

It might not directly solve the problem of receiving the same predicted value in each step, but you might notice that you predictions lie somewhere in the ranges of your input values - as you are using unscaled volume, that is making things difficult for the model. It is essentially have to work on two different scales at the same time, which is cannot do very well.

Have a look at the StandardScaler in sklean for a way how to do that.



Next a few suggestions of things to change, specifically because you are working with stock prices:

I would normally predict the value of the stock market tomorrow, and not the closing prices on the same data, where you are using open/high/low/volume. For me that only make sense if you were to have high-frequency (intraday) data.
Given this, you would need to shift your y value by one step. There is a method on Pandas DataFrames to help with that, but as you dont have a date column and you only need to shift by one timestep anyway, you can just do this:

features = df.loc[:-1, df.columns != 'Closing'].values    # leave out last step
targets = df.loc[1:, df.columns  == 'Closing'].values     # start one step later


You could then even then predict the opening price of the following day, or keep closing data in the features data, as that would not introduce temporal bias.



Something that would require more setup, would be to look at shuffling your data. Again, because you want to use historical values to predict future ones, you need to keep the relevant hsitory together. Have a look at my other answer to this question and the diagram, which explains more about this idea. 

EDIT

You should also scale y_train and y_test, so that the model knows to predict within that range. Do this using the same StandardScaler instance, as not to introduce bias. Have a look at this short tutorial. Your predictions will then be within the same range (e.g. [-1, +1]). You can compute errors on that range too. If you really want, you can then scale your predictions back to the original range so they look more realistic, but that isn't really necessary to validate the model. You can simply plot the predictions against ground truth in the scaled space.

Check out this thread, which explains a few reasons as to why you should use the same instance of StandardScaler on the test data.
",<python><regression><pandas><numpy><svr>
"Suppose I have symmetric positive definite covariance function $k:\mathbb{R}\times\mathbb{R}\rightarrow \mathbb{R}$ that is non-stationary (i.e. $k(x,y) \neq g(|x-y|)$ for any function $g$). Is there a fast way in Python given design points $(x_1,\ldots,x_n$) to calculate its covariance matrix $(k(x_i,x_j))_{i,j}$? 

If the covariance function is stationary then we can compute the whole matrix at once using numpy's matrix operations and avoid slow Python loops - e.g. in this. 

Currently my implementation is:

dim = len(X)
kern_mat = np.zeros((dim,dim))
   for i in range(dim):
      for j in range(i+1):
         kern_mat[i,j] = kern(X[i],X[j])
         kern_mat[j,i] = kern_mat[i,j]

Any help with speedups or otherwise is appreciated!
","I still would apply numpy's covaranice function using numpy.apply_along_axis 

import numpy as np

x = np.array([[0, 2], [1, 1], [2, 0]]).T
np.apply_along_axis(func1d=np.cov, arr=x, axis=0)

",<machine-learning><python><statistics><numpy><gaussian>
"Let's say that I have two 1 dimensional arrays, and when I plot the two arrays they look like this:



If you look at the top and bottom graphs, then you can see that the highlighted parts are very similar (in this case they're exactly the same). I need to find a way to find these sections using some sort of algorithm or method.

I've tried searching everywhere in the numpy and scikit docs, I even searched everywhere on stackexchange and couldn't find a solution for this problem. I don't think anyone published a solution for this yet.

Does anyone have any idea how I can find similar sections in two graphs? My dataset is a 1 dimensional data array for each graph, and I need a algorithm that tells me where the similar parts are. Just remember that the similar sections are never 100% the same, sometimes they're a little bit off and sometimes there's anomalies so a small part would be different but everything else will still look the same. Also you can ignore the curvature of the graph, that's irrelevant. Only the X and Y coordinates of the data points are important.

I can't read explanations that have a lot of maths inside of them and I also can't turn explanations that have a lot of maths into code, I'm still learning how to do that at University. But I'm really good at reading pseudo-code and other programming languages so please give me an answer with real code.
","Well, you need to first define what your threshold for 'similar' is, and also what length of similarity is meaningful to you.

One way of achieving this is by taking a 'slice' of the first set of coordinates, and comparing them against each slice of the same size in the second set. If all values are within a certain threshold distance, bingo.

You can then repeat this with the next slice of coordinates from set #1.

e.g. here is an O(n2) implementation:

slice_len = 10
thresh = 2
overlap_x1 = []
overlap_x2 = []

for i in range(len(x1)-slice_len):
    for j in range(len(x2)-slice_len):
        # checking the y coords are all at most 'threshold' far away
        if max(abs(y1[i:i+slice_len]-y2[j:j+slice_len])) &lt; thresh:
            # Adding the similar x-coords to the containers
            overlap_x1.append(x1[i:i+slice_len])
            overlap_x2.append(x2[i:i+slice_len])

# Converting arrays to ordered sets to remove duplicates from overlap
# Since they are x-coords, they are monotonic increasing, order is preserved
overlap_x1 = OrderedSet(overlap_x1)
overlap_x2 = OrderedSet(overlap_x2)

",<python><scikit-learn><similarity><graphs><numpy>
"Let's say that I have two 1 dimensional arrays, and when I plot the two arrays they look like this:



If you look at the top and bottom graphs, then you can see that the highlighted parts are very similar (in this case they're exactly the same). I need to find a way to find these sections using some sort of algorithm or method.

I've tried searching everywhere in the numpy and scikit docs, I even searched everywhere on stackexchange and couldn't find a solution for this problem. I don't think anyone published a solution for this yet.

Does anyone have any idea how I can find similar sections in two graphs? My dataset is a 1 dimensional data array for each graph, and I need a algorithm that tells me where the similar parts are. Just remember that the similar sections are never 100% the same, sometimes they're a little bit off and sometimes there's anomalies so a small part would be different but everything else will still look the same. Also you can ignore the curvature of the graph, that's irrelevant. Only the X and Y coordinates of the data points are important.

I can't read explanations that have a lot of maths inside of them and I also can't turn explanations that have a lot of maths into code, I'm still learning how to do that at University. But I'm really good at reading pseudo-code and other programming languages so please give me an answer with real code.
","This can be solved in simply O(1) complexity using Deep learning technique called oneshot learning. If you are to find the exact match, we are going to set the cosine similarity to 1 and convolve the kernel over the second image and calculate the difference with the first image to find the difference. Read further about one_shot learning here.
",<python><scikit-learn><similarity><graphs><numpy>
"I'm using python. Some 2D numpy arrays are stored in individual rows of a Series. They are 30x30 images. It looks something like this:

     pixels
0    [[23,4,54...],[54,6,7...],[........]]
1    [[65,54,255,55,...],[43,54,6...],[......]]
...
...
...
7000 [[........]]


For each row in the Series, I want to take these 2D arrays, flatten them to 1D, take the values and assign them to the columns of one row in data frame. Each row will have 30x30 = 900pixels each, storing the values of each pixel. Like this:

    pixel1    pixel2    pixel3...    pixel900
0       23         4        54             77
1       65        54       255             33
...
...
... 


I'm using an elaborate function that extracts one row from the series at a time, flattens the array, converts it to a Series again, and appends it to a dataframe. It takes sooo long. I'm sure there must be a faster way. I'm using this code:

def prep_pixels(X):
    # X is a series
    df = pd.DataFrame()
    for i in range(len(X.index)): #iterate through the whole series
        df = df.append(pd.Series(X[i].flatten()), ignore_index=True) 
    return df


EDIT:
Upon request from a user, I will provide code with how I ended up in this rut in the first place :D

#reading the files
filepath = 'dataset.pickle'
data_np = pd.read_pickle(filepath)
print(data_np[0])


Output:

 [array([[255, 248, 253, 255, 251, 253, 254, 236, 220, 217, 191, 145, 139,
        185, 216, 227, 252, 251, 254, 248, 251, 236, 221, 222, 213, 175,
        120,  75,  74, 209],
       [255, 253, 254, 253, 252, 254, 223, 146,  87,  75,  58,  30,  27,
         58,  86, 116, 157, 168, 164, 165, 167, 136,  96,  71,  59,  49,
         21,   9,  27, 144],
       [255, 255, 255, 248, 252, 255, 202,  88,  15,  16,  14,  11,  11,
         12,  12,  20,  40,  46,  38,  43,  40,  25,  21,  19,  17,  35,
         53,  58,  64, 124],
    ... 30 rows of 30 pixels
    ...
    ... last row coming up ...
    [255, 255, 254, 254, 253, 252, 253, 254, 255, 255, 254, 252, 249,
    249, 251, 213, 126, 178, 231, 252, 248, 250, 254, 254, 252, 253,
    255, 255, 255, 255]], dtype=uint8), 'क']


The last symbol in this list is the character that this image represents. It's the 'label'. It's supervised learning using CNNs. Anyway, I need them to be in the other format I described to be able to work with them. This is how I'm handling this data:

data = pd.DataFrame(data_np, columns=['pixels','labels'])
def prep_pixels(X):
    df = pd.DataFrame()
    for i in range(len(X.index)): #iterate through whole series
        df = df.append(pd.Series(X[i].ravel()), ignore_index=True)  
    return df

X = prep_pixels(data['pixels'])
y = data['labels']


EDIT: a user suggested that I use a mutable datatype to do this procedure. They said that it might speed things up because the computation does not need to make copies of data. I used some nested for loops and it cut the time to half (1 min 22 sec instead of 3 min). I still feel like its pathetic, given that my dataset has just 7000, 30x30 pixel images. Or, maybe I'm just new to data wrangling. 

Here is the code I used. Please let me know if you have any other suggestions:

filepath = 'dataset.pickle'
data_np = pd.read_pickle(filepath)

df = pd.DataFrame()
for row in range(IMG_ROW):   
    for col in range(IMG_COL):
        pixel=[]
        for img in range(len(data_np)):
            pixel.append(data_np[img][0][row][col]) 
        columns = pd.Series(pixel, name=col)
        df = pd.concat([df, columns], ignore_index=True, axis=1)

","I'm getting timeit results of about 1/4 of the time using:

flatX = X.apply(lambda x: x.flatten())
pd.DataFrame(item for item in flatX)


See also 
https://stackoverflow.com/questions/45901018/convert-pandas-series-of-lists-to-dataframe
for some possibly better options for the second line.

(Regarding my earlier comment, I don't get any real savings by just dataframe-ing at the end.)
",<python><pandas><numpy><dataframe><data-wrangling>
"I'm using python. Some 2D numpy arrays are stored in individual rows of a Series. They are 30x30 images. It looks something like this:

     pixels
0    [[23,4,54...],[54,6,7...],[........]]
1    [[65,54,255,55,...],[43,54,6...],[......]]
...
...
...
7000 [[........]]


For each row in the Series, I want to take these 2D arrays, flatten them to 1D, take the values and assign them to the columns of one row in data frame. Each row will have 30x30 = 900pixels each, storing the values of each pixel. Like this:

    pixel1    pixel2    pixel3...    pixel900
0       23         4        54             77
1       65        54       255             33
...
...
... 


I'm using an elaborate function that extracts one row from the series at a time, flattens the array, converts it to a Series again, and appends it to a dataframe. It takes sooo long. I'm sure there must be a faster way. I'm using this code:

def prep_pixels(X):
    # X is a series
    df = pd.DataFrame()
    for i in range(len(X.index)): #iterate through the whole series
        df = df.append(pd.Series(X[i].flatten()), ignore_index=True) 
    return df


EDIT:
Upon request from a user, I will provide code with how I ended up in this rut in the first place :D

#reading the files
filepath = 'dataset.pickle'
data_np = pd.read_pickle(filepath)
print(data_np[0])


Output:

 [array([[255, 248, 253, 255, 251, 253, 254, 236, 220, 217, 191, 145, 139,
        185, 216, 227, 252, 251, 254, 248, 251, 236, 221, 222, 213, 175,
        120,  75,  74, 209],
       [255, 253, 254, 253, 252, 254, 223, 146,  87,  75,  58,  30,  27,
         58,  86, 116, 157, 168, 164, 165, 167, 136,  96,  71,  59,  49,
         21,   9,  27, 144],
       [255, 255, 255, 248, 252, 255, 202,  88,  15,  16,  14,  11,  11,
         12,  12,  20,  40,  46,  38,  43,  40,  25,  21,  19,  17,  35,
         53,  58,  64, 124],
    ... 30 rows of 30 pixels
    ...
    ... last row coming up ...
    [255, 255, 254, 254, 253, 252, 253, 254, 255, 255, 254, 252, 249,
    249, 251, 213, 126, 178, 231, 252, 248, 250, 254, 254, 252, 253,
    255, 255, 255, 255]], dtype=uint8), 'क']


The last symbol in this list is the character that this image represents. It's the 'label'. It's supervised learning using CNNs. Anyway, I need them to be in the other format I described to be able to work with them. This is how I'm handling this data:

data = pd.DataFrame(data_np, columns=['pixels','labels'])
def prep_pixels(X):
    df = pd.DataFrame()
    for i in range(len(X.index)): #iterate through whole series
        df = df.append(pd.Series(X[i].ravel()), ignore_index=True)  
    return df

X = prep_pixels(data['pixels'])
y = data['labels']


EDIT: a user suggested that I use a mutable datatype to do this procedure. They said that it might speed things up because the computation does not need to make copies of data. I used some nested for loops and it cut the time to half (1 min 22 sec instead of 3 min). I still feel like its pathetic, given that my dataset has just 7000, 30x30 pixel images. Or, maybe I'm just new to data wrangling. 

Here is the code I used. Please let me know if you have any other suggestions:

filepath = 'dataset.pickle'
data_np = pd.read_pickle(filepath)

df = pd.DataFrame()
for row in range(IMG_ROW):   
    for col in range(IMG_COL):
        pixel=[]
        for img in range(len(data_np)):
            pixel.append(data_np[img][0][row][col]) 
        columns = pd.Series(pixel, name=col)
        df = pd.concat([df, columns], ignore_index=True, axis=1)

","Or you walk through each column, see Test3.
Mind in general:

&quot;.apply()&quot; is slower than list comprehension, see https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas.
Column-wise is faster than row-wise in the standard case of #rows &gt; #columns (of course).
Use numpy if you can.

Results:

Test1: 232.96580633900157 (@IsuShrestha, flatten each row, append row-wise)
Test2: 6.919965944998694 (@BenReiniger, flatten each row)
Test3: 0.3909464059997845 (append column-wise = over all rows in one go = 29 appends)

(AMD A8-3870 APU with Radeon(tm) HD Graphics, 3000 MHz, 4 cores, 4 logical processors, 8 GB memory, Windows 10 64bit)
import numpy as np
import pandas as pd
# https://stackoverflow.com/questions/7370801/how-to-measure-elapsed-time-in-python
from timeit import default_timer as timer


row = [np.array([
    [255, 248, 253, 255, 251, 253, 254, 236, 220, 217, 191, 145, 139,
    185, 216, 227, 252, 251, 254, 248, 251, 236, 221, 222, 213, 175,
    120,  75,  74, 209],
       [255, 255, 255, 248, 252, 255, 202,  88,  15,  16,  14,  11,  11,
     12,  12,  20,  40,  46,  38,  43,  40,  25,  21,  19,  17,  35,
     53,  58,  64, 124],
       [255, 253, 254, 253, 252, 254, 223, 146,  87,  75,  58,  30,  27,
     58,  86, 116, 157, 168, 164, 165, 167, 136,  96,  71,  59,  49,
     21,   9,  27, 144],
       [255, 255, 255, 248, 252, 255, 202,  88,  15,  16,  14,  11,  11,
     12,  12,  20,  40,  46,  38,  43,  40,  25,  21,  19,  17,  35,
     53,  58,  64, 124],
       [255, 253, 254, 253, 252, 254, 223, 146,  87,  75,  58,  30,  27,
     58,  86, 116, 157, 168, 164, 165, 167, 136,  96,  71,  59,  49,
     21,   9,  27, 144],
       [255, 255, 255, 248, 252, 255, 202,  88,  15,  16,  14,  11,  11,
     12,  12,  20,  40,  46,  38,  43,  40,  25,  21,  19,  17,  35,
     53,  58,  64, 124],
       [255, 253, 254, 253, 252, 254, 223, 146,  87,  75,  58,  30,  27,
     58,  86, 116, 157, 168, 164, 165, 167, 136,  96,  71,  59,  49,
     21,   9,  27, 144],
       [255, 255, 255, 248, 252, 255, 202,  88,  15,  16,  14,  11,  11,
     12,  12,  20,  40,  46,  38,  43,  40,  25,  21,  19,  17,  35,
     53,  58,  64, 124],
       [255, 253, 254, 253, 252, 254, 223, 146,  87,  75,  58,  30,  27,
     58,  86, 116, 157, 168, 164, 165, 167, 136,  96,  71,  59,  49,
     21,   9,  27, 144],
       [255, 255, 255, 248, 252, 255, 202,  88,  15,  16,  14,  11,  11,
     12,  12,  20,  40,  46,  38,  43,  40,  25,  21,  19,  17,  35,
     53,  58,  64, 124],       
       [255, 253, 254, 253, 252, 254, 223, 146,  87,  75,  58,  30,  27,
     58,  86, 116, 157, 168, 164, 165, 167, 136,  96,  71,  59,  49,
     21,   9,  27, 144],
       [255, 255, 255, 248, 252, 255, 202,  88,  15,  16,  14,  11,  11,
     12,  12,  20,  40,  46,  38,  43,  40,  25,  21,  19,  17,  35,
     53,  58,  64, 124],
       [255, 253, 254, 253, 252, 254, 223, 146,  87,  75,  58,  30,  27,
     58,  86, 116, 157, 168, 164, 165, 167, 136,  96,  71,  59,  49,
     21,   9,  27, 144],
       [255, 255, 255, 248, 252, 255, 202,  88,  15,  16,  14,  11,  11,
     12,  12,  20,  40,  46,  38,  43,  40,  25,  21,  19,  17,  35,
     53,  58,  64, 124],
       [255, 253, 254, 253, 252, 254, 223, 146,  87,  75,  58,  30,  27,
     58,  86, 116, 157, 168, 164, 165, 167, 136,  96,  71,  59,  49,
     21,   9,  27, 144],
       [255, 255, 255, 248, 252, 255, 202,  88,  15,  16,  14,  11,  11,
     12,  12,  20,  40,  46,  38,  43,  40,  25,  21,  19,  17,  35,
     53,  58,  64, 124],
       [255, 253, 254, 253, 252, 254, 223, 146,  87,  75,  58,  30,  27,
     58,  86, 116, 157, 168, 164, 165, 167, 136,  96,  71,  59,  49,
     21,   9,  27, 144],
       [255, 255, 255, 248, 252, 255, 202,  88,  15,  16,  14,  11,  11,
     12,  12,  20,  40,  46,  38,  43,  40,  25,  21,  19,  17,  35,
     53,  58,  64, 124],
       [255, 253, 254, 253, 252, 254, 223, 146,  87,  75,  58,  30,  27,
     58,  86, 116, 157, 168, 164, 165, 167, 136,  96,  71,  59,  49,
     21,   9,  27, 144],
       [255, 255, 255, 248, 252, 255, 202,  88,  15,  16,  14,  11,  11,
     12,  12,  20,  40,  46,  38,  43,  40,  25,  21,  19,  17,  35,
     53,  58,  64, 124],    
       [255, 253, 254, 253, 252, 254, 223, 146,  87,  75,  58,  30,  27,
     58,  86, 116, 157, 168, 164, 165, 167, 136,  96,  71,  59,  49,
     21,   9,  27, 144],
       [255, 255, 255, 248, 252, 255, 202,  88,  15,  16,  14,  11,  11,
     12,  12,  20,  40,  46,  38,  43,  40,  25,  21,  19,  17,  35,
     53,  58,  64, 124],
       [255, 253, 254, 253, 252, 254, 223, 146,  87,  75,  58,  30,  27,
     58,  86, 116, 157, 168, 164, 165, 167, 136,  96,  71,  59,  49,
     21,   9,  27, 144],
       [255, 255, 255, 248, 252, 255, 202,  88,  15,  16,  14,  11,  11,
     12,  12,  20,  40,  46,  38,  43,  40,  25,  21,  19,  17,  35,
     53,  58,  64, 124],
       [255, 253, 254, 253, 252, 254, 223, 146,  87,  75,  58,  30,  27,
     58,  86, 116, 157, 168, 164, 165, 167, 136,  96,  71,  59,  49,
     21,   9,  27, 144],
       [255, 255, 255, 248, 252, 255, 202,  88,  15,  16,  14,  11,  11,
     12,  12,  20,  40,  46,  38,  43,  40,  25,  21,  19,  17,  35,
     53,  58,  64, 124],
       [255, 253, 254, 253, 252, 254, 223, 146,  87,  75,  58,  30,  27,
     58,  86, 116, 157, 168, 164, 165, 167, 136,  96,  71,  59,  49,
     21,   9,  27, 144],
       [255, 255, 255, 248, 252, 255, 202,  88,  15,  16,  14,  11,  11,
     12,  12,  20,  40,  46,  38,  43,  40,  25,  21,  19,  17,  35,
     53,  58,  64, 124],
       [255, 253, 254, 253, 252, 254, 223, 146,  87,  75,  58,  30,  27,
     58,  86, 116, 157, 168, 164, 165, 167, 136,  96,  71,  59,  49,
     21,   9,  27, 144],         
    [255, 255, 254, 254, 253, 252, 253, 254, 255, 255, 254, 252, 249,
    249, 251, 213, 126, 178, 231, 252, 248, 250, 254, 254, 252, 253,
    255, 255, 255, 255]
    ], dtype='uint8'), &quot;क&quot;]



data_np = []
for i in range(7000):
    data_np.append(row)
    
    

data = pd.DataFrame(data_np, columns=['pixels','labels'])


# Test1 (@IsuShrestha, flatten each row, append row-wise)
def prep_pixels(X):
    df = pd.DataFrame()
    for i in range(len(X.index)): #iterate through whole series
        df = df.append(pd.Series(X[i].ravel()), ignore_index=True)  
    return df
start = timer()
test = prep_pixels(data['pixels'])
print(timer()-start)
print(test.shape)
# 232.96580633900157
# (7000, 900)


# Test2 (@BenReiniger, flatten each row)
def prep_pixels2(X):
    flatX = X.apply(lambda x: x.flatten())
    return pd.DataFrame(row for row in flatX)
start = timer()
test2 = prep_pixels2(data['pixels'])
print(timer()-start)
print(test2.shape)
# 6.919965944998694
# (7000, 900)


# Test3 (append column-wise = over all rows in one go = 29 appends)
def prep_pixels3(X):
    test = np.array([x[0] for x in X])
    for i in range(len(X[0])-1):
        # print(i)
        test = np.append(arr=test, values=np.array([x[i+1] for x in X]), axis=1)
    return pd.DataFrame(test)
start = timer()
test3 = prep_pixels3(data['pixels'].to_numpy())
print(timer()-start)
print(test3.shape)
# 0.3909464059997845
# (7000, 900)

",<python><pandas><numpy><dataframe><data-wrangling>
"I'm a newbie in data science. I'm working on a regression problem. I'm getting 2.5 MAPE. 400 MAE 437000 MSE. As my MAPE is quite low but why I'm getting high MSE and MAE?  This is the link to my data

from sklearn.metrics import mean_absolute_error 
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import Normalizer
import matplotlib.pyplot as plt
def mean_absolute_percentage_error(y_true, y_pred): 
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

import pandas as pd
from sklearn import preprocessing

features=pd.read_csv('selectedData.csv')
import numpy as np
from scipy import stats
print(features.shape)
features=features[(np.abs(stats.zscore(features)) &lt; 3).all(axis=1)]
target = features['SYSLoad']
features= features.drop('SYSLoad', axis = 1)
names=list(features)

for i in names:
    x=features[[i]].values.astype(float)
    min_max_scaler = preprocessing.MinMaxScaler()
    x_scaled = min_max_scaler.fit_transform(x)
    features[i]=x_scaled


Selecting the target Variable which want to predict and for which we are finding feature imps

import numpy as np
print(features.shape)
print(features.describe())
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = 
train_test_split(features, target, test_size = 0.25, random_state = 42)
trans=Normalizer().fit(train_input);
train_input=Normalizer().fit_transform(train_input);
test_input=trans.fit_transform(test_input);

n=test_target.values;
test_targ=pd.DataFrame(n);

from sklearn.svm import SVR
svr_rbf = SVR(kernel='poly', C=10, epsilon=10,gamma=10)
y_rbf = svr_rbf.fit(train_input, train_target);
predicted=y_rbf.predict(test_input);
plt.figure
plt.xlim(300,500);
print('Total Days For training',len(train_input)); print('Total Days For 
Testing',len(test_input))
plt.ylabel('Load(MW) Prediction 3 '); plt.xlabel('Days'); 
plt.plot(test_targ,'-b',label='Actual'); plt.plot(predicted,'-r',label='POLY 
kernel ');
plt.gca().legend(('Actual','RBF'))
plt.title('SVM')
plt.show();


test_target=np.array(test_target)
print(test_target)
MAPE=mean_absolute_percentage_error(test_target,predicted);
print(MAPE);
mae=mean_absolute_error(test_target,predicted)
mse=mean_squared_error(test_target, predicted)
print(mae);
print(mse);
print(test_target);
print(predicted);

","I'll be honest, I haven't thoroughly checked your code. However, I can see that the range of values of your dataset is approx [0,12000]. As an engineer, I see that:


sqrt(MSE) = sqrt(437000) = 661 units.
MAE = 400 units.
MAPE = 2.5 which means that MAE can be up to 0.025*12000= 250 units.


All three cases show similar magnitude of error, so I wouldn't say that ""MAPE is quite low but you're getting high mse and MAE"". 

Those 3 values explain the results from similar yet different perspectives. Keep in mind, if the values were all the same, there would have been no need for all 3 of those metrics to exist :)
",<data-mining><pandas><svm><numpy>
"I'm a newbie in data science. I'm working on a regression problem. I'm getting 2.5 MAPE. 400 MAE 437000 MSE. As my MAPE is quite low but why I'm getting high MSE and MAE?  This is the link to my data

from sklearn.metrics import mean_absolute_error 
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import Normalizer
import matplotlib.pyplot as plt
def mean_absolute_percentage_error(y_true, y_pred): 
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

import pandas as pd
from sklearn import preprocessing

features=pd.read_csv('selectedData.csv')
import numpy as np
from scipy import stats
print(features.shape)
features=features[(np.abs(stats.zscore(features)) &lt; 3).all(axis=1)]
target = features['SYSLoad']
features= features.drop('SYSLoad', axis = 1)
names=list(features)

for i in names:
    x=features[[i]].values.astype(float)
    min_max_scaler = preprocessing.MinMaxScaler()
    x_scaled = min_max_scaler.fit_transform(x)
    features[i]=x_scaled


Selecting the target Variable which want to predict and for which we are finding feature imps

import numpy as np
print(features.shape)
print(features.describe())
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = 
train_test_split(features, target, test_size = 0.25, random_state = 42)
trans=Normalizer().fit(train_input);
train_input=Normalizer().fit_transform(train_input);
test_input=trans.fit_transform(test_input);

n=test_target.values;
test_targ=pd.DataFrame(n);

from sklearn.svm import SVR
svr_rbf = SVR(kernel='poly', C=10, epsilon=10,gamma=10)
y_rbf = svr_rbf.fit(train_input, train_target);
predicted=y_rbf.predict(test_input);
plt.figure
plt.xlim(300,500);
print('Total Days For training',len(train_input)); print('Total Days For 
Testing',len(test_input))
plt.ylabel('Load(MW) Prediction 3 '); plt.xlabel('Days'); 
plt.plot(test_targ,'-b',label='Actual'); plt.plot(predicted,'-r',label='POLY 
kernel ');
plt.gca().legend(('Actual','RBF'))
plt.title('SVM')
plt.show();


test_target=np.array(test_target)
print(test_target)
MAPE=mean_absolute_percentage_error(test_target,predicted);
print(MAPE);
mae=mean_absolute_error(test_target,predicted)
mse=mean_squared_error(test_target, predicted)
print(mae);
print(mse);
print(test_target);
print(predicted);

","You are stating something that is by definition the case.  A Mean Absolute Percentage Error (MAPE) is typically a small number expressed in percentage, hopefully in the single digit.  Meanwhile the Mean Squared Error (MSE) and Mean Absolute Error) are expressed in square of units and units respectively.  If your units are > 1, the MSE could get easily very large on a relative basis compared to the MAE.  And, the MAE could also be a lot larger than the MAPE.  This is just like saying a nominal number is a lot larger than its log or natural logs.  Your three error measures are measured on pretty different scale.

They just bring some perspective to how well your model fit your data.  Depending on the circumstances or context one error measure may be more relevant than the other.  
",<data-mining><pandas><svm><numpy>
"So when running this example script from Keras repo (https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py), I found that we can easily run into out of memory for the input or output one-hot encoding in this code:

encoder_input_data = np.zeros(
    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),
    dtype='float32')
decoder_input_data = np.zeros(
    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),
    dtype='float32')
decoder_target_data = np.zeros(
    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),
dtype='float32')


When I have a huge size of training samples, this one-hot does not fit into memory. Is there way to handle this issue?

UPDATE:
I changed float32 to uint, but thats about the smallest one hot array can get.
","so I found a solution by dividing input training data into small batches and it did the trick. Here is the code:

divideby=1000
for j in range(divideby):
    start=j*len(input_texts)/divideby
    end=(j+1)*len(input_texts)/divideby if j&lt;divideby+1 else -1

    encoder_input_data = np.zeros(
        (end-start, max_encoder_seq_length, num_encoder_tokens),
        dtype='uint8')  # size_of_training_samples, max_length_word_measured_in_characters,number_of_unique_chars,
    decoder_input_data = np.zeros(
        (end-start, max_decoder_seq_length, num_decoder_tokens),
        dtype='uint8')
    decoder_target_data = np.zeros(
        (end-start, max_decoder_seq_length, num_decoder_tokens),
        dtype='uint8')

    for i, (input_text, target_text) in enumerate(zip(input_texts[start:end], target_texts[start:end])):
        for t, char in enumerate(input_text.split(splitby)):
            encoder_input_data[i, t, input_token_index[char]] = 1.
        for t, char in enumerate(target_text.split(splitby)):
            # decoder_target_data is ahead of decoder_input_data by one timestep
            decoder_input_data[i, t, target_token_index[char]] = 1.
            if t &gt; 0:
                # decoder_target_data will be ahead by one timestep
                # and will not include the start character.
                decoder_target_data[i, t - 1, target_token_index[char]] = 1.

    model.fit([encoder_input_data, decoder_input_data], decoder_target_data,
              batch_size=batch_size,
              epochs=epochs,
              validation_split=0.2)
```

",<python><deep-learning><numpy>
"I am new in using python for data science.
What is the difference between selecting a a column with: df['name'].values and df.iloc[:,1].values and df.iloc[:,1:2].values they return differnt types of numpy vectors. why?
","Not entirely sure what you mean by &quot;numpy vectors&quot; but am assuming the question is why each of these methods return essentially (almost but not quite) the same output...
Reference: pandas docs.
df['name'].values is a &quot;Series corresponding to colname&quot;. In other words, you're just calling the data from that column and putting the in an array by calling .values.
.iloc is a &quot;Purely integer-location based indexing for selection by position&quot;. Same as above but you're calling the indexed location of the column where df.iloc[:, 1] is df.iloc[all rows, col 2]. Probably an easier method to call multiple consecutive columns in a DataFrame then writing out each individual column name.
df.iloc[:,1:2].values &lt;-- creates an array of arrays where the main array is the column that you called (col2) and each row values is contained in a subarray. This is--I think-- because you're slicing the dataframe between column index locations 1 and 2 (rather than just calling loc 1 like above). This would mean that each row is being called individually so that a new array is created for each row that exists between column index locations 1 and 2 (which is the 'parent' array).
",<python><dataset><pandas><numpy><dataframe>
"How come python code suggestion is awful in notebooks and spyder? Smetimes it shows classes methods and variables, sometimes not
","If you are working locally (running e.g. starting up a Jupyter notebook from a terminal on localhost) code completiong should work just fine - I have never had any problems.

If you are running a Jupyter notebook attached to a Python process in a Docker container, there can be issues with code completion (the two-way communication may need to be activated).

You can try running the following ipython ""magic command"" that makes the interpreter greedy - this has worked for me in the past:

%config IPCompleter.greedy = True


Simply run that in the first Jupyter cell.

Here is a list of all built-in magic commands.

I am afraid I don't know of any differences in these cases between jupyter notebooks and Spyder.
",<python><pandas><numpy><jupyter>
"I am working with 3D matrix in Python, for example, given matrix like this with size of 2x3x4:

[[[1 2 1 4]
  [3 2 1 1]
  [4 3 1 4]]

 [[2 1 3 3]
  [1 4 2 1]
  [3 2 3 3]]]


I have task to find the value of entropy in each row in each dimension matrix. For example, in row 1 of dimension 1 of the matrix above [1,2,1,4], the normalized value (as such the total sum is 1) is [0.125, 0.25, 0.125, 0.5] and the value of entropy is calculated by the formula -sum(i*log(i)) where i is the normalized value. The resulting matrix is a 2x3 matrix where in each dimension there are 3 values of entropy (because there are 3 rows).

Here is the working example of my code using random matrix each time:

from scipy.stats import entropy
import numpy as np

matrix = np.random.randint(low=1,high=5,size=(2,3,4)) #how if size is (200,50,1000)
entropy_matrix=np.zeros((matrix.shape[0],matrix.shape[1]))
for i in range(matrix.shape[0]):
    normalized = np.array([float(k)/np.sum(j) for j in matrix[i] for k in j]).reshape(matrix.shape[1],matrix.shape[2])
    entropy_matrix[i] = np.array([entropy(m) for m in normalized])


My question is how do I scale-up this program to work with very large 3D matrix (for example with size of 200x50x1000) ?

I am using Python in Windows 10 (with Anaconda distribution). 
Using 3D matrix size of 200x50x1000, I got running time of 290 s on my computer.
","It is faster if you use the built-in functions of numpy (instead of reimplementing them yourself):

import numpy as np
from scipy.stats import entropy

np.apply_along_axis(func1d=entropy, axis=2, arr=matrix)

",<python><numpy><scipy><matrix>
"I am trying to generate a complex Gaussian white noise, with zero mean and the covariance matrix of them is going to be a specific matrix which is assumed to be given. 

Assume i to be a point on the grid of x axis, where there are N points on the axis. The problem is to generate a complex valued random noise at each point (let's call the random value at the point i as $y_i$), which obeys Gaussian distribution and has a covariance matrix of, 

$E((y_i)(y_j^*)) = c_{ij}$

where $c_{ij}$ is a given diagonal NxN matrix. * is complex conjugate, E represents the expectation value.

I am trying to generate this on Python. 
","Might be a bit late, but if you're still looking for an answer, here it is. You can use np.random.multivariate_normal. You need to provide the said covariance matrix as an argument to the function. You can find the documentation here.
",<python><numpy><gaussian><noise>
"Background: We are trying to build a customized ML library in Python 3 to tackle analysis we often repeat, in a general fashion. But it would not be nearly as general as sklearn. In fact, we are prepared to break some interfaces if that give us enough performance boost in return. 

The basic starting point would be constructing a Learner by feeding it X and y, and predict on new input X0

learner = Learner(X, y)
y0 = learner.predict(X0)


One of the design decision is what data type to use for X and y, here are 3 choices, along with some rudimentary brainstorm advantages for each respectively:


native Python list: X being a list of lists, y being a list. Would this have better performance for being 'closer to metal'?
numpy: X being an ndarray of (n, p), y being an ndarray of (n, ). This would benefit from the richer functionalities in numpy/scipy. This is also the data type choice of sklearn.
pandas: X being a DataFrame, y being a Series. This can utilize more Data Analysis (read: dirty work) functionalities from pandas. This way we can also refer to variables with their names instead of just integer indices. But performance would be the worst? 


Please share your thinking of pros and cons for each choice from both tech and math perspectives. Thanks in advance!

PS: I thought about whether this should be a StackOverflow question, but still feel this is more Data Science.
","Pandas does normally a decent job allowing dataframes to behave as numpy arrays. 

My recommendation is to use numpy types, the reason is that, for consistency with pretty much what the industry is doing, you are much safer with numpy.

I love pandas, and I love the dataframes, but they provide extra functionality that the model does NOT need, the same way in general programming you will not use a String to represent a boolean (even though tou could do it with a String), simply because you should use whatever data types provides you the functionality you need... and nothing else.

So, numpy is the way to go. As for python lists, you do not get the mathematical operations that you get with numpy, so do not consider them.
",<python><scikit-learn><pandas><numpy>
"If I have two arrays as shown below:

a = numpy.array([0, 0, 1, 0, 1, 1, 1, 0, 1])
b = numpy.array([1, 1, 1, 0, 0, 1, 1, 0, 0])


Is there an easy way using numpy to count the number of occurrences where elements at the same index in each of the two arrays have a value equal to one. In the above two arrays, the elements in position(zero-indexed) 2, 5 and 6 are equal to 1 in both the arrays. Thus I want to get a count of 3 here.

Thank you for any help that you may be able to provide.
","sum(a * b) 


Should do the job:)

As pointed out by @n1k31t4 it only works if you have two arrays that contain only 0 and 1. Otherwise you would have to write something like: 

 sum((a == 1) * (b ==1)) 


What I find interesting here is that the sum functions treats boolean vectors as vectors with  0 (for False) and 1 (for True) on which you can perform arithmetic operation (+ for or, * for and etc..)
",<python><numpy>
"If I have two arrays as shown below:

a = numpy.array([0, 0, 1, 0, 1, 1, 1, 0, 1])
b = numpy.array([1, 1, 1, 0, 0, 1, 1, 0, 0])


Is there an easy way using numpy to count the number of occurrences where elements at the same index in each of the two arrays have a value equal to one. In the above two arrays, the elements in position(zero-indexed) 2, 5 and 6 are equal to 1 in both the arrays. Thus I want to get a count of 3 here.

Thank you for any help that you may be able to provide.
","There are two ways I'll show you (there are probably a lot more using NumPy):

First method: chaining operations

You can use ""masking"" followed by the comparison and finally a sum operation:

We want all values in a from the indices where b is equal to 1:

part1 = a[b == 1]


Now we want all places where part1 is equal to 1

part2 = part1[part1 == 1]


now we are left with all the places where a and b are equal to 1, so we can simply sum them up:

result = part2.sum()


Method 2: built in numpy.where

This is much shorted and probably faster to compute. NumPy has a nice function that returns the indices where your criteria are met in some arrays:

condition_1 = (a == 1)
condition_2 = (b == 1)


Now we can combine the operation by saying ""and"" - the binary operator version: &amp;.

part1 = numpy.where(condition_1 &amp; condition_2)


To get your desired output, we can take the length of the resulting set of indices:

result = len(part1)


Read the documentation about numpy.where to see the other things it can do for you!
",<python><numpy>
"If I have two arrays as shown below:

a = numpy.array([0, 0, 1, 0, 1, 1, 1, 0, 1])
b = numpy.array([1, 1, 1, 0, 0, 1, 1, 0, 0])


Is there an easy way using numpy to count the number of occurrences where elements at the same index in each of the two arrays have a value equal to one. In the above two arrays, the elements in position(zero-indexed) 2, 5 and 6 are equal to 1 in both the arrays. Thus I want to get a count of 3 here.

Thank you for any help that you may be able to provide.
","I like @RobinNicole's answer - in terms of Mathematics you are looking for a product of two boolean vectors.

Here are a few Numpy ways to do that:

In [37]: np.dot(a, b)
Out[37]: 3

In [38]: a @ b
Out[38]: 3


Here is another more generic solution which will work also for not-boolean vectors:

In [48]: ((a == 1) &amp; (b == 1)).sum()
Out[48]: 3

",<python><numpy>
"I'm getting an error while processing 0.2 million of text data. I'm using CNN text classification in tensorflow. 
Output raw data shape is 204177x22000. while passing to numpy.array(out_raw), here it is consuming 100% memory(Using 8GB RAM).
Tried with data in batch but didn't work.
If i need to increase my RAM size then kindly help me out with formula.

What are the methods to take care of this problem statement?
","Like @Sagar said, you could convert your pipeline to pyspark (So Spark with python API), and you can set your memory usage to not go above 1G of RAM for example and this will be faster because of the parallelization.

You can try to use generator with Tensorflow, that will fit back and forth your data so it never explode your RAM.

If the issue is before, like getting the data into a variable, you could use Dask. Which is simply a distributed Pandas that helps you fit your data in RAM by processing them by batch
",<machine-learning><tensorflow><pandas><cnn><numpy>
"I'm getting an error while processing 0.2 million of text data. I'm using CNN text classification in tensorflow. 
Output raw data shape is 204177x22000. while passing to numpy.array(out_raw), here it is consuming 100% memory(Using 8GB RAM).
Tried with data in batch but didn't work.
If i need to increase my RAM size then kindly help me out with formula.

What are the methods to take care of this problem statement?
","Are you running in jupyter notebook. There are several options:


Convert your data to feather or hd5 format 
You can opt amazon machine learning EC2 instances 
create an account in Paperspace.com for morecomputing power

",<machine-learning><tensorflow><pandas><cnn><numpy>
"I am not to figure out how to form a ndarray from existing numpy arrays.
Suppose I have three arrays - index, distA, and distB.
Now the I want to form a ndarray of these three arrays

What I have tried - 

&gt;&gt;&gt; indices = np.array([[5,7,4], [6,4,8]])
&gt;&gt;&gt; distances = np.array([[0.2, 0.3, 0.4], [0.3, 0.4, 0.5]])
&gt;&gt;&gt; np.column_stack((indices, distances))
array([[5. , 7. , 4. , 0.2, 0.3, 0.4],
   [6. , 4. , 8. , 0.3, 0.4, 0.5]])


But I want to form a table-like structure so that I can retrieve values in distA and distB.

5, 0.2, 0
7, 0.3, 0
4, 0.4, 0.4
6, 0, 0.3
8, 0, 0.5


Is it possible to do this with numpy?
","If your purpose is to deal with some data analysis from your arrays and have a nice view of it, try this using pandas:

import pandas as pd


df = pd.DataFrame({'distA' : np.array([0.2, 0.3, 0.4]),
                   'distB' : np.array([0.3, 0.4, 0.5])})


df




In the case you want to make computation with vectors, matrixes, tensors, etc .. NumPy is probably a good option. I didn't understand exactly what you've asked, though. Can you explain me what you're asking? 
",<pandas><numpy>
"I'd like to import the Rotten Tomatoes Movie Review dataset into a single data frame. How can I combine two datasets that are 1-column strings into a text, label shape?

Here's where I'm at so far (you can duplicate in Google Colab) :

import os
import pandas as pd

# Reset
!rm -rf ""rt-polarity.csv""

def fetch_rt_polarity_data():
# Fetch Data
if not os.path.isfile(""rt-polaritydata.tar.gz""):
    !wget -q http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz
    !tar -xzf rt-polaritydata.tar.gz
    !mv rt-polaritydata/rt-polarity.pos rt-polarity.pos
    !mv rt-polaritydata/rt-polarity.neg rt-polarity.neg
    !rm -rf rt-*

# Format Data
df_pos = pd.read_csv(""rt-polarity.pos"", encoding='latin-1', sep=""\t"", lineterminator=""\n"")
df_pos = df_pos.reset_index(drop=True)
df_pos.columns = ['text']
df_pos['label'] = 1

df_neg = pd.read_csv(""rt-polarity.neg"", encoding='latin-1', sep=""\t"", lineterminator=""\n"")
df_neg = df_neg.reset_index(drop=True)
df_neg.columns = ['text']
df_neg['label'] = 0

# Combine
df = pd.concat([df_pos, df_neg], ignore_index=True)
df.head(10)

df.to_csv(""rt-polarity.csv"")

df = pd.read_csv(""rt-polarity.csv"", encoding='latin-1', sep=""\t"", lineterminator=""\n"")
return df

df = fetch_rt_polarity_data();
df.head(5)

","I would import the datasets in pandas separately, mold them as you please, and then you can use the pd.concat function. This will assume that the instances are aligned by the automatically assigned index in pandas. If there is more data in one list than the other, the missing values will be NaN.

df1 = pd.DataFrame(data=[1,2,3])
df2 = pd.DataFrame(data=['a','b','c','d'])
dfs = pd.concat([df1, df2], axis=1)


If you have an index to link the text to the labels, then you can use the pd.merge function.

df1 = pd.DataFrame(data=[1,2,3])
df2 = pd.DataFrame(data=['a','b','c','d'])
dfs = df1.merge(df2, on='index')

",<python><pandas><numpy><dataframe>
"I am new at keras and CNN and am working on building at CNN  for sequential analysis of movement in a image. What I am having issues with is the reshaping the data and the labels that go into the fitting and testing the data for the model.
So the original size/shape of the numpy file is (18, 50,50,16) which is saved in a text file from another program.  I know the text file is ok because I can read it in and display it correctly with the debug_potion method. So that looks good. There are 18 images in the folder for the data and otherData variable. I dont really know what the 16 is but the image size are 50*50. 

The issue is the reshaping of that data is the problem. Can anyone suggest how to reshape this data in a way that I can train it. I think I need to do onehot encoding but not quite sure how. Any help will be appreciated. Here is what I have so far.

def debug_potion(data):
    for i in range(0, 16):
        # get for each joint
        potion = Potion(data)
        plt.show(potion.display(joints=[i, i], channels=[0, 1, 2]))

# Build Model for sequential building
def create_model(input_shape):
    model = Sequential()
    model.add(Conv2D(32, kernel_size=(3, 3),
                 activation='relu',
                 input_shape=input_shape))
    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.25))
    model.add(Flatten())
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(num_classes, activation='softmax'))

    model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adadelta(),
              metrics=['accuracy'])
    return model


def read_in_data(file):
    with open(file, ""rb"") as f:
        return pickle.load(f)


batch_size = 1
num_classes = 2
epochs = 12

# input image dimensions
img_rows, img_cols = 50, 50
input_shape = (img_rows, img_cols, 16)

data = read_in_data(""heatmap.txt"")
otherData = read_in_data(""othermove.txt"")


data = np.array(data)
otherData = np.array(otherData)
print(""Data shape"", otherData.shape)


xtrain = []
ytrain = []
xtest  = []
ytest  = []

for x in range(0,len(data)):
    if x &lt; 15:
        xtrain.append(data)
        ytrain.append((""FWAC"", num_classes))
    else:
        xtest.append(data)
        ytest.append((""FWAC"", num_classes))

for x in range(0,len(otherData)):
    if x &lt; 15:
        xtrain.append(otherData)
        ytrain.append((""Other"", num_classes))
    else:
        xtest.append(otherData)
        ytest.append((""Other"", num_classes))

#Create x test and train arrays
xtrain = np.array(xtrain)
xtrain = xtrain.reshape(30,img_rows, img_cols,16)
ytrain = np.array(ytrain)

xtest = np.array(xtest)
xtest = xtest.reshape(108, img_rows, img_cols,16)
ytest = np.array(ytest)
print(xtrain.shape)
print(xtest.shape)
print(ytrain.shape)
print(ytest.shape)

# Build Model
model = create_model(input_shape)

model.fit(xtrain, ytrain,
          batch_size = batch_size,
          epochs = epochs,
          verbose=1,
          validation_data=(xtest, ytest))

score = model.evaluate(xtest,ytest, verbose=0)
print((""Test loss"", score[0]))
print(""Test Accuracy"", score[1])

","As far as labels are concerned, you can one-hot-encode by using (assuming ytrain is converted to numpy array) the below code. Image reshaping looks fine but if you are having issues with image reshaping then, you might be giving the first argument i.e., the number of images wrong. So try this

    xtrain = xtrain.reshape(xtrain.shape[0],img_rows,img_cols,16)

    ytrain = keras.utils.to_categorical(ytrain, num_classes)


Make sure you import to_categorical from keras.utils 
",<python><keras><cnn><numpy><reshape>
"I have a 256*256*3 numpy array ""SP"" out of an autoencoder decoder layer which I want to save and open as an .jpg image. I used something like the following python code snippets: 

img = Image.fromarray(SP, 'RGB')
img.save('my.jpg')
img.show()


However I have noticed the array ""img"" is 256*256 in dimension and the image is just a noise. What can be the right way to display the image? I have attached the array as a output.npy file: ---> https://ufile.io/410iu
","That is right. The size you see is the Frame Size of the image i.e. height and width. It does not refer to the dimensionality of a color image array. To see it try this

rgb = np.zeros((255, 255, 3), dtype=np.uint8)
img = Image.fromarray(rgb, 'RGB')
r = img.getchannel(""R"")
g = img.getchannel(""G"")
b = img.getchannel(""B"")
print(np.array(r.getdata()))
print(np.array(g.getdata()))
print(np.array(b.getdata()))


where output is

[  0   1   2 ... 252 253 254]
[55 55 55 ... 55 55 55]
[  1   0 255 ...   5   4   3]


So you have 3 dimensions (or colors). And the point about noise is in dtype=np.uint8. Convert your array to this and it will work.

You could also simply try 

from scipy.misc import imsave
imsave(""file_name.jpg"", SP)


It does the job.

Good Luck! 
",<python><keras><tensorflow><visualization><numpy>
"I was trying to print  the index  of each of the maximum probability of the probability
 Array, While Executing the following code I get the error

"" The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()""

for prob_ in prob:
        max_prob=max(prob_)
        index=clf.classes_[prob.index(max(prob_))]


where the prob is an array
","I had the same problem in my code:

c = max(
[np.dot(theta[j], X[i]) for j in range(theta.shape[0])]
) / temp_parameter


I ran debugger and figured out that theta[j], X[i] were not the same shape, so dot product gave an ndarray instead of scalar as a result. max() function was a bit confused about it, because it does not work with ndarrays.
So, the conclusion is:


if prob_ is an ndarray, please, use numpy.max(), it should definitely work:

[In]: f = np.array([[1, 1, 1], [2, 2, 2]])
[In]: max(f)
[Out]: Traceback (most recent call last):
  File ""&lt;input&gt;"", line 1, in &lt;module&gt;
ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()
[In]: np.max(f)
[Out]: 2
if prob_ is a list, check whether its components are scalars or lists:

[In]: f = [[1, 1, 1], [2, 2, 2]]
[In]: max(f)
[Out]: [2, 2, 2]
[In]: g = [numpy.array([1, 1, 1]), numpy.array([2, 2, 2])]
[In]: max(g)
[Out]: Traceback (most recent call last):
  File ""&lt;input&gt;"", line 1, in &lt;module&gt;
ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()


",<machine-learning><numpy>
"I am a newbie in Machine learning and I am writing a small code for Perceptron. This is the first time I am writing code in Python. 
I have four training data points (X). As they are used for supervised learning so, each data point has its corresponding correct output pair (D). I have implemented SGD and used generalized Delta rule (wij ← wij + α δixj). I have trained my perceptron 10,000 times (epochs= 10,000).
Although everything looks fine to me, I don't get the right results when I test it with test values. I need some suggestions so that I can improve my results on test data. P.S. How can I improve this code?

Code

import numpy as np 

def sigmoid(x):
return 1 / (1 + np.exp(-x))    

def Delta_SGD(W, X, D):
 N = 4
 for x in range(N):

    v1 = np.dot(X[x][0], W[0])
    v2 = np.dot(X[x][1], W[1])
    v3 = np.dot(X[x][2], W[2])
    #weighted sum
    V = v1+v2+v3

    #output of neuron
    y = sigmoid(V)

    #error 
    e = D[x] - y

    #derivative of sigmoid(y)
    delta = (y)*(1-y)*e

    #Delta rule
    DW = alpha*delta*X[x]

    #updated weights
    W[0] = W[0] + DW[0]
    W[1] = W[1] + DW[1]
    W[2] = W[2] + DW[2]

return W

#input data points
X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])

#Correct output pairs
D = np.array([[0,0,1,1]]).T

#learning rate
alpha = 0.9

#random weights
W =  2*np.random.random((3,1)) - 1

#10000 epochs
for epoch in range(10000):
 W = Delta_SGD(W, X, D)
 print(epoch)

#Final weights after all epochs
print(""Final weights are \n"", W)

#testing network
N = 4
for x in range(N):

 v1 = np.dot(X[x][0], W[0])
 v2 = np.dot(X[x][1], W[1])
 v3 = np.dot(X[x][2], W[2])

 V = v1+v2+v3
 y = sigmoid(V)
 print(""output of neuron is \n "", y)

","Two points about the whole thing


You did not test yet. The point behind the training process is to make machine able to learn from the data conditioned on the ability of generalizing this to predicting samples which it has not seen before. Otherwise, the good training is actually overfitting. So here you trained on X and you need to create new samples and check the result to really call it testing (this is an introductory explanation).
Machine Learning is about features a lot! Playing with features and cleaning, modifying and filtering them is a key point. In your example, the last dimension of your 3d data is always 1. Does it distinguish anything? (in my course you get a complete explanation of this in the Lecture 2). So that feature (dimenstion, element of the vector) can/should be removed. To better understanding, imagine your 3d spread of the data. The z axis is always 1 which means the topology of points is what you see in x-y plain. So use only that one.

",<python><numpy><perceptron>
"I'm trying to do a majority voting of the predictions of two deep learning models.The shape of both y_pred and vgg16_y_pred are (200,1) and type 'int64'.

max_voting_pred = np.array([])
for i in range(0,len(X_test)):
    max_voting_pred = np.append(max_voting_pred, statistics.mode([y_pred[i], vgg16_y_pred[i]])) 


I run into the following error:

TypeError: unhashable type: 'numpy.ndarray'


How should I pass the data?
","The problem is that you're passing a list of numpy arrays to the mode function.
It requires either a single list of values, or a single numpy array with values (basically any single container will do, but seemingly not a list of arrays).

This is because it must make a hash map of some kind in order to determine the most common occurences, hence the mode.  It is unable to hash a list of arrays.

One solution would be to simple index the value out of each array (which then means mode gets a list of integers). Just changing the main line to:

max_voting_pred = np.append(max_voting_pred, mode([a[i][0], b[i][0]]))


Let me know if that doesn't fix things.



If you want something that is perhaps easier than fixing your orignal code, try using the mode function from the scipy module: scipy.stats.mode.

This version allows you to pass the whole array and simply specify an axis along which to compute the mode. Given you have the full vectors of predictions from both models:


Combine both arrays to be the two columns of one single (200, 2) matrix

results = np.concatenate((y_pred, vgg16_y_pred), axis=1)



Now you can perform the mode on that matrix across the single rows, but all in one single operation (no need for a loop):

max_votes = scipy.stats.mode(results, axis=1)


The results contain two things.


the mode values for each row
the counts of that mode within that row.


So to get the results you want (that would match your original max_voters_pred, you must take the first element from max_votes:

max_voters_pred = max_votes[0]

",<python><scikit-learn><numpy>
"I've a categorical column with values such as right('r'), left('l') and straight('s'). I expect these to have a continuum periods in the data and want to impute nans with the most plausible value in the neighborhood. In the beginning of the input signal you can see nans embedded in an otherwise continuum 's' episode. My definition as to what characterizes an episode is occurrence of the corresponding symbol at least 5 times in a row. Also, in my interest to be given more weight to 'r' and 'l' when tied with 's'.

iput = ['s','s','s','s','s','s',np.nan,'s',np.nan,'s','s','s','s','r',np.nan,np.nan,'r','r','r','r','s','s','s','s','s',np.nan,np.nan,'s','s','s','l','l','l','l','l',np.nan,'l','l','l']
oput = ['s','s','s','s','s','s','s','s','s','s','s','s','s','r','r','r','r','r','r','r','s','s','s','s','s','s','s','s','s','s','l','l','l','l','l','l','l','l','l']


I tried knn as following but it is rather suitable for numerical column and also imputing nans with zeros.
I was hoping for some ideas how to tackle this problem.
from fancyimpute import KNN
knnimpute = KNN(k=5)

&gt;&gt;&gt;x = np.array([0,np.nan,1,1,1,np.nan,2,2,2,2,np.nan,2,3,3,3,3,np.nan,1,1,2,2,np.nan,1,3,3,3,3])
&gt;&gt;&gt;x2 = knnimpute.fit_transform(x.reshape(-1,1))
&gt;&gt;&gt;x2
&gt;&gt;&gt; 
array([[0.],
           [0.],
           [1.],
           [1.],
           [1.],
           [0.],
           [2.],
           [2.],
           [2.],
           [2.],
           [0.],
           [2.],
           [3.],
           [3.],
           [3.],
           [3.],
           [0.],
           [1.],
           [1.],
           [2.],
           [2.],
           [0.],
           [1.],
           [3.],
           [3.],
           [3.],
           [3.]])

","The following script will give the value of the most frequent item to the nan value. It is a list of 7 items, since it checks the three samples before the nan, the nan itself and the three after the nan samples.

iput = ['s','s','s','s','s','s',np.nan,'s',np.nan,'s','s','s','s','r',np.nan,np.nan,'r','r','r','r','s','s','s','s','s',np.nan,np.nan,'s','s','s','l','l','l','l','l',np.nan,'l','l','l']
for i in range(len(iput)):
    if type(iput[i]) is float:
        iput[i]=max(iput[i-3:i+3],key=iput[i-3:i+3].count)

",<python><pandas><preprocessing><numpy><data-imputation>
"I have a large dataset

I want to transform this dataset into this format

I have try it through transpose but i couldn't figure out
","Use pandas melt function.

##init dataframe
df = pd.DataFrame({'item': ['a', 'a', 'a', 'b', 'b', 'b'],
             'class_a': [1, 1, 2, 3, 3, 1],
              class_b': [2, 1, 2, 3, 3, 1],
             'class_c': [1, 2, 2, 3, 1, 3]})
##shape it into desired format
pd.melt(df, id_vars='item', value_vars=['class_a', 'class_b', 'class_s'])

",<python><pandas><numpy>
"I have a large dataset

I want to transform this dataset into this format

I have try it through transpose but i couldn't figure out
","I see two ways:

%%timeit
import pandas as pd
import numpy as np

df = pd.DataFrame({'item': ['a', 'b', 'c', 'd', 'e', 'f'],
             'class_a': [1, 1, 2, 3, 3, 1],
             'class_b': [2, 1, 2, 3, 3, 1],
             'class_c': [1, 2, 2, 3, 1, 3]})

df_1 = pd.melt(df, id_vars='item', value_vars=['class_a']).drop('variable', axis=1).rename(columns={'value':'class_a'})
df_2 = pd.melt(df, id_vars='item', value_vars=['class_b']).drop(['variable','item'], axis=1).rename(columns={'value':'class_b'})
df_3 = pd.melt(df, id_vars='item', value_vars=['class_c']).drop(['variable','item'], axis=1).rename(columns={'value':'class_c'})

df_finish = df_1.join(df_2.join(df_3))


timeit gave:

6.11 ms ± 310 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)


And second way:

%%timeit
import pandas as pd
import numpy as np

df = pd.DataFrame({'item': ['a', 'b', 'c', 'd', 'e', 'f'],
             'class_a': [1, 1, 2, 3, 3, 1],
             'class_b': [2, 1, 2, 3, 3, 1],
             'class_c': [1, 2, 2, 3, 1, 3]})
df = df.append(df.append(df))
df.sort_values('item', inplace=True)
df['Range'] = df.groupby((df.item != df.item.shift()).cumsum()).cumcount() + 1
table = pd.pivot_table(df, values=['class_a', 'class_b', 'class_c'], index=['item'], columns=['Range'], aggfunc=np.sum)
table = table['class_a'][[1]].join(table['class_b'][[2]]).join(table['class_c'][[3]])
table.rename(columns={1:'class_a', 2:'class_b', 3:'class_c'},inplace=True)


timeit gave next:

9.81 ms ± 520 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

",<python><pandas><numpy>
"With Numpy, what’s the best way to compute the inner product of a vector of size 10 with each row in a matrix of size (5, 10)?
","As I know, fastest way to do it is:

np.dot(matrix, vector)


It is a function that specifically written and optimized for this. I don't recommend to multiply one by one and adding by yourself, obviously it will take more time.
",<python><deep-learning><numpy>
"With Numpy, what’s the best way to compute the inner product of a vector of size 10 with each row in a matrix of size (5, 10)?
","Here are a few ways, using some dummy data:

In [1]: import numpy as np

In [2]: a = np.random.randint(0, 10, (10,))

In [3]: b = np.random.randint(0, 10, (5, 10))

In [4]: a
Out[4]: array([4, 1, 0, 6, 3, 3, 6, 6, 1, 8])

In [5]: b
Out[5]: 
array([[9, 0, 6, 1, 1, 1, 4, 7, 4, 7],
       [5, 8, 8, 3, 4, 8, 7, 3, 0, 4],
       [2, 2, 5, 3, 9, 6, 1, 5, 8, 3],
       [2, 0, 4, 3, 5, 3, 3, 4, 3, 3],
       [3, 3, 6, 4, 7, 5, 8, 6, 7, 3]])


Because of the dimensions you asked for, in order to compute inner products (a.k.a. scalar products and dot products), we need to transpose the matrix b so that the dimensions work out.
With a vector of length 10, numpy gives it shape (10,). So it seems 10 rows and no columns, however it is kind of ambiguous. Numpy will essentially do what it has to in order to make dimensions work. We could force it into a (10, 1) vector by using a.reshape((10, 1)), but it isn't necessary. The matrix has a defined second dimensions, so we have a shape (5, 10). In order to multiply these two shapes together, we need to make the same dimensions match in the middle. This means making (10,) * (10, 5). Performing the transpose on matrix reverses the dimensions to give us that (10, 5). Those inner 10s will then disappear and leave us with a (1, 5) vector.



That all being said, we can use any of the following to get equivalent answers:


The standard standard dot-product:

In [7]: a.dot(b.T)
Out[7]: array([174, 174, 141, 119, 190])

The convenient numpy notation:

In [6]: a @ b.T
Out[6]: array([174, 174, 141, 119, 190])

The efficient ""Einstein notation"", a subset of Ricci calculus (I leave the interested reader to search online for more information):

In [8]: np.einsum('i,ij-&gt;j', a, b.T)
Out[8]: array([174, 174, 141, 119, 190])

Here as in the comments from shadowstalker:

In [9]: np.array([np.dot(a, r) for r in b])
Out[9]: array([174, 174, 141, 119, 190])





If your matrices are of dimensions (100, 100) or smaller, then the @ method is probably the fastest and most elegant. However, once you start getting into matrices that make you wonder if you laptop will handle it (e.g. with shape (10000, 10000)) - then it is time to read the documentation and this blog about Einstein notation and the amazing einsum module within numpy!
",<python><deep-learning><numpy>
"With Numpy, what’s the best way to compute the inner product of a vector of size 10 with each row in a matrix of size (5, 10)?
","Ok, lets put this to a numpy test!

With Numpy, what’s the best way to compute the inner product of a vector of size 10 with each row in a matrix of size (5, 10)?


np.dot(vector1, matrix1)
np.array([np.dot(row, vector1) for row in matrix1])
np.matmul(vector1, matrix1)
np.dot(matrix1, vector1)
np.sum(matrix1 * vector1, axis=1)


Answer

In short,


  The best answer is 4 and this is because it is the computationally cheapest method that takes a single step to complete.


But if you want to know why, then keep reading…



To answer this, lets see what works and what not. Remember the question asks for “the best way to computate the inner product”, so there should be more than one ways that works, right?

Now lets actually create a vector and a matrix with numpy

np.random.seed(2)

vector1 = np.random.randint(10, size=(1,10))[0]
print('Vector 1\n', vector1)

matrix1 = np.random.randint(10, size=(5,10))
print('\nMatrix 1\n', matrix1)


We will get this output

Vector 1
 [8 8 6 2 8 7 2 1 5 4]

Matrix 1
 [[4 5 7 3 6 4 3 7 6 1]
  [3 5 8 4 6 3 9 2 0 4]
  [2 4 1 7 8 2 9 8 7 1]
  [6 8 5 9 9 9 3 0 0 2]
  [8 8 2 9 6 5 6 6 6 3]]


Awesome!

Now let’s play with each possible answer and see what fits and what not!

But before we do, let’s see how matrix multiplication works.

To multiply two matrices with [ROWS x COLUMNS] the columns of the first must match in size the rows of the second, so we need a scenario like the following.

[A,B] * [B,N]

The resulting matrix will have the dimensions of the outer dimensions of the two matrices
So from the example above, we are allowed to multiply since the inner dimensions of the two matrices match, and we will gain a new matrix with the outer dimensions of the two matrices.

Our resulting matrix will be [A,N] dimensions in this case.


  1. np.dot(vector1, matrix1)


OUTPUT: (EXCEPTION)
shapes (10,) and (5,10) not aligned: 10 (dim 0) != 5 (dim 0)


We have an **exception **here, so (1) cannot be correct.
We cannot have a dot product since [1,10] * [5,10] don’t match the inner requirements for matrix multiplication.


  2. np.array([np.dot(row, vector1) for row in matrix1])


OUTPUT: 
 [243 225 211 309 301]


Here it seems to work.

We have a “for” loop on our matrix for each row. So for 5 times, we will multiply each element of the matrix by the equivalent element of the vector and add them together.

On every run we will get a number, so after 5 times, we will have created a vector of size [1,5] with our product.


  3. np.matmul(vector1, matrix1)


OUTPUT: 
 matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 5 is different from 10)


We have an exception here, so (3) cannot be correct.

Here we try to make matrix multiplication at [1,10]*[5,10]. The inner dimensions don’t match and we have the same problem as with (1).


  4. np.dot(matrix1, vector1)


OUTPUT:
 [243 225 211 309 301]


Here it seems to work.

We multiply a [5,10]*[10,1] matrix! The resulting matrix is a [5,1] matrix and gives the same result as with the case of answer (2).

However, this answer is better than the answer (2) since it is computationally cheaper, since in answer 2 we make a for loop, while here we get the result with a single algebraic calculation.


  5. np.sum(matrix1 * vector1, axis=1)


OUTPUT:
 [243 225 211 309 301]


Here it seems to work.

We get again the same result as with (2) and (4).

But again we do multiple steps here.

First by doing $matrix1 * vector1$ we end up with this matrix

[[32 40 42  6 48 28  6  7 30  4]
 [24 40 48  8 48 21 18  2  0 16]
 [16 32  6 14 64 14 18  8 35  4]
 [48 64 30 18 72 63  6  0  0  8]
 [64 64 12 18 48 35 12  6 30 12]]


Then by summing at axis 1 (y axis), we add each row numbers to a single number (much like the solution with the for loop in $2$).

So what is the BEST SOLUTION?

You remember that the question doesn’t just ask which of the options produces some result, but which one is best to use.
So far answers $2$, $4$, and $5$ seemed to work.


  The best answer is 4 and this is because it is the computationally cheapest method that takes a single step to complete.


RUN IT FOR YOURSELF!

import numpy as np

print(""With Numpy, what’s the best way to compute the inner product of a vector of size 10 with each row in a matrix of size (5, 10)?"")

np.random.seed(2)

print('\nSamples:')

vector1 = np.random.randint(10, size=(1,10))[0]
print('\nVector 1\n', vector1)

matrix1 = np.random.randint(10, size=(5,10))
print('\nMatrix 1\n', matrix1)

print('\n\nPossible Answers:')

#========================


print('\nA) np.dot(vector1, matrix1)', end='')
try: print(' - WORKING \n', np.dot(vector1, matrix1))
except Exception as e:   print(' - EXCEPTION \n', e)


#========================  

print('\nB) np.array([np.dot(row, vector1) for row in matrix1])', end='')
try:
  print(' - WORKING \n', np.array([np.dot(row, vector1) for row in matrix1]))
except Exception as e: 
  print(' - EXCEPTION \n', e)

#========================  

print('\nC) np.matmul(vector1, matrix1)', end='')
try:
  print(' - WORKING \n', np.matmul(vector1, matrix1))
except Exception as e: 
  print(' - EXCEPTION \n', e)

#========================


print('\nD) np.dot(matrix1, vector1)', end='')
try:
  print(' - WORKING \n', np.dot(matrix1, vector1))
except Exception as e: 
  print(' - EXCEPTION \n', e)

#========================  

print('\nE) np.sum(matrix1 * vector1, axis=1)', end='')
try:
  print(' - WORKING \n', np.sum(matrix1 * vector1, axis=1))
except Exception as e: 
  print(' - EXCEPTION \n', e)


print('\n=======================================')

print(""As the question asks, we shouldn't just look for a working result, but for the best result, and it seems to be (D) as it gets the best result with a single math step, thus computationally more efficient "")

",<python><deep-learning><numpy>
"I've an array like this:

 array([[ 0,  1],
        [ 2,  3],
        [ 4,  5],
        [ 6,  7],
        [ 8,  9],
        [10, 11],
        [12, 13],
        [14, 15]])


I want to make normalize this array between -1 and 1. I'm currently using numpy as a library.
","Suppose you have an array arr. You can normalize it like this:

arr = arr - arr.mean()
arr = arr / arr.max()


You first subtract the mean to center it around $0$, then divide by the max to scale it to $[-1, 1]$.
",<python><numpy><matrix>
"I've an array like this:

 array([[ 0,  1],
        [ 2,  3],
        [ 4,  5],
        [ 6,  7],
        [ 8,  9],
        [10, 11],
        [12, 13],
        [14, 15]])


I want to make normalize this array between -1 and 1. I'm currently using numpy as a library.
","nazz's answer doesn't work in all cases and is not a standard way of doing the scaling you try to perform (there are an infinite number of possible ways to scale to [-1,1] ). I assume you want to scale each column separately: 

1) you should divide by the absolute maximum:

arr = arr - arr.mean(axis=0)
arr = arr / np.abs(arr).max(axis=0)


2) But if the maximum of one column is 0 (which happens when the column if full of zeros) you'll get an error (you can't divide by 0). 

arr = arr - arr.mean(axis=0)
safe_max = np.abs(arr).max(axis=0)
safe_max[safe_max==0] = 1
arr = arr / safe_max


Still, this is not the standard way to do this. You're trying to do some ""Feature Scaling"" see here

Then the formula is:

import numpy as np

def scale(X, x_min, x_max):
    nom = (X-X.min(axis=0))*(x_max-x_min)
    denom = X.max(axis=0) - X.min(axis=0)
    denom[denom==0] = 1
    return x_min + nom/denom 

X = np.array([
    [ 0,  1],
    [ 2,  3],
    [ 4,  5],
    [ 6,  7],
    [ 8,  9],
    [10, 11],
    [12, 13],
    [14, 15]
])
X_scaled = scale(X, -1, 1)
print(X_scaled)


Result:

[[-1.         -1.        ]
 [-0.71428571 -0.71428571]
 [-0.42857143 -0.42857143]
 [-0.14285714 -0.14285714]
 [ 0.14285714  0.14285714]
 [ 0.42857143  0.42857143]
 [ 0.71428571  0.71428571]
 [ 1.          1.        ]]


If you want to scale the entire matrix (not column wise), then remove the axis=0 and change the lines denom[denom==0] = 1 for denom = denom + (denom is 0).  
",<python><numpy><matrix>
"I've been working on neural network for a while and I built simple network from scratch with python but before using TensorFlow, I would like to have a complete understanding of it.

Here is my question :

Lets say you have 3 layers you have 3 weights to update :

1) --> the weight between the outputlayer and the hiddenLayer2

2) -->  the weight between the hiddenLayer2 and the hiddenLayer1

3) -->  the weight between the hiddenLayer1 and the inputLayer

For the 1) the calculation is quite simple we got :

weight_3 += LEARNING_RATE * ((2*(target - output)) * sigmoid'(output) * layer2)


For the 2) the calculation is more complicated  and we got :

weight_2 += LEARNING_RATE * ((2*(target - output)) * sigmoid'(output) * weight_3) * sigmoid'(hiddenLayer2)


I need help for the 3rd part, I tried to calculate and find on internet but not a lot of people uses 2 hidden layer when they work from scratch.  

I also tried to resolve the chain rule but its too long and I can't resolve.

Does someone know the formula to get the weight between the hiddenLayer1 and the inputLayer ?

Thank you so much in advance
","Here is a good example of how to implement forward and backward propagation, in numpy, but there should be similar functions in tf.
",<machine-learning><python><neural-network><deep-learning><numpy>
"I have a 3D matrix like this:

array([[[ 0,  1],
      [ 2,  3]],

      [[ 4,  5],
      [ 6,  7]],

      [[ 8,  9],
      [10, 11]],

      [[12, 13],
      [14, 15]]])


and would like to stack them in a grid format, ending up with:

 array([[ 0,  1],
        [ 2,  3],
        [ 4,  5],
        [ 6,  7],
        [ 8,  9],
        [10, 11],
        [12, 13],
        [14, 15]])


Currently, I'm using numpy as a library.
","I suggest you to visit this link
also for this case would work np.reshape((-1,2))
",<python><numpy>
"I have a 3D matrix like this:

array([[[ 0,  1],
      [ 2,  3]],

      [[ 4,  5],
      [ 6,  7]],

      [[ 8,  9],
      [10, 11]],

      [[12, 13],
      [14, 15]]])


and would like to stack them in a grid format, ending up with:

 array([[ 0,  1],
        [ 2,  3],
        [ 4,  5],
        [ 6,  7],
        [ 8,  9],
        [10, 11],
        [12, 13],
        [14, 15]])


Currently, I'm using numpy as a library.
","The numpy.reshape() allows you to do reshaping in multiple ways. 

It usually unravels the array row by row and then reshapes to the way you want it. 

If you want it to unravel the array in column order you need to use the argument order='F'

Let's say the array is a.
For the case above, you have a (4, 2, 2) ndarray

numpy.reshape(a, (8, 2)) will work.

In the general case of a (l, m, n) ndarray:

numpy.reshape(a, (l*m, n)) should be used.

Or, numpy.reshape(a, (a.shape[0]*a.shape[1], a.shape[2])) should be used.

For more info numpy.reshape()
",<python><numpy>
"So there is a function in Dino_Name_Generator at Deeplearning.ai notebook

def sample(parameters, char_to_ix, seed):  
    # Retrieve parameters and relevant shapes from ""parameters"" dictionary
    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'],parameters['Wya'], parameters['by'], parameters['b']
    vocab_size = by.shape[0]
    n_a = Waa.shape[1]

    ### START CODE HERE ###
    # Step 1: Create the one-hot vector x for the first character (initializing the sequence generation). (≈1 line)
    x = np.zeros((vocab_size, 1))

    # Step 1': Initialize a_prev as zeros (≈1 line)
    a_prev = np.zeros((n_a, 1))

    # Create an empty list of indices, this is the list which will contain the list of indices of the characters to generate (≈1 line)
    indices = []

    # Idx is a flag to detect a newline character, we initialize it to -1
    idx = -1 

    # Loop over time-steps t. At each time-step, sample a character from a probability distribution and append 
    # its index to ""indices"". We'll stop if we reach 50 characters (which should be very unlikely with a well 
    # trained model), which helps debugging and prevents entering an infinite loop. 
    counter = 0
    newline_character = char_to_ix['\n']

    while (idx != newline_character and counter != 50):

        # Step 2: Forward propagate x using the equations (1), (2) and (3)
        a = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b)
        z = np.dot(Wya, a) + by
        y = softmax(z)

        # for grading purposes
        np.random.seed(counter+seed) 

        # Step 3: Sample the index of a character within the vocabulary from the probability distribution y
        idx = np.random.choice(vocab_size, size=None, p = y.ravel())

        # Append the index to ""indices""
        indices.append(idx)

        # Step 4: Overwrite the input character as the one corresponding to the sampled index.
        x = np.zeros((vocab_size, 1))
        x[[idx]] = 1

        # Update ""a_prev"" to be ""a""
        a_prev = a

        # for grading purposes
        seed += 1
        counter +=1


    ### END CODE HERE ###

    if (counter == 50):
        indices.append(char_to_ix['\n'])

    return indices


Can someone please help and explain what benefit of returned indices over normal char_to_integer indices? 

I want to understand the text processing in the link carried out before feeding into the network.
","From the link you provided:


  Sample a sequence of characters according to a sequence of probability
  distributions output of the RNN
  
  Arguments:
      parameters -- python dictionary containing the parameters Waa, Wax, Wya, by, and b. 
      char_to_ix -- python dictionary mapping each character to an index.
  
  Returns:
      indices -- a list of length n containing the indices of the sampled characters


You are returning the indices from a dictionary you gave as argument. Why should you use char_to_integer indices.
",<python><data-cleaning><probability><numpy><text-generation>
"LeNet accepts 32X32 image. So, to use LeNet for MNIST dataset,we have to change the size from 28X28 to 32X32. I came across This implementation. i am confused about how the following line of code work.

np.pad(X_train, ((0,0),(2,2),(2,2),(0,0)), 'constant')


Above line of code pad 28X28 pixed image to become a 32X32 image. Can anyone help me understand how exactly its done.
","Basically, it does exactly what you specify. The used numpy function appends values in each dimension. The amount of ""pads"" on each axis is specified by ((0,0),(2,2),(2,2),(0,0)), given the dimension of your dataset, which is:

10000 (samples) x 28 (image dimension 1) x 28 (image dim. 2) x 1 (grayscale value!)


Now let's see what your specification means in that regard: 

(0,0)Pad 0(as in the amount) values before and after each row
(2,2)Pad 2 before and 2 after each value of dim. 1 of your image data: 28 values -&gt; 32
(2,2)Pad 2 before and 2 after each value of dim. 2 of your image data: 28 values -&gt; 32
(0,0)Pad, again, nothing in the grayscale value dimension


That means you will end up with a 32x32 image in the respective dimension. Now, the only thing that's left is: Which values do we pad? The answer is quite simple, you do not specify any constant_values, meaning it will pad with the default constant_values (which is specified on the above linked page). Namely this value is 0. 

To sum it up, simply imagine you have a 32x32 image, your 28x28 is in the middle, and on the outside you have a 2-value-thick border of 0's. 
",<deep-learning><numpy><mnist>
"I am working on lung CT images from luna16 dataset, the dataset have a 3d lung image and a label from CSV file, I have a code for constructing 2d list from 3d array 25x25x25 (the 3d image) and a label [0,1] or [1,0] from CSV file, after creating the 2d list I want to save it in numpy file, below is my code for creating the 2d list and saving it in numpy file:

def getIDlist(csv_Dist,Data_Dist):
    # receive marked coords and ID in annotations.csv, and return the distination with coords.
    print('loading')
    data = np.loadtxt(csv_Dist, delimiter = ',', dtype = 'str')
    # delete the header file via 1:0, and receive the ID, x, y, z, r via 0:5 to a list.
    ID_coords = data[1:,0:5][0:10000] # get list of 'seriesuid' 'coordX' 'coordY' 'coordZ' 'class' (without header).
    # define the output file.
    ID_dist = []

    print('strat finding')
    process_bar = ShowProcess(len(ID_coords))

    for ID,x,y,z,label in ID_coords: 
        ID = ID +'.mhd' 
        found = 0       
        for parent, dirnames, filenames in os.walk(Data_Dist):
            for filename in filenames:# loop inside all files                 
                if ID == filename: # ID + .mhd in csv equal to filename in files
                    process_bar.show_process()
                    ID = parent + '\\' + ID# ID gets full path of the founded file
                    ID_dist.append([ID,x,y,z,label])# ID_dist gets info of founded files
                    found = 1
                    #print(""found: "", found)
                    break
            if found == 1:
                break
        if found == 1:
            continue

    process_bar.close()                 
    return ID_dist 

def get3Dmatrix(ID_dist):

    print('preparing the 3d matrix')
    matrixlist = []
    for Dist, xcoords, ycoords, zcoords, label in tqdm(ID_dist):
        # read the image
        imagearray,origin,spacing = load_itk_image(Dist)
        # resample in to 1mm*1mm*1mm
        imagearray = resample(imagearray,spacing,(1,1,1))

        # transfer world coordinates to voxel-coordinates, divide new spacing 1mm
        z = int(round((float(zcoords)-float(origin[0]))/1))
        y = int(round((float(ycoords)-float(origin[1]))/1))
        x = int(round((float(xcoords)-float(origin[2]))/1))

        # get the 3D array with shape 25*25*25           
        imagearray = imagearray[z-13:z+12,y-13:y+12,x-13:x+12]

        #converting the label number into a one-hot-encoding
        if int(label) == 1: 
            label=np.array([0,1])
        elif int(label) == 0: 
            label=np.array([1,0])

        # put it into output file
        matrixlist.append([imagearray,label])# 2d list consist of 3d array + label of all cases.
    return matrixlist 

 def main():
    start_time = time.time()
    # get ID_list from the csv and data dist.
    ID_list = getIDlist(candidates_V2_Dist, Data_Dist)# nested list - get file name with dist + x,y,z,class
    # Data_set[i][0] is the 3D array, Data_set[i][1] is the label
    Data_set = get3Dmatrix(ID_list) # 2d list consist of 3d array + label of all cases.
    print(""Begin saving in numpy file"")
    np.save(output_path+'np_ds(10000)-25-25-25(zyx)_one_hot.npy', Data_set)
    print(""%s time takes in seconds"" % (time.time() - start_time))

if __name__ == ""__main__"":
    main()


my problem is:

1- with approximatly 550 samples, the RAM gets fulled and I get memory error, I am working on dell inspiron core i7 with 16 gb ram laptop.

2- it takes 34 seconds for creating each sample, and I see this is huge amount of time for only one sample.

I did a lot of search in google and asked a question in some other forums but didn't get any answer, can anyone help me, please? realy I am confused with that error.
image below is the error message:

","I would recommend breaking the problem down a little bit to reduce the memory usage at any one time.

The first part of your main function gets the IDs using getIDList. That seems fine, so leave it there.

I would then break that list down into smaller chunks, calling get3Dmatrix on each chunk in turn. Altering your code, it might look something like this:

# Get number of entries in ID list
N = len(ID_list)

# break it down into a number of chunks e.g. 4, based on your progress bar
import numpy as np    # should already be imported

N = len(ID_list)
num_chunks = 4           # you can play with this number, making it larger until you don't get emmory errors
chunks = np.linspace(0, N, num_chunks)

for i in range(len(chunks) - 1):
    this_sublist = ID_list[chunks[i] : chunks[i + 1]]
    sub_data_set = get3Dmatrix(this_sublist)

    # At this point, either save this sub_data_set, or try appending it to another list toi make one final numpy matrix at the end before saving

...

print(""Begin saving in numpy file"")
np.save(output_path+'np_ds(10000)-25-25-25(zyx)_one_hot.npy', Data_set)
print(""%s time takes in seconds"" % (time.time() - start_time))


Even from the traceback you added, it is hard to say where exactly in your code that is happening.

Roughly looking at the dimensions you mention, it also doesn't seem plausible the a 16Gb machine is running out of memory - so I must not completely understand just how many images/patches are being saved.
",<python><numpy>
"I'm running the Neural Network example written in in BogoToBogo

The program worked fine:

(array([0, 0]), array([  2.55772644e-08]))
(array([0, 1]), array([ 0.99649732]))
(array([1, 0]), array([ 0.99677086]))
(array([1, 1]), array([-0.00028738]))


The neural network learned XOR, using tanh as activation function by default. However, after I changed the activation function to ""sigmoid""

nn = NeuralNetwork([2,2,1], 'sigmoid')


Now the program outputs:

epochs: 0
...
epochs: 90000
(array([0, 0]), array([ 0.45784467]))
(array([0, 1]), array([ 0.48245772]))
(array([1, 0]), array([ 0.47365194]))
(array([1, 1]), array([ 0.48966856]))


The output for the 4 inputs are all near 0.5. The result shows that neural network (with the sigmoid function) didn't learn XOR.

I was expecting the program would output:


~0 for (0, 0) and (1, 1)
~1 for (0, 1) and (1, 0)


Can somebody explain why this example with sigmoid doesn't work with XOR?
","I found the answer by myself. The reason of the difference is that the definition of prime of tanh in BogoToBogo (tanh_prime) takes arguments that's already applied with activation function:

def tanh_prime(x):
    return 1.0 - x**2


while sigmoid_prime is not. It calls sigmoid in it:

def sigmoid_prime(x):
    return sigmoid(x)*(1.0-sigmoid(x))


So the definition of sigmoid_prime seems more accurate than tanh_prime. Then why not sigmoid is working? It's because their parameters are already applied with the activation function.

Background

The derivatives of sigmoid ($\sigma$) and tanh share the same attribute in which these derivatives can be expressed in terms of sigmoid and tanh functions themselves.

$$
\frac{d\tanh (x)}{d(x)} = 1 - \tanh (x)^2
$$
$$
\frac{d\sigma (x)}{d(x)} = \sigma(x) (1 - \sigma(x))
$$

When performing backpropagation to adjust their weights, neural networks apply the derivative ($g^{'}$) to the values that's before applied with activation function. In BogoToBogo's explanation, that's variable $ z^{(2)} $ in

$$
\delta^{(2)} = (\Theta^{(2)})^T \delta^{(3)} \cdot g^{'}(z^{(2)}).
$$

In its source code, the variable dot_value holds such values. The Python implementation, however, calls the derivative with the vector stored in variable a. The vector is after applied with activation function. Why?

I interpret this as optimization to leverage the fact that derivatives of sigmoid and tanh use their parameters only to apply the original function. As the neural network already holds the value after activation function (as a), it can skip unnecessary calculation of calling sigmoid or tanh when calculating the derivatives. That's why the definition of tanh_prime in BogoToBogo does NOT call original tanh within it. However, the definition of sigmoid_prime, on the other hand, calls sigmoid function unexpectedly, resulting in miscalculation of derivative function.

Solution

Once I define sigmoid_prime in such a way that it assumes the parameter is already applied with sigmoid, then it works fine.

def sigmoid_prime(x):
    return x*(1.0-x)


Then calling the implementation with 

nn = NeuralNetwork([2,2,1], 'sigmoid', 500000)


successfully outputs:

(array([0, 0]), array([ 0.00597638]))
(array([0, 1]), array([ 0.99216467]))
(array([1, 0]), array([ 0.99332048]))
(array([1, 1]), array([ 0.00717885]))

",<python><neural-network><numpy><activation-function>
"Suppose I have 3 csv files which forms the dataset for training a machine learning model in Keras.

file1.csv

Name, X1,       X2,         X3
Joe,  1.16,                 1.00,                   1.11
Joe,  1.19,                 1.11,                   1.17
Joe,  1.17,                 1.13,                   1.16


file2.csv

Name, X1,       X2,         X3
Jack,   1.81,               1.23,                   1.15
Jack,   1.34,               1.53,                   1.87
Jack,   1.35,               1.64,                   1.75


file3.csv

Name, X1,       X2,         X3
Bo,     1.42,               1.64,                   1.43
Bo,     1.35,               1.53,                   1.32
Bo,     1.46,               1.64,                   1.53


Based on the data, I will classify whether the person has good or bad performance. For the data above, Joe has good performance while the rest have bad performance.

In keras, the above dataset will be transformed into numpy ndarrays X_train and Y_train to be fed into model.fit() like below;

model.fit(X_train, 
          Y_train, 
          nb_epoch=5, 
          batch_size = 128, 
          verbose=1, 
          validation_split=0.1)


I am confused over how X_train and Y_train should look like. What should be the shape of X_train and Y_train?

Suppose I have the following dataframes read from the csv files.

df1 = pd.read_csv(file1.csv)
df2 = pd.read_csv(file2.csv)
df3 = pd.read_csv(file3.csv)


How should I use these dataframes to get X_train and Y_train?

I am using python v3, keras with tensorflow.
","There are two steps:

The CSVs need to be merged and munged into tidy data form. Pandas and DataFrames are the most common choice for those operations.

The resulting dataframe needs to be converted to a NumPy array. The text needs to be encoded as numerical values. One option is one-hot encoding.


",<machine-learning><classification><keras><preprocessing><numpy>
"Note: The question is not about validating/testing a trained model.

Say i have an unlabeled features set, I want to approximate the true labels (for the sake of argument lets assume it's a binary classification prob).

I also have a trained model to predict the labels.
Now I want to use this model (since it is trained on the true labels, it should approx. my distribution)  along with some added noise (maybe gaussian) to generate the labels for this unknown dataset.
This should always result in a fixed effect size.

How can I do that? My features are mix of categorical and continuous values.   

Would be awesome to illustrate the additive noise via python numpy package.

Apologies, I am a rookie!
","Adding noise to do pertubation of the data, to check the collinearity and multicollinearity in data to check whether we can use weight in Logistic Regression or not

dimesions = data.shape #to get the dimesion of the data
noise = np.random.rand(dimesion)
noisy_data = data + noise # to add noise the existing data 


you can also use np.random.(A or B)
A=Normal
B=Uniform
",<python><statistics><machine-learning-model><numpy>
"Note: The question is not about validating/testing a trained model.

Say i have an unlabeled features set, I want to approximate the true labels (for the sake of argument lets assume it's a binary classification prob).

I also have a trained model to predict the labels.
Now I want to use this model (since it is trained on the true labels, it should approx. my distribution)  along with some added noise (maybe gaussian) to generate the labels for this unknown dataset.
This should always result in a fixed effect size.

How can I do that? My features are mix of categorical and continuous values.   

Would be awesome to illustrate the additive noise via python numpy package.

Apologies, I am a rookie!
","Based on the comments and responses, it is unclear if noise is to be added to the features (some of which are categorical) or to the output preds.

In case it is the former, I would strongly advice against pertubating the features directly, instead pertubate/add noise to the intermediate embeddings generated. There are libraries which do so.
If it is generating distributions around the preds, you are looking at a PMMI models. Basically the embeddings (from a deep layer) are fed to the probabilistic model which then uses some priors to return the final preds.

",<python><statistics><machine-learning-model><numpy>
"Note: The question is not about validating/testing a trained model.

Say i have an unlabeled features set, I want to approximate the true labels (for the sake of argument lets assume it's a binary classification prob).

I also have a trained model to predict the labels.
Now I want to use this model (since it is trained on the true labels, it should approx. my distribution)  along with some added noise (maybe gaussian) to generate the labels for this unknown dataset.
This should always result in a fixed effect size.

How can I do that? My features are mix of categorical and continuous values.   

Would be awesome to illustrate the additive noise via python numpy package.

Apologies, I am a rookie!
","I'm not sure why/where you want to apply the noise, but if you want to add some Gaussian noise to a variable, you can do this:

import numpy as np

target_dims = your_target.shape
noise = np.random.rand(target_dims)
noisy_target = your_target + noise


Now use the noisy_target as input to your model.
",<python><statistics><machine-learning-model><numpy>
"So I'm working on linear regression. So far I've managed to plot in linear regression, but currently I'm on Multiple Linear Regression and I couldn't manage to plot it, I can get some results if I enter the values manually, but I couldn't manage to plot it. Below is my code block and dataset and error, what can i change to plot it?

Dataset: 

deneyim maas    yas
0.5 2500    22
0   2250    21
1   2750    23
5   8000    25
8   9000    28
4   6900    23
15  20000   35
7   8500    29
3   6000    22
2   3500    23
12  15000   32
10  13000   30
14  18000   34
6   7500    27


Code block:

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.linear_model import LinearRegression

dataset = pd.read_csv(""multiple-linear-regression-dataset.csv"",sep = "";"")

x = dataset.iloc[:,[0,2]].values
y = dataset.maas.values.reshape(-1,1)

multiple_lr = LinearRegression()
multiple_lr.fit(x,y)

b0 = multiple_lr.intercept_
b1 = multiple_lr.coef_
b2 = b1

multiple_lr.predict(np.array([[10,35],[5,35]]))

array = np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]).reshape(-1,1)
y_head = multiple_lr.predict(array)

plt.scatter(x,y)
plt.plot(array, y_head, color = ""red"")
plt.show()


It says ValueError: shapes (16,1) and (2,1) not aligned: 1 (dim 1) != 2 (dim 0) when I try to compile it.
","You cannot plot graph for multiple regression like that. Multiple regression yields graph with many dimensions. The dimension of the graph increases as your features increases. In your case, X has two features. Scatter plot takes argument with only one feature in X and only one class in y.Try taking only one feature for X and plot a scatter plot. By doing so you will be able to study the effect of each feature on the dependent variable (which i think is more easy to comprehend than multidimensional plots).I think your issue should resolve.
",<machine-learning><python><numpy><matplotlib>
"So I'm working on linear regression. So far I've managed to plot in linear regression, but currently I'm on Multiple Linear Regression and I couldn't manage to plot it, I can get some results if I enter the values manually, but I couldn't manage to plot it. Below is my code block and dataset and error, what can i change to plot it?

Dataset: 

deneyim maas    yas
0.5 2500    22
0   2250    21
1   2750    23
5   8000    25
8   9000    28
4   6900    23
15  20000   35
7   8500    29
3   6000    22
2   3500    23
12  15000   32
10  13000   30
14  18000   34
6   7500    27


Code block:

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.linear_model import LinearRegression

dataset = pd.read_csv(""multiple-linear-regression-dataset.csv"",sep = "";"")

x = dataset.iloc[:,[0,2]].values
y = dataset.maas.values.reshape(-1,1)

multiple_lr = LinearRegression()
multiple_lr.fit(x,y)

b0 = multiple_lr.intercept_
b1 = multiple_lr.coef_
b2 = b1

multiple_lr.predict(np.array([[10,35],[5,35]]))

array = np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]).reshape(-1,1)
y_head = multiple_lr.predict(array)

plt.scatter(x,y)
plt.plot(array, y_head, color = ""red"")
plt.show()


It says ValueError: shapes (16,1) and (2,1) not aligned: 1 (dim 1) != 2 (dim 0) when I try to compile it.
","See my answer over here : Plotting multivariate linear regression

The catch is that you can't plot more than three variable at once, so you are left with : 


observing the interactions of the expected output with one to three variable, either by plotting the observed (or predicted) y against your variable or by using y as a color. This won't work for more than three variables. 
Plotting the contours of the output of the model. It has some limitations as you need to fix a value for variables that are not plotted. It shouldn't really work for more than two variables. 


So, generally speaking (quite independently of the model you want to use), you can only observe the interaction of y to only a few variables at once. You don't really get good visualisation with more than one variable. Plotting y against two variable is ok to see an interraction. Using three variables (and y as a color) is not really good ihmo, As you don't really see anything.
",<machine-learning><python><numpy><matplotlib>
"Using numpy isclose in the following example:

import numpy as np
np.isclose(1533761040,1533748023.0, atol=1)


Returns True, which is False. 
","No, it uses both an absolute and relative tolerance, and the default of rtol is nonzero, adding some more tolerance. See https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.isclose.html
",<python><numpy>
"Are view() in torch and reshape() in Numpy similar?

view() is applied on torch tensors to change their shape and reshape() is a numpy function to change shape of ndarrays.
","Yes, for most intents and purposes, they can do the same job. From this link, an example:

&gt;&gt;&gt; import torch
&gt;&gt;&gt; t = torch.ones((2, 3, 4))
&gt;&gt;&gt; t.size()
torch.Size([2, 3, 4])
&gt;&gt;&gt; t.view(-1, 12).size()
torch.Size([2, 12])




If you are concerned with memory allocation, here is another answer on StackOverflow with a little more information. PyTorch's view function actually does what the name suggests - returns a view to the data. The data is not altered in memory as far as I can see.

In numpy, the reshape function does not guarantee that a copy of the data is made or not. It will depend on the original shape of the array and the target shape. Have a look here for further information.
",<deep-learning><numpy><pytorch><reshape>
"I need to understand how the splitting of labels in the following code happened :

import keras
import librosa
import librosa.feature
import librosa.display
import glob
import numpy as np   
from keras.models import Sequential 
from keras.layers import Dense , Activation
from keras.utils.np_utils import to_categorical


def extract_features_song(f):
    y, _ = librosa.load(f)

    # get mfcc
    mfcc = librosa.feature.mfcc(y)

    # make value between 1 -1 
    mfcc /= np.amax(np.absolute(mfcc))
    return np.ndarray.flatten(mfcc)[:25000]

def extrac_features_and_labels():
    all_features = []
    all_labels = []

    genres = ['blues' , 'classical', 'country' , 'disco' , 'hiphop', 'jazz', 'metal' , 'pop', 'reggae', 'rock']
    for genre in genres:

        sound_files = glob.glob('genres/'+genre+'/*.au')


        print ('prcoessing %d songs in %s genre'% (len(sound_files), genre))

        for f in sound_files:
            features =extract_features_song(f)
            all_features.append(features)
            all_labels.append(genre)

    # one hot encoding
    label_uniq_ids , label_row_ids = np.unique(all_labels, return_inverse= True)
    label_row_ids = label_row_ids.astype(np.int32, copy= False)
    onehot_labels = to_categorical(label_row_ids, len(label_uniq_ids))

    return np.stack(all_features), onehot_labels 

features , labels = extrac_features_and_labels()



print (np.shape(features))
print (np.shape(labels))

training_split = 0.8

alldata = np.column_stack(features , labels)

np.random.shuffle(alldata)
splitidx = int(len(alldata))*training_split
train , test = alldata[:splitidx,:], alldata[splitidx:, :]

print (np.shape(train))
print (np.shape(test))


# the concerned part: begin
train_input = train [:,:-10]
train_labels = train [:,-10:]

test_input = test [:,:-10]
test_labels = test [:,-10:]
#the concerned part: end



print (np.shape(train_input))
print (np.shape(train_labels))


the output is as follows:

(1000, 25000)
(1000, 10)
(800, 25010)
(200, 25010)
(800, 25000)
(800, 10)


Now, when he - code instructor - stacked both arrays together and shuffled them, he can't be sure that the last ten elements are the labels, am I right? If so how did he do it like this, this would cause an error.
","Nope, the last 10 elements are definitely the labels. np.column_stack appends the elements of the second array to the corresponding row of the first array. 

A quick example (Python 3.5), random one-hot encoding snippet was copied from this answer on SO

&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; np.__version__
'1.15.0'
&gt;&gt;&gt; rand_enc = np.random.randint(0,3,size=(10))
&gt;&gt;&gt; labels = np.zeros((10,3))
&gt;&gt;&gt; labels[np.arange(10), rand_enc] = 1
&gt;&gt;&gt; labels
array([[0., 0., 1.],
       [0., 1., 0.],
       [1., 0., 0.],
       [1., 0., 0.],
       [1., 0., 0.],
       [1., 0., 0.],
       [0., 0., 1.],
       [0., 0., 1.],
       [0., 1., 0.],
       [1., 0., 0.]])
&gt;&gt;&gt; features = np.random.rand(10,4)
&gt;&gt;&gt; features
array([[0.51599703, 0.24758895, 0.70951355, 0.52074341],
       [0.64851074, 0.14481191, 0.88837244, 0.86901571],
       [0.36303914, 0.93464881, 0.21238598, 0.24998789],
       [0.74290604, 0.96704141, 0.16971294, 0.99628383],
       [0.20225753, 0.78450864, 0.42730556, 0.84727098],
       [0.95372573, 0.38092036, 0.73628447, 0.22120431],
       [0.10821134, 0.54467407, 0.00452555, 0.15295404],
       [0.7353348 , 0.26629375, 0.96241551, 0.84573258],
       [0.80659848, 0.34873381, 0.12219345, 0.46671669],
       [0.07837654, 0.82128673, 0.79523531, 0.10652154]])
&gt;&gt;&gt; np.column_stack((features, labels))
array([[0.51599703, 0.24758895, 0.70951355, 0.52074341, 0.        ,
        0.        , 1.        ],
       [0.64851074, 0.14481191, 0.88837244, 0.86901571, 0.        ,
        1.        , 0.        ],
       [0.36303914, 0.93464881, 0.21238598, 0.24998789, 1.        ,
        0.        , 0.        ],
       [0.74290604, 0.96704141, 0.16971294, 0.99628383, 1.        ,
        0.        , 0.        ],
       [0.20225753, 0.78450864, 0.42730556, 0.84727098, 1.        ,
        0.        , 0.        ],
       [0.95372573, 0.38092036, 0.73628447, 0.22120431, 1.        ,
        0.        , 0.        ],
       [0.10821134, 0.54467407, 0.00452555, 0.15295404, 0.        ,
        0.        , 1.        ],
       [0.7353348 , 0.26629375, 0.96241551, 0.84573258, 0.        ,
        0.        , 1.        ],
       [0.80659848, 0.34873381, 0.12219345, 0.46671669, 0.        ,
        1.        , 0.        ],
       [0.07837654, 0.82128673, 0.79523531, 0.10652154, 1.        ,
        0.        , 0.        ]])


As you can see, the labels get appended at the end! The shuffling code only manipulates the rows, so the column ordering is preserved. You do need to pass np.column_stack a tuple of your NumPy arrays, but otherwise this gets the job done.

Hope that clears things up!

Edit: I don't know how the instructor got a size mismatch between the train_input and train_labels, from what I understand from the code you shared they should be (800,25000) and (800,10)
",<neural-network><numpy>
"I'm new to Data Science, so hopefully this question makes sense. 

I have a dataset with ~50,000 rows. It consists of one column that has item category in it and one column with item description in it. 

I'm trying to get bigrams from the item description with the highest correlation to each category.

i'm using TfidfVectorizer to do this. 

I compress the item descriptions into an array using this: 

features = tfidf.fit_transform(df.ItemDescription).toarray()


In Spyder, when i even click on the features under variable explorer, the memory immediately shoots up to 99% and my computer freezes.

If I try to call features at all, it will also freeze. 

Here is how i'm calling features later in my code after making it into an array:

from sklearn.feature_selection import chi2
import numpy as np
N = 2
for UNSPSC, category_id in sorted(category_to_id.items()):
  features_chi2 = chi2(features, labels == category_id)


The array size is 50000X12000

Is that too big? Is there anything I can do to get the bigrams without my computer freezing? As a last resort, I will trim down my data. But i feel like that will reduce the accuracy of any model i try to train later. 

Please let me know if you need full code. 
","I have now resolved. 

I think the issue has to do with Spyder and not Python itself. 

I ran the script using the terminal directly and was able to process. Although it does take a large amount of memory, but it still allows me to function (sub-par) on my computer. 
",<python><feature-selection><numpy>
"I'm new to Data Science, so hopefully this question makes sense. 

I have a dataset with ~50,000 rows. It consists of one column that has item category in it and one column with item description in it. 

I'm trying to get bigrams from the item description with the highest correlation to each category.

i'm using TfidfVectorizer to do this. 

I compress the item descriptions into an array using this: 

features = tfidf.fit_transform(df.ItemDescription).toarray()


In Spyder, when i even click on the features under variable explorer, the memory immediately shoots up to 99% and my computer freezes.

If I try to call features at all, it will also freeze. 

Here is how i'm calling features later in my code after making it into an array:

from sklearn.feature_selection import chi2
import numpy as np
N = 2
for UNSPSC, category_id in sorted(category_to_id.items()):
  features_chi2 = chi2(features, labels == category_id)


The array size is 50000X12000

Is that too big? Is there anything I can do to get the bigrams without my computer freezing? As a last resort, I will trim down my data. But i feel like that will reduce the accuracy of any model i try to train later. 

Please let me know if you need full code. 
","
If your downstream algorithm can work with sparse matrices, you can proceed with sparse matrix rather than using toarray.
Alternatively, try to minimize the number of features which are created. Check documentation of TfidfVectorizer (parameters like min_df, max_df, stopwords) to filter out un-important words.

",<python><feature-selection><numpy>
"I have two columns of training data for a neural net which are missing values. (There are many other columns which aren't missing values.)

For example

Height  | Weight
180     | 70
175     | N/A
N/A     | N/A


I want to fill missing values and normalise the columns.

The data is heights and weights so I thought a good fill value would be 0 or -1. This is based on the book Deep Learning in Python:


  In general, with neural networks, it's safe to input missing values as 0, with the condition that 0 isn't already a meaningful value.


EDIT I assumed 0 wasn't meaningful in a dataset with values from 150-200

I was also recommended to normalise the data by subtracting the mean and dividing by the std for each column.

Both of those are fine on their own - I understand how  and why to do them. What I don't get is how to combine them. I can either ...


Fill missing values then normalise, but then a) my zeros will no longer be zeros (will my network still learn they are a special value?), and b) the zeros will affect the mean/std to a degree determine by how many values are missing. I suppose I'm concerned this would give a weird distribution
Normalise then fill missing values. But after I've normalised my data, 0 is now the mean of my column and so isn't a fill value of the same kind. I'd rather let the network know the values are unfilled than assume they all take the mean value


I'm using Keras, Numpy and Pandas with Dense layers for a multiclass classification problem.
","I don't understand why you would like to fill values with zeros ! This would basically mean, ""this guy, who is 170 cm tall, weights 0 kg"" and would fool your network. In my opinion, you have two options:


discard missing values (the entire row): you end up with less but more consistent training data
if you really need these rows, then fill missing values with some heuristics: for example, give them the mean of their column, or apply a simple linear regression. Beware that this will add a bias in the learning process, but it would be definitely better than giving random values. 


At very least, if you have a lot of missing values, then maybe you should think of selecting a specialized model for partial training data. You actually don't tell us what network you are using, but you might modify it to handle missing values more intelligently than by filling missing values. 

Finally, the need of normalizing once again depend on your model (which you don't describe). But this would definitely come after the processing part.   
",<keras><pandas><normalization><missing-data><numpy>
"I have two columns of training data for a neural net which are missing values. (There are many other columns which aren't missing values.)

For example

Height  | Weight
180     | 70
175     | N/A
N/A     | N/A


I want to fill missing values and normalise the columns.

The data is heights and weights so I thought a good fill value would be 0 or -1. This is based on the book Deep Learning in Python:


  In general, with neural networks, it's safe to input missing values as 0, with the condition that 0 isn't already a meaningful value.


EDIT I assumed 0 wasn't meaningful in a dataset with values from 150-200

I was also recommended to normalise the data by subtracting the mean and dividing by the std for each column.

Both of those are fine on their own - I understand how  and why to do them. What I don't get is how to combine them. I can either ...


Fill missing values then normalise, but then a) my zeros will no longer be zeros (will my network still learn they are a special value?), and b) the zeros will affect the mean/std to a degree determine by how many values are missing. I suppose I'm concerned this would give a weird distribution
Normalise then fill missing values. But after I've normalised my data, 0 is now the mean of my column and so isn't a fill value of the same kind. I'd rather let the network know the values are unfilled than assume they all take the mean value


I'm using Keras, Numpy and Pandas with Dense layers for a multiclass classification problem.
","Trial and error is an important part of Deep Learning. There are situations where missing data has meaning and there situations where missing data is just noise. As an example, when tracking facial features like eyes, nose, or ears, missing data is informing the neural network that the feature is outside of view.  Other times it is noise from bad data collection. 

I recommend fitting your deep learning model with the following data: 


Use 0 for missing data. 
Remove rows with missing data.
Use the mean for missing data.
Single variable feature imputation or Multivariate interpolation.
Use Multivariate feature imputation 

",<keras><pandas><normalization><missing-data><numpy>
"I want to create a dataset from three numpy matrices - train1 = (204,), train2 = (204,) and train3 = (204,). Basically all sets are of same length. I am applying a sliding window function on each of window 4. Each set become of shape =(201,4) I want a new array in which all these values are appended row wise. Like for first train1 then train2 then train3. And final output set is of size =(603,4). 

This is a sliding window function which converts array of shape (204,) to (201,4)

def moving_window(x, length, step=1):
    streams = it.tee(x, length) 
    return zip(*[it.islice(stream, i, None, step) for stream, i in zip(streams, it.count(step=step))]) 


Create dataset fucntion is:

def create_dataset(dataset1,dataset2):
    dataX=[]       
    x=list(moving_window(dataset1,4))
    x=np.asarray(x) 
    dataX.append(x)
    y=list(moving_window(dataset2,4)) 
    y=np.asarray(y) 
    dataX.append(y) 
    return np.array(dataX)

data_new=create_dataset(train1,train2)


It is returning a dataset of shape 0(2,201,4). I think this is appending differently, but I want row wise appending. so that the new _dataset is of shape= (402,4) with two sets and (603,4) with three sets. I want to generalize as well like if I want for 10 training sets or twenty training sets. How can I do that?
","I think its because of the way you are appending the datasets in list and then converting it to numpy array.

Solution 1

One quick solution is to reshape your array as -

data_new = data_new.reshape(data_new.shape[0]*data_new.shape[1], data_new.shape[2])

So, your data of shape (2,201,4) will become (2*201,4) = (402,4).

Solution 2

Another solution is to append the arrays in the function you have defined, instead of returning np.array(dataX), use -

return np.append(x, y, axis = 0)


So, you don't have to use dataX anywhere.
",<python><deep-learning><numpy>
"I am trying to copy some code from a video to do a decision tree program, which will predict if a student will pass or not depending on 30 parameters given. I did exactly as written but get an error as below:

import pandas as pd 
import numpy as np
from sklearn import tree 
import graphviz

#importing the data set
d = pd.read_csv('student-por.csv', sep= ';')

#Each 'G' grade is out of 20
#Setting the pass mark as 35 /60    
d['pass'] = d.apply(lambda row: 1 if (row['G1']+ row['G2']+ row ['G3']) &gt;= 35 else 0 , axis=1)
d = d.drop(['G1', 'G2','G3'], axis=1 )


#shuffle rows
d = d.sample(frac=1)

#split traning and test
d_train = d[:500]
d_test = d[500:]

# to be used in .fit
d_train_att = d_train.drop(['pass'], axis=1)
d_train_pass= d_train['pass']

#I don't know why he did this one
d_test_att = d_test.drop(['pass'], axis=1)
d_test_pass= d_test['pass']


d_att = d.drop(['pass'], axis=1)
d_pass = d['pass']

#Calculating how many students passed
print ('passing: %d out of %d (%.2f%%)'%(np.sum(d_pass), len(d_pass), 100*float(np.sum(d_pass)/len(d_pass))) )


t = tree.DecisionTreeClassifier(criterion ='entropy', max_depth = 5)
t= t.fit (d_train_att, d_train_pass)

# To visualize the decision tree
dot_data = tree.export_graphviz(t,out_file = None, label ='all', imputiry=False, proportion= True, feature_names=list(d_train_att), class_names=['fail', 'pass'], filled = True, rounded=True)
graph = graphviz.Source (dot_data)


And the output is:

Traceback (most recent call last):
  File ""students.py"", line 29, in &lt;module&gt;
    t= t.fit (d_train_att, d_train_pass)
  File ""/home/mohamed/.virtualenvs/cv/lib/python3.5/site-packages/sklearn/tree/tree.py"", line 790, in fit
    X_idx_sorted=X_idx_sorted)
  File ""/home/mohamed/.virtualenvs/cv/lib/python3.5/site-packages/sklearn/tree/tree.py"", line 116, in fit
    X = check_array(X, dtype=DTYPE, accept_sparse=""csc"")
  File ""/home/mohamed/.virtualenvs/cv/lib/python3.5/site-packages/sklearn/utils/validation.py"", line 433, in check_array
    array = np.array(array, dtype=dtype, order=order, copy=copy)
ValueError: could not convert string to float: 'no'


If possible, could you please explain to me how to fix it?

edit: solved it after the explanation of pcko1. I used pd.get_dummies to get rid of floats.
","I cannot see your data (included in student-por.csv) but I suspect that it includes strings (maybe for student names). You should either drop the string variables or convert them to categorical (one character for each different value of the variable).

This means that if you have a column with subject names, you should convert ""Maths"" to ""0"", ""History"" to ""1"", ""Biology"" to ""2"" and so on. A very convenient way of doing this is with the sklearn.preprocessing.LabelEncoder, please check this.

In the end you should either feed continuous or categorical values to your Decision Tree during fit, no strings. Hope it helps :)
",<machine-learning><scikit-learn><decision-trees><numpy>
"I am trying to copy some code from a video to do a decision tree program, which will predict if a student will pass or not depending on 30 parameters given. I did exactly as written but get an error as below:

import pandas as pd 
import numpy as np
from sklearn import tree 
import graphviz

#importing the data set
d = pd.read_csv('student-por.csv', sep= ';')

#Each 'G' grade is out of 20
#Setting the pass mark as 35 /60    
d['pass'] = d.apply(lambda row: 1 if (row['G1']+ row['G2']+ row ['G3']) &gt;= 35 else 0 , axis=1)
d = d.drop(['G1', 'G2','G3'], axis=1 )


#shuffle rows
d = d.sample(frac=1)

#split traning and test
d_train = d[:500]
d_test = d[500:]

# to be used in .fit
d_train_att = d_train.drop(['pass'], axis=1)
d_train_pass= d_train['pass']

#I don't know why he did this one
d_test_att = d_test.drop(['pass'], axis=1)
d_test_pass= d_test['pass']


d_att = d.drop(['pass'], axis=1)
d_pass = d['pass']

#Calculating how many students passed
print ('passing: %d out of %d (%.2f%%)'%(np.sum(d_pass), len(d_pass), 100*float(np.sum(d_pass)/len(d_pass))) )


t = tree.DecisionTreeClassifier(criterion ='entropy', max_depth = 5)
t= t.fit (d_train_att, d_train_pass)

# To visualize the decision tree
dot_data = tree.export_graphviz(t,out_file = None, label ='all', imputiry=False, proportion= True, feature_names=list(d_train_att), class_names=['fail', 'pass'], filled = True, rounded=True)
graph = graphviz.Source (dot_data)


And the output is:

Traceback (most recent call last):
  File ""students.py"", line 29, in &lt;module&gt;
    t= t.fit (d_train_att, d_train_pass)
  File ""/home/mohamed/.virtualenvs/cv/lib/python3.5/site-packages/sklearn/tree/tree.py"", line 790, in fit
    X_idx_sorted=X_idx_sorted)
  File ""/home/mohamed/.virtualenvs/cv/lib/python3.5/site-packages/sklearn/tree/tree.py"", line 116, in fit
    X = check_array(X, dtype=DTYPE, accept_sparse=""csc"")
  File ""/home/mohamed/.virtualenvs/cv/lib/python3.5/site-packages/sklearn/utils/validation.py"", line 433, in check_array
    array = np.array(array, dtype=dtype, order=order, copy=copy)
ValueError: could not convert string to float: 'no'


If possible, could you please explain to me how to fix it?

edit: solved it after the explanation of pcko1. I used pd.get_dummies to get rid of floats.
","The answer by pcko1 is useful in the sense that it will make the code run. But what you exactly require to do is one hot encoding. I say that because encoding the categorical variables that are nominal with increasing numbers like 1, 2, 3 etc. does not make sense. Have a look at this question. 

You need to encode all the categorical variables as 0-1. By that I mean, attach one column for each categorical variable in the data set, denoting its presence by a 1 and absence by a 0 in the respective rows of the data set. 


  #I don't know why he did this one


That has been done to have a separate data set for testing the model after it has been trained over the training set.

Just a reminder.


  I am trying to copy a code from a video to do a decision tree program, which will predict if a student will pass or not depending on 30 parameters given. 


The correct term should be variables not parameters.
",<machine-learning><scikit-learn><decision-trees><numpy>
"I have my dataset that has multiple features and based on that the dependent variable is defined to be 0 or 1.
I want to get a scatter plot such that all my positive examples are marked with 'o' and negative ones with 'x'.
I am using python and here is the code for the beginning.

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the dataset
dataset = pd.read_csv('/home/Dittu/Desktop/Project/creditcard.csv')


now I know how to make scatter plots for two different classes.

fig = plt.figure()
ax1 = fig.add_subplot(111)

ax1.scatter(x[:4], y[:4], s=10, c='b', marker=""s"", label='first')
ax1.scatter(x[40:],y[40:], s=10, c='r', marker=""o"", label='second')
plt.show()


but how to segregate both class of examples and the plot them or plot them with distinct marks without separating?
","Let's assume that the name of your dependent variable column is ""target"", and you have stored the data in ""dataset"" variable. You can segregate the dataset based on value of target in following way:

import numpy as np    
idx_1 = np.where(dataset.target == 1)
idx_0 = np.where(dataset.target == 0)


The above code with return indices of dataset with target values 0 and 1.

Now, to display the data, use:

plt.scatter(dataset.iloc[idx_1].x, dataset.iloc[idx_1].y, s=10, c='b', marker=""o"", label='first')
plt.scatter(dataset.iloc[idx_0].x, dataset.iloc[idx_0].y, s=10, c='r', marker=""o"", label='second')
plt.ylabel('y')
plt.xlabel('x')
plt.show()

",<python><pandas><plotting><numpy><matplotlib>
"I have my dataset that has multiple features and based on that the dependent variable is defined to be 0 or 1.
I want to get a scatter plot such that all my positive examples are marked with 'o' and negative ones with 'x'.
I am using python and here is the code for the beginning.

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the dataset
dataset = pd.read_csv('/home/Dittu/Desktop/Project/creditcard.csv')


now I know how to make scatter plots for two different classes.

fig = plt.figure()
ax1 = fig.add_subplot(111)

ax1.scatter(x[:4], y[:4], s=10, c='b', marker=""s"", label='first')
ax1.scatter(x[40:],y[40:], s=10, c='r', marker=""o"", label='second')
plt.show()


but how to segregate both class of examples and the plot them or plot them with distinct marks without separating?
","One approach is to plot the data as a scatter plot with a low alpha, so you can see the individual points as well as a rough measure of density.

from sklearn.datasets import load_iris
iris = load_iris()
features = iris.data.T

plt.scatter(features[0], features[1], alpha=0.2,
            s=100*features[3], c=iris.target, cmap='viridis')
plt.xlabel(iris.feature_names[0])
plt.ylabel(iris.feature_names[1]);




We can see that this scatter plot has given us the ability to simultaneously explore four different dimensions of the data: 


the (x, y) location of each point corresponds to the sepal length and width,
the size of the point is related to the petal width, and 
the color is related to the particular species of flower, i.e the Target Variable...


Multicolor and multifeature scatter plots like this can be useful for both exploration and presentation of data.
",<python><pandas><plotting><numpy><matplotlib>
"I have my dataset that has multiple features and based on that the dependent variable is defined to be 0 or 1.
I want to get a scatter plot such that all my positive examples are marked with 'o' and negative ones with 'x'.
I am using python and here is the code for the beginning.

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the dataset
dataset = pd.read_csv('/home/Dittu/Desktop/Project/creditcard.csv')


now I know how to make scatter plots for two different classes.

fig = plt.figure()
ax1 = fig.add_subplot(111)

ax1.scatter(x[:4], y[:4], s=10, c='b', marker=""s"", label='first')
ax1.scatter(x[40:],y[40:], s=10, c='r', marker=""o"", label='second')
plt.show()


but how to segregate both class of examples and the plot them or plot them with distinct marks without separating?
","Found the answer. Thank you @Aditya

import seaborn as sns
sns.lmplot('Time', 'Amount', dataset, hue='Class', fit_reg=False)
fig = plt.gcf()
fig.set_size_inches(15, 10)
plt.show()


where Time and Amount are the two features I needed to plot. Class is the column of the dataset that has the dependent binary class value.

And this is the plot I got as required. 
",<python><pandas><plotting><numpy><matplotlib>
"Question:

I want to compare every item in a 3D array with its first neighborhoods. It's really slow when I have a 500x500x500 (e.g. with only values of 0, 1, 2) ndarray. I post the principle lines here:

import numpy as np

# Create a list to stock all the neighbours' coordinations of the voxel wanted 
def check_neighbor(array, x, y, z):
   #top
   t = array[x, y , z + 1]
   #down
   d = array[x, y , z - 1]
   #left
   l = array[x, y - 1 , z]
   #right
   r = array[x, y + 1 , z]
   #front
   f = array[x - 1, y , z]
   #back
   b = array[x + 1, y , z]
   return [t, d, l, r, f, b]

# Check the voxel with all its neighborhood
def compare_neighbor(array, Value2match, Value2Bmatched):
   for index in np.argwhere(array==Value2match)
      output[index] = 1 if Value2BMatched in checkneighbor(array, index[0],index[1], index[2]) else 0 
   return output

# Main
array = np.random.randint(3, shape=(500, 500, 500))
output = compare_neighbor(array, 1, 2)


This code take me hours only for the n=1 neighbour! Is there an efficient way which can also check the 2, 3... nearest neighbours? Can somebody help me?

Solution 1:

Based on jayprich's answer and n1k31t4's comments, I fused the both function into one and replace the argwhere() by where(). The advantage of this code is that we don't iterate voxel by voxel but do it in a vectorized way:

import numpy as np
# Build a helper function to SHIFT(not roll) a 3Darray
def shift_helper(array, neib_value, shift=0, axis=0):
    #Roll the 3D array along axis with certain unity
    _array = np.roll(array == neib_value, shift=shift, axis=axis)

    # Cancel the last/first slice shifted to the first/last slice
    if axis == 0:
        if shift &gt;= 0:
            _array[:1, :, :] = 0
        else:
            _array[-1:, :, :] = 0
        return _array
    elif axis == 1:
        if shift &gt;= 0:
            _array[:, :1, :] = 0
        else:
            _array[:, -1:, :] = 0
        return _array
    elif axis == 2:
        if shift &gt;= 0:
            _array[:, :, :1] = 0
        else:
            _array[:, :, -1:] = 0
        return _array


def compneib(array, that_value, neib_value):
    _array = np.zeros(array.shape)
    _array[np.where((array == that_value)
                    &amp; (shift_helper(array, neib_value, shift=-1, axis=0)
                    | shift_helper(array, neib_value, shift=1, axis=0)
                    | shift_helper(array, neib_value, shift=-1, axis=1)
                    | shift_helper(array, neib_value, shift=1, axis=1)
                    | shift_helper(array, neib_value, shift=-1, axis=2)
                    | shift_helper(array, neib_value, shift=1, axis=2)
                    ))] = 1
    return _array

# Main
array = np.random.randint(3, shape=(500, 500, 500))
output = compneib(array, 1, 2)


Solution 2:

If there would still be a faster way.
","NumPy performs operations in a vectorised manner, you should try and operate on the whole array and avoid explicit loops.

e.g. np.argwhere( (test == 1) &amp; np.roll( test==2 , shift=-1 , axis=0 ) )

can be done for different shift and axis values and results combined
",<python><bigdata><numpy>
"I am working on a deep learning problem to detect cancer in images of size 250 x 250. I have hardware limitations and I have been running out of memory. 

I decided to convert my images to Matlab formatted files ("".mat""), with some improvement; however, I still run out of memory. I have explored some resources that highly recommend using NumPy files ("".npy"").

It would be costly to convert my images to NumPy files, so I would like to make sure that converting will make a difference. I am not asking for memory enhancement algorithms (e.g. batching), just the memory difference between "".mat"" and "".npy"" files.
","From my research:


both can store data in binary format
both store the data type of the data
I am unsure about compression ratios and load time, which seems to be the subtext of your question.


One thing you don't seem to address is what you are loading the data into, or whether you are considering moving from a MATLAB environment to Python environment or visa-versa.

That said, I found this post useful and thought it may be helpful to you if you have not seen it already.  https://stackoverflow.com/a/10997335/3259054 Perhaps you could write a small script to sample some files and see the difference.

Have you considered the HDF5 format?  If you are looking to make a change, you might as well test other options too and HDF5 has a lot of momentum towards becoming the de facto standard for scientific computing.

Finally, purely out of a desire to learn, why are you concerned about the file format if you have memory constraints?
",<python><deep-learning><matlab><numpy>
"I am working on a deep learning problem to detect cancer in images of size 250 x 250. I have hardware limitations and I have been running out of memory. 

I decided to convert my images to Matlab formatted files ("".mat""), with some improvement; however, I still run out of memory. I have explored some resources that highly recommend using NumPy files ("".npy"").

It would be costly to convert my images to NumPy files, so I would like to make sure that converting will make a difference. I am not asking for memory enhancement algorithms (e.g. batching), just the memory difference between "".mat"" and "".npy"" files.
","MATLAB has known for it's memory consumption. So even if you use same data for processing in Python overall system memory utilization will be less in Python.

Based on my experience so far using Python helped me dealing more data with better performance.

One other hand Python have many libraries/Frameworks out of box to further enhance the overall performance and Machine Learning/ Deep Learning (I am not much sure if similar Libraries &amp; Frameworks are available in Matlab also). 
",<python><deep-learning><matlab><numpy>
"I am working on a deep learning problem to detect cancer in images of size 250 x 250. I have hardware limitations and I have been running out of memory. 

I decided to convert my images to Matlab formatted files ("".mat""), with some improvement; however, I still run out of memory. I have explored some resources that highly recommend using NumPy files ("".npy"").

It would be costly to convert my images to NumPy files, so I would like to make sure that converting will make a difference. I am not asking for memory enhancement algorithms (e.g. batching), just the memory difference between "".mat"" and "".npy"" files.
","One way to reduce in-memory bottlenecks is to more efficiently handle data processing (regardless of the on-disk format).

There are software frameworks designed to improve the training process, especially for loading images. Dask is one such framework to scale existing Python workflows, thus mostly likely it will reduce the memory bottleneck for .npy files relative to .mat files (the only way to be sure is to benchmark).
",<python><deep-learning><matlab><numpy>
"When i run this function - I get the following error below:
I also checked my numpy version is 1.14 (which is higher than 1.7 when this issue was reported/resolved previously) - but still cant get to run this without error.

def rank_to_dict(ranks, names, order=1):  
    minmax = MinMaxScaler()  
    ranks = minmax.fit_transform(order*np.array([ranks]).T).T[0]  
    ranks = map(lambda x: round(x, 2), ranks)  
    return dict(zip(names, ranks ))  

Error
&lt;ipython-input-12-d23066e235d3&gt; in rank_to_dict(ranks, names, order)  
     21 def rank_to_dict(ranks, names, order=1):  
     22     minmax = MinMaxScaler()  
---&gt; 23     ranks = minmax.fit_transform(order*np.array([ranks]).T).T[0]  
     24     ranks = map(lambda x: round(x, 2), ranks)  
     25     return dict(zip(names, ranks ))  

TypeError: unsupported operand type(s) for *: 'int' and 'map  

","I couldn't think of a situation, where np.array would return a map, however if your input ranks is a map, it seems you do get an array of map objects back!

Here is an example:

In [1]: ranks = map(lambda x, y: x + y, [1, 2, 3, 4], [2, 3, 4, 5])

In [2]: ranks
Out[2]: &lt;map at 0x7f79d79c0e10&gt;

In [3]: np.array(ranks)
Out[3]: array(&lt;map object at 0x7f79d79c0e10&gt;, dtype=object)

In [4]: 1 * ranks
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-34-ff69339d7f75&gt; in &lt;module&gt;()
----&gt; 1 1 * ranks

TypeError: unsupported operand type(s) for *: 'int' and 'map'


I create a map object (just adds the two lists element-wise), then create a numpy array of that map. If I multiply it by an integer, I get your error.

You need to therefore make sure that your input argument ranks is not a map object. If you provide more information as to what it is, maybe I can help convert it as necessary.
",<numpy>
"I came across different approaches to creating a test set. Theoretically, it's quite simple, just pick some instances randomly, typically 20% of the dataset and set them aside. Below are the approaches 


  The naive way of creating the test set is 


def split_train_test(data,test_set_ratio):
  #create indices
  shuffled_indices = np.random.permutation(len(data))
  test_set_size = int(len(data) * test_set_ratio)
  test_set_indices = shuffled_indices[:test_set_size]
  train_set_indices = shuffled_indices[test_set_size:]
  return data.iloc[train_set_indices],data.iloc[test_set_indices]


The above splitting mechanism works, but if the program is run, again and again, it will generate a different dataset. Over the time, the machine learning algorithm will get to see all the examples. The solutions to fix the above problem was (guided by the author of the book) 


Save the test set on the first run and then load it in subsequent runs
To set the random number generator's seed(np.random.seed(42)) before calling np.random.permutation() so that it always generates the same shuffled indices


But both the above solutions break when we fetch the next updated dataset. I am not still clear with this statement. 


  Can someone give me an intuition behind how do the above two solutions breaks when we fetch the next updated dataset ?.


Then the author came up with another reliable approach to create the test.

 def split_train_test_by_id(data, test_ratio, id_column):
   ids = data[id_column]
   in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))
   return data.loc[~in_test_set], data.loc[in_test_set]



  Approach #1     


 def test_set_check(identifier, test_ratio, hash=hashlib.md5):
    return bytearray(hash(np.int64(identifier)).digest())[-1] &lt; 256 * test_ratio



  Approach #2


 def test_set_check(identifier, test_ratio):
    return crc32(np.int64(identifier)) &amp; 0xffffffff &lt; test_ratio * 2**32



  Approaches #1,#2, why are we making use of crc32, 0xffffffff, byte array?.  


Just out of curiosity, I passed different values for identifier variable into hash_function(np.int64(identifier)).digest() and I got different results. 


  Is there any intuition behind these results ?.

","It gets a little complicated, I've attached links at the end of the answer to explain as well. 

def test_set_check(identifier, test_ratio, hash=hashlib.md5):
    return bytearray(hash(np.int64(identifier)).digest())[-1] &lt; 256 * test_ratio


The hash(np.int64(identifier)).digest() part returns a hash value of the identifier (which we cast as an int of 8 bytes with np.int64()). 

The bytearray method stores the hash value into an array. The [-1] represents getting 1 byte/the last byte from the array/hash value. This byte will be a # between 0 and 255 (1 byte=11111111, or 255 in decimal).

Assuming our test_ratio is .20, the byte value should be less than or equal to 51 (256*.20=51.2). If the byte is less than or equal to 51, the row/instance will be added to the test set.

Rest is explained well in these links:

https://stackoverflow.com/questions/50646890/how-does-the-crc32-function-work-when-using-sampling-data

https://github.com/ageron/handson-ml/issues/71

https://docs.python.org/3/library/hashlib.html
",<machine-learning><python><preprocessing><numpy>
"I have a set of search results with ranking position, keyword and URL. I want to make a distance matrix so I can cluster the keywords (or the URLs). One approach would be to take the first n URL rankings for each keyword and use Jaccard similarity. 

However, I also want higher position ranks to be weighted more highly than lower position ranks - for example two keywords that have the same URL in positions 1 and 2 are more similar than two keywords that have the same URL ranking in positions 39 and 40.

I saw suggested that I could add in higher ranks multiple times, for example:

keyword_1 = [1,1,1,1,1,2,2,2,2,3,3,3,4,4,5]


where the number is the id of the URLs that rank in positions 1-5 of keyword_1.

This means that I can't use for example sklearn Jaccard implementation because sets are assumed.

I had a go at implementing this myself and intuitively the results seem to make sense, but I would like it to run faster, as I could use data for rankings up to 100. There is a lot of looping involved - is there a way of using numpy better to make this code more efficient?

Alternatively, is there a different approach that I haven't found to use already built algorithms?

def jaccard_sim_with_dupes(item1, item2):
        intersection = 0
        for i in item1:
            if i in item2:
                intersection += 1
        for j in item2:
            if j in item1:
                intersection += 1
        union = len(item1) + len(item2)
        return intersection / float(union)

def make_jaccard_distance_matrix(X):
    shape = (len(X), len(X))
    jaccard_distance_matrix = np.zeros(shape)
    for i in range(shape[0]):
        for j in range(shape[1]):
            jaccard_distance_matrix[i][j] = 1 - JaccardDistanceMatrix.jaccard_sim_with_dupes(X[i], X[j])
        if (i % 100 == 0):
            print (""\r{0:.0%}"".format(float(i) / shape[0]))
    return jaccard_distance_matrix


Here X is a list of each keyword, where each keyword is represented as a list of URLs, with higher ranked keywords added in multiple times.

X = [[1,1,1,1,1,2,2,2,2,3,3,3,4,4,5], [6,6,6,6,6,1,1,1,1,7,7,7,8,8,9]]

make_jaccard_distance_matrix(X)


returns

array([[ 0. ,  0.7],
   [ 0.7,  0. ]])

","I realised if I used the format

X = np.array([[5,4,2,3,1,0,0,0,0],[4,0,0,0,0,5,3,2,1]])


and a sparse matrix from the scipy module, I could use efficient matrix calculations. This ran in about 2 seconds for 7k samples and 400 features. 


from scipy.sparse import csr_matrix
import numpy as np


def jaccard_sim_matrix(X):
    """"""X is an integer array of features""""""

    sparseX = csr_matrix(X)

    # make a binary version of the matrix
    binX = sparseX
    binX.data[:] = 1

    intersection = ((sparseX * binX.T) + (binX * sparseX.T))

    rowwise_sum = np.sum(sparseX, axis=1)
    union = np.repeat(rowwise_sum, intersection.shape[0], axis=1) + np.repeat(rowwise_sum.T, intersection.shape[0],
                                                                              axis=0)

    return intersection / union


Then of course I got the distance by 1 - similarity
",<python><numpy><jaccard-coefficient>
"I have a set of search results with ranking position, keyword and URL. I want to make a distance matrix so I can cluster the keywords (or the URLs). One approach would be to take the first n URL rankings for each keyword and use Jaccard similarity. 

However, I also want higher position ranks to be weighted more highly than lower position ranks - for example two keywords that have the same URL in positions 1 and 2 are more similar than two keywords that have the same URL ranking in positions 39 and 40.

I saw suggested that I could add in higher ranks multiple times, for example:

keyword_1 = [1,1,1,1,1,2,2,2,2,3,3,3,4,4,5]


where the number is the id of the URLs that rank in positions 1-5 of keyword_1.

This means that I can't use for example sklearn Jaccard implementation because sets are assumed.

I had a go at implementing this myself and intuitively the results seem to make sense, but I would like it to run faster, as I could use data for rankings up to 100. There is a lot of looping involved - is there a way of using numpy better to make this code more efficient?

Alternatively, is there a different approach that I haven't found to use already built algorithms?

def jaccard_sim_with_dupes(item1, item2):
        intersection = 0
        for i in item1:
            if i in item2:
                intersection += 1
        for j in item2:
            if j in item1:
                intersection += 1
        union = len(item1) + len(item2)
        return intersection / float(union)

def make_jaccard_distance_matrix(X):
    shape = (len(X), len(X))
    jaccard_distance_matrix = np.zeros(shape)
    for i in range(shape[0]):
        for j in range(shape[1]):
            jaccard_distance_matrix[i][j] = 1 - JaccardDistanceMatrix.jaccard_sim_with_dupes(X[i], X[j])
        if (i % 100 == 0):
            print (""\r{0:.0%}"".format(float(i) / shape[0]))
    return jaccard_distance_matrix


Here X is a list of each keyword, where each keyword is represented as a list of URLs, with higher ranked keywords added in multiple times.

X = [[1,1,1,1,1,2,2,2,2,3,3,3,4,4,5], [6,6,6,6,6,1,1,1,1,7,7,7,8,8,9]]

make_jaccard_distance_matrix(X)


returns

array([[ 0. ,  0.7],
   [ 0.7,  0. ]])

","You can use a Counter to store the number of times each element appears.  That class provides a way to compute the union and intersection using the &amp; and | operators.  sum(c.values()) will return the number of items in the result, counting multiplicities.  This will let you implement jaccard_sim_with_dupes more efficiently (in linear time, rather than quadratic time).
",<python><numpy><jaccard-coefficient>
"I have dealt with all the Nan values in the features dataframe, then why I am still getting this error? 




sns.heatmap(features, annot=True, annot_kws={""size"": 7})
sns.plt.show()




TypeError                                 Traceback (most recent call last)
    &lt;ipython-input-11-534a699b432d&gt; in &lt;module&gt;()
    ----&gt; 1 sns.heatmap(features, annot=True, annot_kws={""size"": 7})
          2 sns.plt.show()

    c:\users\jetjo\appdata\local\programs\python\python36\lib\site-packages\seaborn\matrix.py in heatmap(data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, linewidths, linecolor, cbar, cbar_kws, cbar_ax, square, xticklabels, yticklabels, mask, ax, **kwargs)
        515     plotter = _HeatMapper(data, vmin, vmax, cmap, center, robust, annot, fmt,
        516                           annot_kws, cbar, cbar_kws, xticklabels,
    --&gt; 517                           yticklabels, mask)
        518 
        519     # Add the pcolormesh kwargs here

    c:\users\jetjo\appdata\local\programs\python\python36\lib\site-packages\seaborn\matrix.py in __init__(self, data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, cbar, cbar_kws, xticklabels, yticklabels, mask)
        166         # Determine good default values for the colormapping
        167         self._determine_cmap_params(plot_data, vmin, vmax,
    --&gt; 168                                     cmap, center, robust)
        169 
        170         # Sort out the annotations

    c:\users\jetjo\appdata\local\programs\python\python36\lib\site-packages\seaborn\matrix.py in _determine_cmap_params(self, plot_data, vmin, vmax, cmap, center, robust)
        203                                cmap, center, robust):
        204         """"""Use some heuristics to set good defaults for colorbar and range.""""""
    --&gt; 205         calc_data = plot_data.data[~np.isnan(plot_data.data)]
        206         if vmin is None:
        207             vmin = np.percentile(calc_data, 2) if robust else calc_data.min()

    TypeError: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''

","There may be column of type object in your dataFrame. Remove it or convert it into float,int etc. if possible

From Documentation of Seaborn: 

data : rectangular dataset

2D dataset that can be coerced into an ndarray. If a Pandas DataFrame is provided, the index/column information will be used to label the columns and rows.

so you can try

sns.heatmap(features.drop(['columnName01_OfTypeObject','columnName02_OfTypeObject'],axis=1), annot=True, annot_kws={""size"": 7})


This will drop your columns temporarily. 
",<pandas><numpy><seaborn>
"I'm struggling to find the most efficient way to combine multiple dataframes with columns that are years and country names for the index. As an example:

GDP.csv

     1900  1901  1902
USA  500   600   700
MEX  400   500   600
CAN  300   400   500


lifeEx.csv

     1900  1901  1902
USA  50   60   70
MEX  40   50   60
CAN  30   40   50


I can list the dataframes and use concat to combine them, but this makes it awkward to access each dataframe.

pieces = {'GDP': GDP, 'lifeEx': lifeEx}
result = pd.concat(pieces)

This will show me the value for one dataframe, country, and year. However, I cannot easily list all the data from all countries for a given year.

print(result.loc['GDP','USA'].loc['1900'])

Ideally, I think I want to use groupby() to sort the data by country, year, indicator (GDP, life expectancy, ...), or some combination of these. However, I don't see a clear way to do this.

I also considered iterating over the dataframe to create a year column. However, this is not very efficient. The result would be a file that looks like this, with country and year being the key:

GDP.csv

     year  GDP
USA  1900  500
USA  1901  600
USA  1902  700
MEX  1900  400
MEX  1901  500
MEX  1902  600
CAN  1900  300
CAN  1901  400
CAN  1902  500


After transforming the data like this, I could combine the dataframes using the country and year as the key. Since the column heading GDP would be unique for each data set, these columns would extend the dataframs rather than being added together.

What is a more elegant solution to this problem?
","After asking around, melt() was the most common answer I received. While implementing this using melt() I learned about stack() and that also offered a solution to this problem. Therefore, I setup a couple tests to determine which is the best solution to this problem.

The result appears to be that melt() takes longer to rearrange the data, but offers faster processing for future manipulations of that data. If I was only performing a few operations on the data, stack() may be a better solution.

When combining the dataframes, these were the execution times for melt() and stack() working with the same data set:
Melt: 63ms Stack: 19ms

When retrieving data for a single country from the combined data set, these were the execution times for melt() and stack():
Melt: 6ms Stack: 12ms

As you can see, stack() rearranges the data in 30% of the time required by melt(). However, melt() retrieves the data in 50% of the time required by stack().

It is also important to note that when using melt() I had to call dropna() as an additional step. This removed any rows which contained only NaN for all the columns. With stack() this was done by default.
",<python><pandas><numpy>
"I'm struggling to find the most efficient way to combine multiple dataframes with columns that are years and country names for the index. As an example:

GDP.csv

     1900  1901  1902
USA  500   600   700
MEX  400   500   600
CAN  300   400   500


lifeEx.csv

     1900  1901  1902
USA  50   60   70
MEX  40   50   60
CAN  30   40   50


I can list the dataframes and use concat to combine them, but this makes it awkward to access each dataframe.

pieces = {'GDP': GDP, 'lifeEx': lifeEx}
result = pd.concat(pieces)

This will show me the value for one dataframe, country, and year. However, I cannot easily list all the data from all countries for a given year.

print(result.loc['GDP','USA'].loc['1900'])

Ideally, I think I want to use groupby() to sort the data by country, year, indicator (GDP, life expectancy, ...), or some combination of these. However, I don't see a clear way to do this.

I also considered iterating over the dataframe to create a year column. However, this is not very efficient. The result would be a file that looks like this, with country and year being the key:

GDP.csv

     year  GDP
USA  1900  500
USA  1901  600
USA  1902  700
MEX  1900  400
MEX  1901  500
MEX  1902  600
CAN  1900  300
CAN  1901  400
CAN  1902  500


After transforming the data like this, I could combine the dataframes using the country and year as the key. Since the column heading GDP would be unique for each data set, these columns would extend the dataframs rather than being added together.

What is a more elegant solution to this problem?
","It is difficult to give an exact solution, because it depends on what you eventually want to do with the data after you have merged it. If you only want to sort, like you mentioned, you could just use a pure database

Otherwise, you could look into Pandas multi-indexing. This will make things clearer when you look at the tables, however I find that I end up wanting to get back into your so-called melted form, for things such as plotting and input to neural networks.

Using groupby could indeed work, and I find those solution quite elegant. In your case you could just group by year and/or country and then apply the function for your use case.

Given your vernacular, you might be interested to look at the standard database-like functionality of a dataframe, such as join and merge
",<python><pandas><numpy>
"I'm struggling to find the most efficient way to combine multiple dataframes with columns that are years and country names for the index. As an example:

GDP.csv

     1900  1901  1902
USA  500   600   700
MEX  400   500   600
CAN  300   400   500


lifeEx.csv

     1900  1901  1902
USA  50   60   70
MEX  40   50   60
CAN  30   40   50


I can list the dataframes and use concat to combine them, but this makes it awkward to access each dataframe.

pieces = {'GDP': GDP, 'lifeEx': lifeEx}
result = pd.concat(pieces)

This will show me the value for one dataframe, country, and year. However, I cannot easily list all the data from all countries for a given year.

print(result.loc['GDP','USA'].loc['1900'])

Ideally, I think I want to use groupby() to sort the data by country, year, indicator (GDP, life expectancy, ...), or some combination of these. However, I don't see a clear way to do this.

I also considered iterating over the dataframe to create a year column. However, this is not very efficient. The result would be a file that looks like this, with country and year being the key:

GDP.csv

     year  GDP
USA  1900  500
USA  1901  600
USA  1902  700
MEX  1900  400
MEX  1901  500
MEX  1902  600
CAN  1900  300
CAN  1901  400
CAN  1902  500


After transforming the data like this, I could combine the dataframes using the country and year as the key. Since the column heading GDP would be unique for each data set, these columns would extend the dataframs rather than being added together.

What is a more elegant solution to this problem?
","I think I know what you are going for.  Here is some example code.  

# Import pandas
import pandas as pd

# Load data
df_a = pd.read_csv('gdp.csv')
df_b = pd.read_csv('le.csv')

# Use melt to pull the columns from each df into separate rows tied to the key column.
df_a = df_a.melt(id_vars='country', var_name='year', value_name='gdp')
df_b = df_b.melt(id_vars='country', var_name='year', value_name='le')

# Then it is rather trivial to just merge based on the column names given above.
pd.merge(df_a, df_b, on=['country','year'])

# That would create the following output given your input files (as long as the column naming is correct.

country year gdp le
0   usa 1900 500 50
1   mex 1900 400 40
2   can 1900 300 30
3   usa 1901 600 60
4   mex 1901 500 50
5   can 1901 400 40
6   usa 1902 700 70
7   mex 1902 600 60
8   can 1902 500 50

",<python><pandas><numpy>
"Can anyone explain why the following code produces input_t with a shape of (32,) instead of (,32), given the fact that inputs has a shape (100, 32)? Shouldn't input_t produce a vector with 32 attributes/columns?

import numpy as np

timesteps = 100
input_features = 32
output_features = 64

inputs = np.random.random((timesteps, input_features))

state_t = np.zeros((output_features,))

W = np.random.random((output_features, input_features))
U = np.random.random((output_features, output_features))
b = np.random.random((output_features,))

successive_outputs = [ ]

for input_t in inputs:
    output_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b)
    successive_outputs.append(output_t)
    state_t = output_t 

","Imagine the matrix inputs as a 2D table. You have 100 rows and 32 columns. Then the for loop acts as an iterator which will return values along the first dimensional axis. This dimension thus disappears and returns the remaining dimensions. When there is a single dimension the default in Python is $(n,)$.

A matrix $(100, 32)$ iterates through $(32,)$

A matrix $(100,28,28)$ iterated through $(28,28)$ 

A matrix $(100,2,2,2)$ iterated through $(2,2,2)$ 
",<python><numpy>
"Goal
I am trying to build a neural network that recognizes multiple label within a given image. I started with a database composed of 1800 images (each image is an array of shape (204,204,3). I trained my model and concluded that data used wasn't enough in order to build a good model ( with respect to chosen metric). So i decided to apply data augmentation technique in order to get more images. I managed to get 25396 images ( all of them are of shape (204,204,3)).
I stored all of them in arrays . I obtained (X,Y) where X are the training examples (is an array of shape (25396,204,204,3)) and Y are the labels ( an array of shape (25396,39) : the number 39 refers to the possible labels in a given image).
Issues
My data (X,Y) weights approximately arround 26 giga bytes. I successfully managed to use them .  However, when i try to do manipulation (like permutations) I encounter memory Error in python.
Exemple
1. I started jupyter and successfully imported my data (X,Y)

x=np.load('x.npy')
y=np.load('y.npy')




output: x is an np.array of shape (25396,204,204,3) and y is an np.array of shape (25396,39).


2. I divide my dataSet in train and test by using sklearn built in function train_test_split

X_train, X_valid, Y_train, Y_valid= `train_test_split(x_train,y_train_augmented,test_size=0.3, random_state=42)`




output
-------------testing size of different elements et toplogie: 
-------------x size: (25396, 204, 204, 3)
-------------y size: (25396, 39) 
-------------X_train size:  (17777, 204, 204, 3)
-------------X_valid size:  (7619, 204, 204, 3)
-------------Y_train size:  (17777, 39)
-------------Y_valid size:  (7619, 39)


3. I am creating a list composed of random batches extracted from (X,Y) and then iterate over the batches in order to complete the learning process for a given epoch :'this opperation is done in each epoch of the training part. 
Here is the function used in order to create the list of random batches:

def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):
    """"""
    Creates a list of random minibatches from (X, Y)

    Arguments:
    X -- input data, of shape (input size, number of examples)
    Y -- true ""label"" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)
    mini_batch_size -- size of the mini-batches, integer

    Returns:
    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)
    """"""


    np.random.seed(seed)            
    m = X.shape[0]                  
    mini_batches = []
    # Step 1: Shuffle (X, Y)
    permutation = list(np.random.permutation(m))



    shuffled_X = X[permutation,:]
    shuffled_Y = Y[permutation,:]


    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.
    num_complete_minibatches = floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning
    for k in range(0, num_complete_minibatches):


        mini_batch_X = shuffled_X[k * mini_batch_size : (k + 1) * mini_batch_size, :]
        mini_batch_Y = shuffled_Y[k * mini_batch_size : (k + 1) * mini_batch_size, :]
        mini_batch = (mini_batch_X, mini_batch_Y)
        mini_batches.append(mini_batch)
        '''
        mini_batches.append((X[permutation,:][k * mini_batch_size : (k + 1) * mini_batch_size, :], Y[permutation,:][k * mini_batch_size : (k + 1) * mini_batch_size, :]))
        '''
    # Handling the end case (last mini-batch &lt; mini_batch_size)
    if m % mini_batch_size != 0:
        ### START CODE HERE ### (approx. 2 lines)
        mini_batch_X = shuffled_X[ num_complete_minibatches * mini_batch_size:, :]
        mini_batch_Y = shuffled_Y[ num_complete_minibatches * mini_batch_size:, :]
        ### END CODE HERE ###
        mini_batch = (mini_batch_X, mini_batch_Y)
        mini_batches.append(mini_batch)
        '''
        mini_batches.append((X[permutation,:][ num_complete_minibatches * mini_batch_size:, :], Y[permutation,:][ num_complete_minibatches * mini_batch_size:, :]))
        '''
    shuffled_X=None
    shuffled_Y=None
    return mini_batches


4. I am creating a loop (of 4 iterations) and i am testing the random_mini_batch function in each iteration. At the end of each iteration I am assigning None values to the list of mini_batches in order to liberate memory and redo the random_mini_batch_function in the next iteration .So these line of codes works fine and I ve got no memory issues:

minibatch_size=32
seed=2
for i in range(4):
    seed=seed+1
    minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)

    minibatches=None 
    minibatches_valid=create_mini_batches(X_valid, Y_valid, minibatch_size)
    print(i)
minibatches_valid=None


5. If I add iteration over the different batches! then I am getting a memory issue. In other words, if a run this code i get an error:

minibatch_size=32
seed=2
for i in range(4):
    seed=seed+1
    minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)
    #added code: iteration over mini_batches
    for minibatch in minibatches:
                print('batch training number ')
    #end of added code        
    minibatches=None 
    minibatches_valid=create_mini_batches(X_valid, Y_valid, minibatch_size)
    print(i)
    minibatches_valid=None




MemoryError                               Traceback (most recent call last)
&lt;ipython-input-13-9c1942cdf0bc&gt; in &lt;module&gt;()
      3 for i in range(4):
      4     seed=seed+1
----&gt; 5     minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)
      6     
      7     for minibatch in minibatches:

&lt;ipython-input-3-2056fee14def&gt; in random_mini_batches(X, Y, mini_batch_size, seed)

     23 
---&gt; 24     shuffled_X = X[permutation,:]
     25     shuffled_Y = Y[permutation,:]
     26 

MemoryError: 


Does any one knows what's the issue with np.arrays ? And why does the simple fact of adding an loop (iterating over the list of batches) result in a memory error.

Questions

1.Is it a good idea to load the whole dataset and then proceed to training? ( I need to create random batches in each epoch, so I don't see how to do so if the data is not preloaded ? You take random mini-batches from preloaded data, right?)
2. Are there any possible solutions guys?
","
  Does any one knows what's the issue with np.arrays ?


shuffled_X = X[permutation,:] makes copies, so it will allocate new array each time you do a permutation and blow up your memory.

If you don't have problems with storing whole dataset in memory you should be fine if you create batches just by using random indices, not shuffling the entire data matrix (np.random.choice is your friend).


  Is it a good idea to load the whole dataset and then proceed to
  training?


If your data fits into memory, then yes. You might want to try to learn what to do when that's not the case though - I personally find Keras - stuff from keras.preprocessing.image useful for that (at least for loading images).
",<python><deep-learning><tensorflow><numpy>
"Task: Build CNN Model (preferably Keras or TensorFlow) to Predict Labels Associated to Each Image in CelebA Dataset (Multi-label Image Classification)

In past, for majority of multiclass/binary image classification problems, I used to feed images efficiently using ImageDataGenerator and .flow_from_directory in Keras after images are properly organized in a separate directory for each class. Therefore, I have never bothered converting images to numpy.arrary prior to feeding to the model, unless I had to and of course datasets were small so that I could do it easily in my local machine.

However, CelebA is a Multi-label Image Classification with each images having 40 labels (attributes like Smiling, Eyeglasses, Young etc.)  meaning that I can not organize them in subclasses as I used to do, so .flow_from_directory is off the table (as far as I know!). Still I've managed to convert the images to numpy.array by the following simple loop:

import numpy as np
import skimage.transform

images_path='../img_align_celeba/'

train_images=[]
from skimage import data

for filename in train['Images'].tolist()):
    tmp=np.array(skimage.transform.resize(io.imread(os.path.join(images_path,filename))/255., (64, 64)))
    train_images.append(tmp)

x_train=np.array(train_images)
del train_images


Well it was not an impossible task. CelebA dataset is large, well not super large compared to many other image datasets (>200K RGB images, totally 1.4GB in size, each image ~ 8 KB). YET surprisingly it takes the hell of the time to convert these images to numpy arrays and even stuck during the run of a small CNN model.

My computer specs: MacBook Pro (2015), Memory: 8GB, Harddisk: 128 GB.

Even with almost more than 4GM free memory, and 20 GB free hard-disk I could not manage this on my local machine. 

UPDATE: It seems quite possible and way more efficient via .flow_from_directory method of ImageDataGenerator in Keras. While it is not that option for this multi label classification at hand, I simply made some dummy subclasses and it worked and the model runs much faster!! 

To My Questions (finally!!):


Maybe I am not doing the image-to-numpy conversion efficiently? Please advice how I could construct the arrays in a more efficient approach!

UPDATE: I found a very similar question in stackoverflow a year ago, yet answers do not seem to offer any better alternative.

Maybe it is what it is and I only need better hardware to do it locally!? 
How then Keras manage to do the conversion efficiently under the hood then?


At the end (as we speak), I sampled only 20% of the images to have at least a model prototype up and running, although accuracy is not impressive!! 
","I ended up writing a python generator, which actually works very well, for manually feeding desired number of images chunk by chunk into my CNN model like this: 

def image_batch_generator(df,images_path, batch_size):

    '''
    A generator that takes a dataframe (for image names) and
    with a given image path goes to conver images to numpy array over
    batch (chunk by chunk).

    ""df"": is the ""Attributes Annotations"" text file from CelebA dataset.
    It has a column for image names, and another 40 attributes columns 
    (binary) for each image. 

    ""images_path"": it is the path to CelebA dataset image files.

    ""batch_size"": the batch size by which image will be read chunk by chunk.
    '''

    L = df.shape[0]
    files = df['Images'].tolist()

    #this line is just to make the generator infinite, keras needs that    
    while True:

        batch_start = 0
        batch_end = batch_size

        while batch_start &lt; L:
            limit = min(batch_end, L)

            X= np.array([np.array(skimage.transform.resize(io.imread(os.path.join(images_path,fname))/255., (64, 64))) for fname in files[batch_start:limit]])
            y=df.loc[df[""Images""].isin(files[batch_start:limit]), :].drop(['Images'],axis=1).values

            yield (X,y) #a tuple with two numpy arrays with batch_size samples     

            batch_start += batch_size   
            batch_end += batch_size


Hope it will help someone out there.
",<machine-learning><image-classification><cnn><multilabel-classification><numpy>
"I got this matrix 

    120     100     80      40      20      10      5       0
120 64.21   58.20   51.20   56.37   47.00   45.61   46.86   2.16
100 62.84   57.80   50.60   51.32   39.43   39.30   42.80   0.89
80  62.62   56.20   51.20   51.61   46.23   37.20   42.20   5.32
40  62.05   52.10   44.20   48.79   42.22   35.16   41.80   1.81
20  61.65   50.90   42.30   46.23   44.83   32.70   41.50   6.24
10  59.69   50.20   40.10   40.20   44.28   32.80   39.90   12.31
5   59.05   49.20   40.60   38.90   44.10   30.80   32.80   9.91
0   56.20   49.10   40.50   38.60   36.20   32.20   31.50   0.00


I know how to plot heatmap for the values inside by specifying it as numpy array and then using

ax = sns.heatmap(nd, annot=True, fmt='g')


But can someone help me how do I include the column and row labels? The column labels and row labels are given (120,100,80,42,etc.)
","Here's how we can add simple X-Y labels in sns heatmap:
s = sns.heatmap(cm_train, annot=True, fmt='d', cmap='Blues')
s.set(xlabel='X-Axis', ylabel='Y-Axis')

OR
s.set_xlabel('X-Axis', fontsize=10)
s.set_ylabel('Y-Axis', fontsize=10)

",<python><visualization><numpy><seaborn>
"I got this matrix 

    120     100     80      40      20      10      5       0
120 64.21   58.20   51.20   56.37   47.00   45.61   46.86   2.16
100 62.84   57.80   50.60   51.32   39.43   39.30   42.80   0.89
80  62.62   56.20   51.20   51.61   46.23   37.20   42.20   5.32
40  62.05   52.10   44.20   48.79   42.22   35.16   41.80   1.81
20  61.65   50.90   42.30   46.23   44.83   32.70   41.50   6.24
10  59.69   50.20   40.10   40.20   44.28   32.80   39.90   12.31
5   59.05   49.20   40.60   38.90   44.10   30.80   32.80   9.91
0   56.20   49.10   40.50   38.60   36.20   32.20   31.50   0.00


I know how to plot heatmap for the values inside by specifying it as numpy array and then using

ax = sns.heatmap(nd, annot=True, fmt='g')


But can someone help me how do I include the column and row labels? The column labels and row labels are given (120,100,80,42,etc.)
","I got your problem like this way:
You want to show labels on the x and y-axis on the seaborn heatmap. So for that, sns.heatmap() function has two parameters which are xticklabels for x-axis and yticklabels for y-axis labels.
Follow the code snippet below:
import seaborn as sns # for data visualization
flight = sns.load_dataset('flights') # load flights datset from GitHub seaborn repository

# reshape flights dataset in proper format to create seaborn heatmap
flights_df = flight.pivot('month', 'year', 'passengers') 

sns.heatmap(flights_df)# create seaborn heatmap

Output &gt;&gt;&gt;

Now, we are changing x and y-axis labels using  xticklabels and yticklabels sns.heatmap() parameters.
x_axis_labels = [1,2,3,4,5,6,7,8,9,10,11,12] # labels for x-axis
y_axis_labels = [11,22,33,44,55,66,77,88,99,101,111,121] # labels for y-axis

# create seabvorn heatmap with required labels
sns.heatmap(flights_df, xticklabels=x_axis_labels, yticklabels=y_axis_labels)

Output &gt;&gt;&gt;

For an in-depth explanation follow the seaborn heatmap tutorial.
",<python><visualization><numpy><seaborn>
"My data is:
userID, gameID, rating (1.0 through 10.0)
First, I normalize the values the ratings of each row.
I use cosine similarity to create a similarity matrix where each cell represents similarity between a pair of userIDs (value 0.0 through 1.0).
For each unrated gameID, I find the 10 most similar users that have rated the gameID. The predicted rating is equal to the sum of each neighbors rating times similarity, then divided by 10 (number of neighbors).

This seems fine for finding the best predictions, just take the top N rating values, but for actually predicting the values it doesn't perform so well. Intuitively, the average of a group of similar userID's ratings would be an accurate prediction. When each rating is made smaller by multiplying by a value that is always less than 1.0, the predicted value is consistently smaller than expected. 

This seemed to follow formulas that I found, but I feel like I missed something. I am using Python, Pandas, and Numpy.

Edit
I am seeing that this final value is sometimes referred to as a ""weighted predicted rating"". I am wondering how to ""de-weight"".
","
  The predicted rating is equal to the sum of each neighbors rating times similarity, then divided by 10 (number of neighbors).


You want to obtain a weighted-average of ratings, where the weight is the similarity score. Instead of dividing by 10 above, you should divide by the sum of similarities, to get a correct normalized score. Dividing by 10 is the likely reason for the predictions to be smaller than expected.

The formula you should use is:


  Final score = Sum_over_neighbours(Rating * Similarity) / Sum_over_neighbours(Similarity)

",<machine-learning><python><pandas><recommender-system><numpy>
"I was trying the Keras CNN Stater Code on Ubuntu 16.04, from the below link:
https://www.hackerearth.com/challenge/competitive/deep-learning-3/machine-learning/predict-the-energy-used-612632a9/#c144537

I get “MemoryError:” for

X_train = np.array(train_img, np.float32) / 255.


Any idea, what should I be doing?
","MemoryError is exactly what it means, you have run out of memory in your RAM for your code to execute. 

When this error occurs it is likely because you have loaded the entire data into memory. For large datasets you will want to use batch processing. Instead of loading your entire dataset into memory you should keep your data in your hard drive and access it in batches. If you are using Keras there is a helper class with a very efficient implementation of batch processing. Take a look at this blog post. This is a good starting point for avoiding MemoryError. 



As a short term fix you can train your model using a subset of the data available to you and discard the rest. Doing this really is a shame however.
",<python><jupyter><numpy>
"I am very new to machine learning, and it's hard for me to know in what direction I should go. Thanks in advance for your help.

I have a set of data showing what customers bought:


A customer may have bought 1 or more items
Each item is discribed by :


A quality level which is an integer between 1 and 7
Its content from a first point of view (the item has x1% of content 1, x2% of content 2, ...)
Its content from a second point of view (the item has xA% of content A, xB% of content B, ...)

A customer says that he belongs to the group G1, G2 or G3


I would like to be able to guess the group a new customer belongs to, given the items he bought.

What algorithm should I ideally use ?
What algorithm would you recommend if I can only use numpy, without scipy or scikit (it's on a server which only has numpy) ?

Thank you very much

Edit : I'm adding the appropriate words to the title and to the tags
","As per the problem statement statement it does look like 3 class classification problem. 
But I don't think you can use any ML algorithms using Python without sklearn package in it. Try getting it installed and you can implement the problem using Logistic regression, Decision Tree, there are still more classification algorithms.
However if you are planning to extract a pattern from the dataset and making into cluster, you can use K-means clustering which is a un-supervised algorithm. 
",<classification><supervised-learning><numpy>
"Was trying my hand at the Titanic dataset, when I wanted to One Hot Encode a categorical feature, after which I wanted to combine the original data with the new one hot vectors. The datatypes are as such:

data : Pandas Dataframe

Titles_ohe : Numpy sparse matrix (float64)

I tried to merge them into a dataframe using np.c_ :

columns = (list(data))+list(Titles.values)

data = pd.DataFrame(np.c_[data.values, Titles_ohe.toarray()], columns=columns)

However on checking the data type of the resulting Dataframe, all the attributes have been changed to the object datatype. Is there any way I can prevent this while using np.c_, or is there an alternative solution? Thanks in advance for any help!
","I'd use DataFrame.join() in this case:

data = data.join(pd.SparseDataFrame(Titles_ohe, index=data.index, columns=Titles))

",<machine-learning><python><pandas><data-cleaning><numpy>
"I have a column in my dataset by name 'production_output' whose values are integers in the range of 2.300000e+01 to 1.110055e+08.

I want to arbitrarily split the values in this column into different buckets based on say, percentile ranges like say [0, 25, 50, 75, 100] and get count of the length of each of theses buckets.

How do I do this using python data-science packages?
","numpy.histogram

https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.histogram.html

Use numpy.percentile to get the bin edges you desire. 

https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.percentile.html
",<pandas><numpy>
"I want to plot some data that contains timedeltas. However, the numpy plot() and errorbar() return:

TypeError: float() argument must be a string or a number, not 'Timedelta'


The question is how can I efficiently convert the timedeltas to a float or integer number. E.g. when I use minutes as base:

0 days 01:30:00 -&gt; 90


or hours

0 days 01:30:00 -&gt; 1.5


Similar to Series.dt.days which would return 0 in this case.
","One way to do this would be to use Series.dt.seconds and ´Series.dt.days´ and  multiply with a factor for the desired unit:

(Series.dt.seconds/3600) + (Series.dt.days*24)  # for values with [hours]

",<python><pandas><numpy>
"I want to plot some data that contains timedeltas. However, the numpy plot() and errorbar() return:

TypeError: float() argument must be a string or a number, not 'Timedelta'


The question is how can I efficiently convert the timedeltas to a float or integer number. E.g. when I use minutes as base:

0 days 01:30:00 -&gt; 90


or hours

0 days 01:30:00 -&gt; 1.5


Similar to Series.dt.days which would return 0 in this case.
","You can divide by the timedelta you want to use as unit:
totalDays = my_timedelta = pd.TimeDelta('1D')

",<python><pandas><numpy>
"I'm interested in exporting a correlation matrix to csv. I've tried using the to.csv functionality, but my matrix is obtained from a function and I get an ""object has no attribute 'to_csv'"" error. I appreciate any guidance. Code is below.

import numpy as np
from matplotlib import pyplot as plt
from matplotlib import cm as cm

def correlation_matrix(df):

    fig = plt.figure()
    ax1 = fig.add_subplot(111)
    cmap = cm.get_cmap('jet', 30)
    cax = ax1.imshow(df.corr(), interpolation=""nearest"", cmap=cmap)
    ax1.grid(True)
    plt.title('Feature Correlation')
    labels = None
    # Add colorbar, and make sure to specify tick locations to match desired ticklabels.
    cbar = fig.colorbar(cax, ticks=[-1.1, -0.8, -0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6,.8,1])
    plt.show()

correlation_matrix(pd.DataFrame(np.c_[Xs,Y]))

","Don't use the matrix obtained from your function rather,

It looks like correlation is a DataFrame too, so you can simply use to_csv:

correlation_df(within your function itself).to_csv(""C:\destinationfolder\file.csv"")

",<machine-learning><feature-selection><numpy><matplotlib>
"I have a dataframe with a bunch of columns (words).

df

        arg1 predicate
    0   PERSON        be
    1       it      Pick
    2  details      Edit
    3    title   Display
    4    title   Display


I used a pretrained word2vec model to create a new df with all words replaced by vectors (1-D numpy arrays).

 get updated_df

    updated_df = df.applymap(lambda x: self.filterWords(x))
    def filterWords(self, x):
        model = gensim.models.KeyedVectors.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin', binary=True)
        if x in model.vocab:
            return model[x]
        else:
            return model['xxxxx']


updated_df print:

             arg1  \
        0  [0.16992188, -0.48632812, 0.080566406, 0.33593...   
        1  [0.084472656, -0.0003528595, 0.053222656, 0.09...   
        2  [0.06347656, -0.067871094, 0.07714844, -0.2197...   
        3  [0.06640625, -0.032714844, -0.060791016, -0.19...   
        4  [0.06640625, -0.032714844, -0.060791016, -0.19...   

                                                   predicate  
        0  [-0.22851562, -0.088378906, 0.12792969, 0.1503...  
        1  [0.018676758, 0.28515625, 0.08886719, 0.213867...  
        2  [-0.032714844, 0.18066406, -0.140625, 0.115722...  
        3  [0.265625, -0.036865234, -0.17285156, -0.07128...  
        4  [0.265625, -0.036865234, -0.17285156, -0.07128...


I need to train a SVM(sklearn Linear SVC) with this data.
When I pass the updated_df as X_Train, I get 

clf.fit(updated_df, out_df.values.ravel())    
array = np.array(array, dtype=dtype, order=order, copy=copy)
ValueError: setting an array element with a sequence


What is the right way of passing this as the input data to the classifier? 
My y_train is fine.
If I get a hash of the words to create the updated_df like below, it works fine.

updated_df = df.applymap(lambda x: hash(x))


But I need to pass the word2vec vectors to establish a relationship between the words. I am new to python/ML and appreciate the guidance.

Editing with the current status based on Theudbald's suggestion:

class ConcatVectorizer(object):
def __init__(self, word2vec):
    self.word2vec = word2vec
    # if a text is empty we should return a vector of zeros
    # with the same dimensionality as all the other vectors
    self.dim = len(word2vec.itervalues().next())
    print ""self.dim = "", self.dim

def fit(self, X, y):
    print ""entering concat embedding fit""
    print ""fit X.shape = "", X.shape
    return self

def transform(self, X):
    print ""entering concat embedding transform""
    print ""transform X.shape = "", X.shape
    dictionary = {':': 'None', '?': 'None', '': 'None', ' ': 'None'}
    X = X.replace(to_replace=[':','?','',' '], value=['None','None','None','None'])
    X = X.fillna('None')
    print ""X = "", X
    X_array = X.values
    print ""X_array = "", X_array

    vectorized_array = np.array([
        np.concatenate([self.word2vec[w] for w in words if w in self.word2vec]
                or [np.zeros(self.dim)], axis=0)
        for words in X_array
    ])

    print ""vectorized array"", vectorized_array
    print ""vectorized array.shape"", vectorized_array.shape
    return vectorized_array


model = gensim.models.KeyedVectors.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin', binary=True)
    w2v = {w: vec for w, vec in zip(model.wv.index2word, model.wv.syn0)}
etree_w2v_concat = Pipeline([
    (""word2vec vectorizer"", ConcatVectorizer(w2v)),
    (""extra trees"", ExtraTreesClassifier(n_estimators=200))])
rf.testWordEmbClassifier(etree_w2v_concat)

       def testWordEmbClassifier(self, pipe_obj):
    kb_fname = 'kb_data_3.csv'
    test_fname = 'kb_test_data_3.csv'
    kb_data = pd.read_csv(path + kb_fname, usecols=['arg1',
                                                        'feature_word_0',
                                                        'feature_word_1',
                                                        'feature_word_2',
                                                        'predicate'])
    kb_data_small = kb_data.iloc[0:5]
    kb_data_out = pd.read_csv(path + kb_fname, usecols=['output'])
    kb_data_out_small = kb_data_out.iloc[0:5]
    print kb_data_small
    pipe_obj.fit(kb_data_small, kb_data_out_small.values.ravel())
    print pipe_obj.predict(kb_data_small)
    self.wordemb_predictResult(pipe_obj, test_fname, report=True)

","In my opinion, scikit-learn raises an error because updated_df is composed of 2 features (columns) with list formats. Therefore, for a given observation x_i : 

x_i = [arg1_i, predicate_i] = [[vector_arg1_i], [vector_predicate_i]].


Scikit-learn can't handle this format of input features.

There are mutiple ways to train a suprevised machine learning model after Word2Vec text processing. A common one is to sum or to average columns arg1 and predicate in order to have following observation x_i structure : 

x_i = [(arg1_i + predicate_i) / 2] = [(vector_arg_i + vector_predicate_i) / 2]


More explanations and a gentle comparison between Word2Vec and CountVectorizer features engineering approaches for text classification : 

http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/
",<python><scikit-learn><pandas><svm><numpy>
"Ok guys, I might be very tired here, but I can't figure out why this matrix multiplication by a scalar gives the following result (python)


  Matrix named 'dx'
        [ 1.6,  3.6,  0.4, 14.4, 25.6],
        [10. , 10. ,  0.4, 14.4,  3.6],
        [ 0.4,  0. ,  0. ,  1.6, 10. ],
        [ 6.4,  0. ,  3.6,  1.6,  0.4],
        [14.4,  0. , 25.6,  0.4,  6.4]
  
  10 * dx, in python, gives 
        [ -40.,  -60.,  -20., -120., -160.],
        [-100., -100.,  -20., -120.,  -60.],
        [ -20.,    0.,    0.,  -40., -100.],
        [ -80.,    0.,  -60.,  -40.,  -20.],
        [-120.,    0., -160.,  -20.,  -80.]


From what I understand, each member should be multiplicated by 10, but it's clearly not the case.

I'm using Python 3.6 &amp; numpy

What Am I missing ? 
Link to matrix multiplication by scalar , pretty basic stuff :
http://www.purplemath.com/modules/mtrxmult.htm

Thanks !
","It works for me.

&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; dx = np.matrix([[ 1.6, 3.6, 0.4, 14.4, 25.6],
... [10. , 10. , 0.4, 14.4, 3.6],
... [ 0.4, 0. , 0. , 1.6, 10. ],
... [ 6.4, 0. , 3.6, 1.6, 0.4],
... [14.4, 0. , 25.6, 0.4, 6.4]])
&gt;&gt;&gt; 10 * dx
matrix([[  16.,   36.,    4.,  144.,  256.],
        [ 100.,  100.,    4.,  144.,   36.],
        [   4.,    0.,    0.,   16.,  100.],
        [  64.,    0.,   36.,   16.,    4.],
        [ 144.,    0.,  256.,    4.,   64.]])


You might want to check if you have redefined dx before the multiplication.
",<matrix-factorisation><numpy>
"I have a pandas dataframe like this

0    15.55
1    15.55
2    15.55
3    15.55
4    20.84
Name: Y1, dtype: float64


I want to convert the values of Y1 to categorical (i.e) if its greater than 18.25, I want it 1 else 0

Can someone please help me on how to do it

This is what i tried so far

for temp in TRAIN_ID1:
    train_ID1.loc[(train_ID1['Y1'] &gt; 18.250000), 'Y1'] = 1
    train_ID1.loc[(train_ID1['Y1'] &lt; 18.250000), 'Y1'] = 0


But im getting an error

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item()

TypeError: an integer is required

During handling of the above exception, another exception occurred:

KeyError                                  Traceback (most recent call last)
&lt;ipython-input-118-2cccb791d834&gt; in &lt;module&gt;()
      1 for temp in train_ID1:
----&gt; 2     train_ID1.loc[(train_ID1['Y1'] &gt; 18.250000), 'Y1'] = 1
      3     train_ID1.loc[(train_ID1['Y1'] &lt; 18.250000), 'Y1'] = 0

~\Anaconda3\envs\deeplearning\lib\site-packages\pandas\core\series.py in __getitem__(self, key)
    621         key = com._apply_if_callable(key, self)
    622         try:
--&gt; 623             result = self.index.get_value(self, key)
    624 
    625             if not is_scalar(result):

~\Anaconda3\envs\deeplearning\lib\site-packages\pandas\core\indexes\base.py in get_value(self, series, key)
   2558         try:
   2559             return self._engine.get_value(s, k,
-&gt; 2560                                           tz=getattr(series.dtype, 'tz', None))
   2561         except KeyError as e1:
   2562             if len(self) &gt; 0 and self.inferred_type in ['integer', 'boolean']:

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_value()

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_value()

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

KeyError: 'Y1'

","As the error suggests, you don't have a column called Y1. Hence the error. Here is my suggestion to fix this. Assuming you input data looks like this - 

15.55
15.55
15.55
15.55
20.84


Read it in pandas this way - 

import pandas as pd

df = pd.read_csv('path/to/file.csv', header=None)


Provide a column name for this - 

df.columns = ['Y1']


If you have more columns, just fill the df.columns list accordingly.

Finally, use the pandas best practices as per their latest documentation to assign a new column - 

df = df.assign(Y2= (df['Y1'] &gt; 18.250000).astype(int))


Output

print(df)

 Y1  Y2
0  15.55   0
1  15.55   0
2  15.55   0
3  15.55   0
4  20.84   1


Note: Since I don't have full visibility on what you are working on, I have assumed what might be the problems you are facing. If this doesn't work let me know.
",<machine-learning><dataset><pandas><numpy>
"I have a large training set of ~300GB (which is a subset of an even larger dataset ~15TB). 

I am trying train a Convnet with Keras (Tensorflow backend) to do something similar to semantic segmentation. 

I couldn't find any valuable resources to handle such large data. Any suggestions for best practices for such humungous data is appreciated.

Thanks!
","You don't need to load the whole dataset into memory at once. The only data you need in memory are the samples in a single training batch. Use the fit_generator method rather than fit to pass in an iterator that feeds samples to your model from disk rather than loading all of that data at once. Here's a tutorial that discusses this more.
",<bigdata><convolutional-neural-network><numpy>
"I use numpy arrays to work with deep learning images. But as the data gets bigger, I'm facing issue with RAM even before training the model when using techniques like data augmentation.

Can someone suggest me how to work with large data for eg. 30GB of data in my system which has 16gb ram.

P.S. I'm worried about RAM during preprocessing and training, while i do batch processing with my GPU
","Do all image preparation and data augmentation during preprocessing and save the result as arrays of one or more samples (up to mini-batch size). Do not convert the arrays back to images. Read these prepared arrays with the wrapper that trains your model. I recommend numpy.save for its simplicity and transparency. Other options are discussed here: Stackoverflow - persisting numpy arrays  .
",<python><deep-learning><keras><numpy>
"I use numpy arrays to work with deep learning images. But as the data gets bigger, I'm facing issue with RAM even before training the model when using techniques like data augmentation.

Can someone suggest me how to work with large data for eg. 30GB of data in my system which has 16gb ram.

P.S. I'm worried about RAM during preprocessing and training, while i do batch processing with my GPU
","Dask is designed to manage these types of workloads. It provides an interface like NumPy, Pandas, or Python iterators for larger-than-memory operations. An example of using Dask with TensorFlow can be found here.
",<python><deep-learning><keras><numpy>
"I use numpy arrays to work with deep learning images. But as the data gets bigger, I'm facing issue with RAM even before training the model when using techniques like data augmentation.

Can someone suggest me how to work with large data for eg. 30GB of data in my system which has 16gb ram.

P.S. I'm worried about RAM during preprocessing and training, while i do batch processing with my GPU
","During image preprocessing in Keras, you may run out of memory when doing zca_whitening, which involves taking the dot product of an image with itself. This depends on the size of individual images in your dataset, not on the total size of your dataset. 

The memory required for zca_whitening will exceed 16GB for all but very small images, see here for an explanation. 

To solve this you can set zca_whitening=False in ImageDataGenerator.
",<python><deep-learning><keras><numpy>
"I have a model with this summary:



___________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
input_1 (InputLayer)             (None, 30, 37)        0                                            
____________________________________________________________________________________________________
s0 (InputLayer)                  (None, 128)           0                                            
____________________________________________________________________________________________________
bidirectional_1 (Bidirectional)  (None, 30, 128)       52224       input_1[0][0]                    
____________________________________________________________________________________________________
repeat_vector_1 (RepeatVector)   (None, 30, 128)       0           s0[0][0]                         
                                                                   lstm_1[0][0]                     
                                                                   lstm_1[1][0]                     
                                                                   lstm_1[2][0]                     
                                                                   lstm_1[3][0]                     
                                                                   lstm_1[4][0]                     
                                                                   lstm_1[5][0]                     
                                                                   lstm_1[6][0]                     
                                                                   lstm_1[7][0]                     
                                                                   lstm_1[8][0]                     
____________________________________________________________________________________________________
concatenate_1 (Concatenate)      (None, 30, 256)       0           bidirectional_1[0][0]            
                                                                   repeat_vector_1[0][0]            
                                                                   bidirectional_1[0][0]            
                                                                   repeat_vector_1[1][0]            
                                                                   bidirectional_1[0][0]            
                                                                   repeat_vector_1[2][0]            
                                                                   bidirectional_1[0][0]            
                                                                   repeat_vector_1[3][0]            
                                                                   bidirectional_1[0][0]            
                                                                   repeat_vector_1[4][0]            
                                                                   bidirectional_1[0][0]            
                                                                   repeat_vector_1[5][0]            
                                                                   bidirectional_1[0][0]            
                                                                   repeat_vector_1[6][0]            
                                                                   bidirectional_1[0][0]            
                                                                   repeat_vector_1[7][0]            
                                                                   bidirectional_1[0][0]            
                                                                   repeat_vector_1[8][0]            
                                                                   bidirectional_1[0][0]            
                                                                   repeat_vector_1[9][0]            
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 30, 1)         257         concatenate_1[0][0]              
                                                                   concatenate_1[1][0]              
                                                                   concatenate_1[2][0]              
                                                                   concatenate_1[3][0]              
                                                                   concatenate_1[4][0]              
                                                                   concatenate_1[5][0]              
                                                                   concatenate_1[6][0]              
                                                                   concatenate_1[7][0]              
                                                                   concatenate_1[8][0]              
                                                                   concatenate_1[9][0]              
____________________________________________________________________________________________________
attention_weights (Activation)   (None, 30, 1)         0           dense_1[0][0]                    
                                                                   dense_1[1][0]                    
                                                                   dense_1[2][0]                    
                                                                   dense_1[3][0]                    
                                                                   dense_1[4][0]                    
                                                                   dense_1[5][0]                    
                                                                   dense_1[6][0]                    
                                                                   dense_1[7][0]                    
                                                                   dense_1[8][0]                    
                                                                   dense_1[9][0]                    
____________________________________________________________________________________________________
dot_1 (Dot)                      (None, 1, 128)        0           attention_weights[0][0]          
                                                                   bidirectional_1[0][0]            
                                                                   attention_weights[1][0]          
                                                                   bidirectional_1[0][0]            
                                                                   attention_weights[2][0]          
                                                                   bidirectional_1[0][0]            
                                                                   attention_weights[3][0]          
                                                                   bidirectional_1[0][0]            
                                                                   attention_weights[4][0]          
                                                                   bidirectional_1[0][0]            
                                                                   attention_weights[5][0]          
                                                                   bidirectional_1[0][0]            
                                                                   attention_weights[6][0]          
                                                                   bidirectional_1[0][0]            
                                                                   attention_weights[7][0]          
                                                                   bidirectional_1[0][0]            
                                                                   attention_weights[8][0]          
                                                                   bidirectional_1[0][0]            
                                                                   attention_weights[9][0]          
                                                                   bidirectional_1[0][0]            
____________________________________________________________________________________________________
c0 (InputLayer)                  (None, 128)           0                                            
____________________________________________________________________________________________________
lstm_1 (LSTM)                    [(None, 128), (None,  131584      dot_1[0][0]                      
                                                                   s0[0][0]                         
                                                                   c0[0][0]                         
                                                                   dot_1[1][0]                      
                                                                   lstm_1[0][0]                     
                                                                   lstm_1[0][2]                     
                                                                   dot_1[2][0]                      
                                                                   lstm_1[1][0]                     
                                                                   lstm_1[1][2]                     
                                                                   dot_1[3][0]                      
                                                                   lstm_1[2][0]                     
                                                                   lstm_1[2][2]                     
                                                                   dot_1[4][0]                      
                                                                   lstm_1[3][0]                     
                                                                   lstm_1[3][2]                     
                                                                   dot_1[5][0]                      
                                                                   lstm_1[4][0]                     
                                                                   lstm_1[4][2]                     
                                                                   dot_1[6][0]                      
                                                                   lstm_1[5][0]                     
                                                                   lstm_1[5][2]                     
                                                                   dot_1[7][0]                      
                                                                   lstm_1[6][0]                     
                                                                   lstm_1[6][2]                     
                                                                   dot_1[8][0]                      
                                                                   lstm_1[7][0]                     
                                                                   lstm_1[7][2]                     
                                                                   dot_1[9][0]                      
                                                                   lstm_1[8][0]                     
                                                                   lstm_1[8][2]                     
____________________________________________________________________________________________________
dense_2 (Dense)                  (None, 11)            1419        lstm_1[0][0]                     
                                                                   lstm_1[1][0]                     
                                                                   lstm_1[2][0]                     
                                                                   lstm_1[3][0]                     
                                                                   lstm_1[4][0]                     
                                                                   lstm_1[5][0]                     
                                                                   lstm_1[6][0]                     
                                                                   lstm_1[7][0]                     
                                                                   lstm_1[8][0]                     
                                                                   lstm_1[9][0]                     
====================================================================================================
Total params: 185,484
Trainable params: 185,484
Non-trainable params: 0
____________________________________________________________________________________________________


The model is further summarised as:



And the ""attention"" block summarised as:



The input is a fuzzy date, e.g. ""November 17, 1979"" (capped at 30 characters) and the output is the 10 character representation ""YYYY-mm-dd"".

I would like to plot the values of the attention_weights layer.

I would like to see which part of ""Saturday, 17th November, 1979"" the network ""looks at"" when it predicts each of YYYY, mm, and dd. I'm expecting to see it ignores the day (""Saturday"") completely.

I've tried following the Keras documentation for obtaining the output of an intermediate layer.

However, the attention node has 10 inputs, so I have to grab each of those:

f = K.function(model.inputs, [model.get_layer('attention_weights').get_output_at(t) for t in range(10)])
r = f([source, np.zeros((1,128)), np.zeros((1,128))])


With source e.g. ""17 November 1979"" encoded as

[[[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.
    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    0.]
  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.
    0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.
    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.
    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.
    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.
    0.]
  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    0.]
  [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.
    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.
    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.
    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    0.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    1.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    1.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    1.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    1.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    1.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    1.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    1.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    1.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    1.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    1.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    1.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    1.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    1.]
  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.
    1.]]]


r is then a matrix of shape (10,1,30,1) and the attention map I'm plotting it thus:

attention_map = np.zeros((10, 30))
for t in range(10):
    for t_prime in range(30):
        attention_map[t][t_prime] = r[t][0,t_prime,0]


...but all the values are the same! I'm expecting some variation.

I've also tried adding K.learning_phase() to no avail. What am I doing wrong? 
","The problem was that I tried to plot the attention map of a model which was loaded from a saved model.

The output when the model was saved was:


  /home/opyate/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:2361:
  UserWarning: Layer lstm_1 was passed non-serializable keyword
  arguments: {'initial_state': [, ]}.
  They will not be included in the serialized model (and thus will be
  missing at deserialization time).   str(node.arguments) + '. They will
  not be included '
  /home/opyate/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:2361:
  UserWarning: Layer lstm_1 was passed non-serializable keyword
  arguments: {'initial_state': [, ]}. They will not be included in the
  serialized model (and thus will be missing at deserialization time).
  str(node.arguments) + '. They will not be included '
  /home/opyate/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:2361:
  UserWarning: Layer lstm_1 was passed non-serializable keyword
  arguments: {'initial_state': [,
  ]}.
  They will not be included in the serialized model (and thus will be
  missing at deserialization time).   str(node.arguments) + '. They will
  not be included '
  /home/opyate/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:2361:
  UserWarning: Layer lstm_1 was passed non-serializable keyword
  arguments: {'initial_state': [,
  ]}.
  They will not be included in the serialized model (and thus will be
  missing at deserialization time).   str(node.arguments) + '. They will
  not be included '
  /home/opyate/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:2361:
  UserWarning: Layer lstm_1 was passed non-serializable keyword
  arguments: {'initial_state': [,
  ]}.
  They will not be included in the serialized model (and thus will be
  missing at deserialization time).   str(node.arguments) + '. They will
  not be included '
  /home/opyate/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:2361:
  UserWarning: Layer lstm_1 was passed non-serializable keyword
  arguments: {'initial_state': [,
  ]}.
  They will not be included in the serialized model (and thus will be
  missing at deserialization time).   str(node.arguments) + '. They will
  not be included '
  /home/opyate/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:2361:
  UserWarning: Layer lstm_1 was passed non-serializable keyword
  arguments: {'initial_state': [,
  ]}.
  They will not be included in the serialized model (and thus will be
  missing at deserialization time).   str(node.arguments) + '. They will
  not be included '
  /home/opyate/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:2361:
  UserWarning: Layer lstm_1 was passed non-serializable keyword
  arguments: {'initial_state': [,
  ]}.
  They will not be included in the serialized model (and thus will be
  missing at deserialization time).   str(node.arguments) + '. They will
  not be included '


However, if I construct the model from code, and just load the saved weights, it works.

The assumption is that the UserWarnings when saving the model has something to do with my problem.
",<python><keras><numpy>
"1 1:31080.410200 2:2.871828 3:5.862267 4:7.100850 5:8.283706 6:-5.427875 7:-6.667087 8:-8.888233 9:28898.943400


Can someone please tell me how to load this data into a dataframe from .dat file. The data is given such that attribute number:value. I want only the values into  the dataframe
","Given just one line of the data, it's a little hard to go off of, but I'm assuming you're trying to get at the number after each colon, and the number before it refers to the column name?

If so, you can use read_csv with a little tweaking:

import pandas as pd
from pandas.compat import StringIO

temp='1 1:31080.410200 2:2.871828 3:5.862267 4:7.100850 5:8.283706 6:-5.427875 7:-6.667087 8:-8.888233 9:28898.943400'
#after testing replace StringIO(temp) to filename

df = pd.read_csv(StringIO(temp), 
                 sep=""\s+"", #separator whitespace
                 index_col=0,
                 header=None) 

for c in df.columns.values:
    df[c] = df[c].apply(lambda x: float(str(x).split(':')[1]))

df.head()


Which will output:

    1           2           3           4       5           6           7           8           9                               
1   31080.4102  2.871828    5.862267    7.10085 8.283706    -5.427875   -6.667087   -8.888233   28898.9434

",<python><pandas><dataframe><numpy>
"I am trying to fit my data into my model which takes numpy as input, so I feed the model with the dataframe values

stacked_averaged_models.fit(train.values, y_train1)


I am getting the following error

ValueError                                Traceback (most recent call last)
&lt;ipython-input-145-9ba69af8df05&gt; in &lt;module&gt;()
      1 X_traintrain = train.as_matrix().astype(np.float)
      2 from sklearn.metrics import r2_score
----&gt; 3 stacked_averaged_models.fit(train.values, y_train1)
      4 stacked_train_pred = stacked_averaged_models.predict(train.values)
      5 stacked_pred = np.expm1(stacked_averaged_models.predict(test.values))

&lt;ipython-input-140-dfca4af6e9d1&gt; in fit(self, X, y)
     18                 instance = clone(model)
     19                 self.base_models_[i].append(instance)
---&gt; 20                 instance.fit(X[train_index], y[train_index])
     21                 y_pred = instance.predict(X[holdout_index])
     22                 out_of_fold_predictions[holdout_index, i] = y_pred

~\Anaconda3\envs\deeplearning\lib\site-packages\sklearn\pipeline.py in fit(self, X, y, **fit_params)
    248         Xt, fit_params = self._fit(X, y, **fit_params)
    249         if self._final_estimator is not None:
--&gt; 250             self._final_estimator.fit(Xt, y, **fit_params)
    251         return self
    252 

~\Anaconda3\envs\deeplearning\lib\site-packages\sklearn\linear_model\coordinate_descent.py in fit(self, X, y, check_input)
    705                              order='F', dtype=[np.float64, np.float32],
    706                              copy=self.copy_X and self.fit_intercept,
--&gt; 707                              multi_output=True, y_numeric=True)
    708             y = check_array(y, order='F', copy=False, dtype=X.dtype.type,
    709                             ensure_2d=False)

~\Anaconda3\envs\deeplearning\lib\site-packages\sklearn\utils\validation.py in check_X_y(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)
    574     if multi_output:
    575         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,
--&gt; 576                         dtype=None)
    577     else:
    578         y = column_or_1d(y, warn=True)

~\Anaconda3\envs\deeplearning\lib\site-packages\sklearn\utils\validation.py in check_array(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)
    451                              % (array.ndim, estimator_name))
    452         if force_all_finite:
--&gt; 453             _assert_all_finite(array)
    454 
    455     shape_repr = _shape_repr(array.shape)

~\Anaconda3\envs\deeplearning\lib\site-packages\sklearn\utils\validation.py in _assert_all_finite(X)
     42             and not np.isfinite(X).all()):
     43         raise ValueError(""Input contains NaN, infinity""
---&gt; 44                          "" or a value too large for %r."" % X.dtype)
     45 
     46 

ValueError: Input contains NaN, infinity or a value too large for dtype('float64').


I did a check on NaN and infinity, it did pass the test

X_traintrain = train.as_matrix().astype(np.float)
print(np.any(np.isnan(X_traintrain)))
print(np.all(np.isfinite(X_traintrain)))


Output:

False
True


How else can I solve, or at least, debug this?

X1      X2       X3     X4       X5    X6   X7     X8   Y1      Y2
0.64    784.00  343.00  220.50  3.50    5   0.00    0   10.56   16.67
0.62    808.50  367.50  220.50  3.50    2   0.00    0   8.60    12.07
0.62    808.50  367.50  220.50  3.50    5   0.00    0   8.50    12.04
0.98    514.50  294.00  110.25  7.00    2   0.10    1   24.58   26.47


This is few rows of my dataset
","See if one of the answers in this discussion is helpful.
In my case, the error was caused by a df = df.reindex(index=my_index): The dataframe's index started at 1, but my_index contained a 0, so pandas silently inserted a row full of NaNs...
",<machine-learning><python><scikit-learn><pandas><numpy>
"I am trying to fit my data into my model which takes numpy as input, so I feed the model with the dataframe values

stacked_averaged_models.fit(train.values, y_train1)


I am getting the following error

ValueError                                Traceback (most recent call last)
&lt;ipython-input-145-9ba69af8df05&gt; in &lt;module&gt;()
      1 X_traintrain = train.as_matrix().astype(np.float)
      2 from sklearn.metrics import r2_score
----&gt; 3 stacked_averaged_models.fit(train.values, y_train1)
      4 stacked_train_pred = stacked_averaged_models.predict(train.values)
      5 stacked_pred = np.expm1(stacked_averaged_models.predict(test.values))

&lt;ipython-input-140-dfca4af6e9d1&gt; in fit(self, X, y)
     18                 instance = clone(model)
     19                 self.base_models_[i].append(instance)
---&gt; 20                 instance.fit(X[train_index], y[train_index])
     21                 y_pred = instance.predict(X[holdout_index])
     22                 out_of_fold_predictions[holdout_index, i] = y_pred

~\Anaconda3\envs\deeplearning\lib\site-packages\sklearn\pipeline.py in fit(self, X, y, **fit_params)
    248         Xt, fit_params = self._fit(X, y, **fit_params)
    249         if self._final_estimator is not None:
--&gt; 250             self._final_estimator.fit(Xt, y, **fit_params)
    251         return self
    252 

~\Anaconda3\envs\deeplearning\lib\site-packages\sklearn\linear_model\coordinate_descent.py in fit(self, X, y, check_input)
    705                              order='F', dtype=[np.float64, np.float32],
    706                              copy=self.copy_X and self.fit_intercept,
--&gt; 707                              multi_output=True, y_numeric=True)
    708             y = check_array(y, order='F', copy=False, dtype=X.dtype.type,
    709                             ensure_2d=False)

~\Anaconda3\envs\deeplearning\lib\site-packages\sklearn\utils\validation.py in check_X_y(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)
    574     if multi_output:
    575         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,
--&gt; 576                         dtype=None)
    577     else:
    578         y = column_or_1d(y, warn=True)

~\Anaconda3\envs\deeplearning\lib\site-packages\sklearn\utils\validation.py in check_array(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)
    451                              % (array.ndim, estimator_name))
    452         if force_all_finite:
--&gt; 453             _assert_all_finite(array)
    454 
    455     shape_repr = _shape_repr(array.shape)

~\Anaconda3\envs\deeplearning\lib\site-packages\sklearn\utils\validation.py in _assert_all_finite(X)
     42             and not np.isfinite(X).all()):
     43         raise ValueError(""Input contains NaN, infinity""
---&gt; 44                          "" or a value too large for %r."" % X.dtype)
     45 
     46 

ValueError: Input contains NaN, infinity or a value too large for dtype('float64').


I did a check on NaN and infinity, it did pass the test

X_traintrain = train.as_matrix().astype(np.float)
print(np.any(np.isnan(X_traintrain)))
print(np.all(np.isfinite(X_traintrain)))


Output:

False
True


How else can I solve, or at least, debug this?

X1      X2       X3     X4       X5    X6   X7     X8   Y1      Y2
0.64    784.00  343.00  220.50  3.50    5   0.00    0   10.56   16.67
0.62    808.50  367.50  220.50  3.50    2   0.00    0   8.60    12.07
0.62    808.50  367.50  220.50  3.50    5   0.00    0   8.50    12.04
0.98    514.50  294.00  110.25  7.00    2   0.10    1   24.58   26.47


This is few rows of my dataset
","I have tried so many suggested solutions but I found this one solve the problem. 

data =data[~data.isin([np.nan, np.inf, -np.inf]).any(1)]
",<machine-learning><python><scikit-learn><pandas><numpy>
"I have a list of files, an I use the KNN algorithm to classify these files.

dataset = pd.read_csv(file)
training_samples = get_sample_number(dataset)
X_train = dataset.iloc[:training_samples, 5:9]
y_train = dataset.iloc[:training_samples, 9]
X_test = dataset.iloc[training_samples:, 5:9]

# Feature Scaling
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.fit_transform(X_test)

# Fitting classifier to the training set
classifier = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)


Now I have my categories in my y_pred array. But I want to save the result in the file where I read the dataset. How can I link a prediction to the right row in the file (or dataset)?
","First and foremost: I would strongly advise against modifying your original data file. It introduces risk that your workflow will become unrepeatable.

To persist your results, I recommend doing something like this:

in_fname = '{}.csv'.format(filename)
out_fname = '{}_SCORED.csv'.format(filename)
dataset = pd.read_csv(in_fname)

... do stuff

dataset.loc[training_samples:, 'scores'] = y_pred
dataset.to_csv(out_fname, header=True, index=False)    


If you really want to overwrite your original data, just set in_fname=out_fname. But I'd advise against it.
",<pandas><numpy>
"I have a list of files, an I use the KNN algorithm to classify these files.

dataset = pd.read_csv(file)
training_samples = get_sample_number(dataset)
X_train = dataset.iloc[:training_samples, 5:9]
y_train = dataset.iloc[:training_samples, 9]
X_test = dataset.iloc[training_samples:, 5:9]

# Feature Scaling
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.fit_transform(X_test)

# Fitting classifier to the training set
classifier = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)


Now I have my categories in my y_pred array. But I want to save the result in the file where I read the dataset. How can I link a prediction to the right row in the file (or dataset)?
","First as ""timleathart"" mentioned, you need to fix your code by changing this line : 

X_test = sc.fit_transform(X_test)


to:

X_test = sc.transform(X_test)


For your question :


you have already the number of samples (training_samples) used for training. so all you need is to iterate over the y_pred and save the values in new column in the dataset starting from ""training_samples"" as row index. 

",<pandas><numpy>
"my array is looking like this


a=np.array([[ 25,  29,  19,  93],
       [ 27,  59,  23,  345],
       [ 24,  426,  15,  593],
       [ 24,  87,  50.2, 139],
       [ 13,  86,  12.4, 139],
       [ 13,  25,  85, 142],
       [ 62,  62,  68.2, 182],
       [ 27,  25,  20, 150],
       [ 25,  53,  71, 1850],
       [ 64,  67,  21.1, 1570],
       [ 64,  57,  73, 1502]])



i want to return the lowest value of column 2 based on the unique value of column 0. column 0 should contain unique values.
I tries the following code, but was not giving me the exact result.
Can some one help me to sort out this? thanks


sidx = np.lexsort(a[:,[2,0]].T)
dx = np.append(np.flatnonzero(a[2:,0] >a[:-2,0]), a.shape[0]-1) 
result = a[sidx[idx]]
print result


I want to get result like


[25...
 27
 24
 13
 62
 64...]



a=[[196512 28978 Decimal('12.7805170314276')]
 [196512 34591 Decimal('12.8994111000000')]
 [196512 13078 Decimal('12.9135746000000')]
 [196641 114569 Decimal('12.9267705000000')]
 [196641 118910 Decimal('12.8983353775637')]
 [196641 100688 Decimal('12.9505091000000')]]this is a big list
i used,
df = pd.DataFrame(a)
df.columns = ['a','b','c']
df.index = df.a.astype(str) 
dd=df.groupby('a').min()['c']

but i am getting,

195556    12.7805170314276
195937    12.7805170314276
196149    12.7805170314276
196152    12.7805170314276
196155    12.7805170314276
196262    12.7805170314276


","Here's an easy solution. The sort order changes, but that shouldn't be difficult to address if you really care:

import pandas as pd

df = pd.DataFrame(a)
df.columns = ['a','b','c','d']
df.index = df.a.astype(str) # to preserve correspondence
df.groupby('a').min()['b']

a
13.0    25.0
24.0    87.0
25.0    29.0
27.0    25.0
62.0    62.0
64.0    57.0
Name: b, dtype: float64


Edit: I think you meant to name your array y instead of a. This works for me:

 from decimal import Decimal

 y=np.array([[196512, 28978, Decimal('12.7805170314276')], 
    [196512, 34591, Decimal('12.8994111000000')] ,
    [196512, 13078, Decimal('12.9135746000000')] ,
    [196641, 114569, Decimal('12.9267705000000')] ,
    [196641, 118910, Decimal('12.8983353775637')] ,
    [196641, 100688, Decimal('12.9505091000000')]])


 df = pd.DataFrame(y) 
 df.columns = ['a','b','c'] 
 df.index = df.a.astype(str) 
 dd=df.groupby('a').min()['c'] 

In [210]: dd
Out[210]:
a
196512    12.7805170314276
196641    12.8983353775637
Name: c, dtype: object

",<python><numpy>
"

I'm getting the above error while I'm extracting the sensor data from serial port and giving it for prediction.Tried reshape and string functions but every trial throws back me same error.i'm using mlp neuralnetwork in my model.Thanks in advance
","Spent some time with error and found that serial data consists a newline character""\n"" in it and it caused error pitch data is a float and due added character roll value is a string.So, changed code as below a little and it worked 


",<classification><scikit-learn><numpy>
"My question is the same as here:
How to force weights to be non-negative in Linear regression
Except that I can only use Numpy (I cannot use Scipy or Scikit Learn).
Indeed, I am running my Python script on a server which doesn't include these modules.
Is there any solution ?
","The sklearn implementation of Lasso that can force non-negative weights (as in this answer) is based on the coordinate descent algorithm. You can reimplement it, using for example coordinate-wise Newton method. For simplicity, I did not inclide intercept into the model:

import numpy as np
# generate the data
np.random.seed(1)
n = 1000
X = np.random.normal(size=(n, 5))
y = np.dot(X, [0,0,1,2,3]) + np.random.normal(size=n, scale=3)
# initial solution (with some negative weights):
beta = np.dot(np.linalg.inv(np.dot(X.transpose(),X)), np.dot(X.transpose(), y))
print(beta)
# clip the solution from below with zero
prev_beta = beta.copy()
beta = np.maximum(beta, 1)
# improve the solution by restricted coordinate-wise Newton descent
hessian = np.dot(X.transpose(), X)
while not (prev_beta == beta).all():
    prev_beta = beta.copy()
    for i in range(len(beta)):
        grad = np.dot(np.dot(X,beta)-y, X)
        beta[i] = np.maximum(0, beta[i] - grad[i] / hessian[i,i])
print(beta)


This code will output initial and final beta's:

[-0.01404546 -0.02633036  1.06028543  1.99696564  2.93511618]
[ 0.          0.          1.05919989  1.99673774  2.93442334]


You can see that OLS beta differ from your optimal beta not only in the first two coefficients (that have been negative), but the rest of coefficients were also adjusted.
",<python><regression><numpy>
"I want to train a OneClassSVM() using sklearn, and I have a set of around 800 images in my training set.

I am using opencv to read the images and resize them to constant dimensions (960x540) and then adding them to a numpy-array. The images are RGB and thus have 3-dimensions. For that, I am reshaping the numpy array after reading all the images:

#Assume X is my numpy array which contains all the images before reshaping
#Now I reshape X
n_samples = len(X)
X = X.reshape(n_samples, 950*540*3)


As you can see, the number of features is huge (1,539,000 to be exact).

Now I try to train my model:

model = OneClassSVM(kernel='rbf', gamma=0.001)
model.fit(X)


After running my code, it crashed due to MemoryError. If I'm not mistaken this is obvious due the large number of features? So, is there a better way to pre-process the images before fitting them, or to decrease the number of features? 
","You should try converting them to Principle Components using PCA. Please refer this Analytics Vidya PCA, this should give give you a good understanding.

PCA converts n large vector into p Principle components. 


  First principal component is a linear combination of original predictor variables which captures the maximum variance in the data set


Second PC is also a linear combination of original predictor variables which captures remaining variance. All the succeeding ones follow the same concept.

This way you can select top Principle components which explains good enough cumulative variance in your data
",<machine-learning><python><scikit-learn><preprocessing><numpy>
"I want to train a OneClassSVM() using sklearn, and I have a set of around 800 images in my training set.

I am using opencv to read the images and resize them to constant dimensions (960x540) and then adding them to a numpy-array. The images are RGB and thus have 3-dimensions. For that, I am reshaping the numpy array after reading all the images:

#Assume X is my numpy array which contains all the images before reshaping
#Now I reshape X
n_samples = len(X)
X = X.reshape(n_samples, 950*540*3)


As you can see, the number of features is huge (1,539,000 to be exact).

Now I try to train my model:

model = OneClassSVM(kernel='rbf', gamma=0.001)
model.fit(X)


After running my code, it crashed due to MemoryError. If I'm not mistaken this is obvious due the large number of features? So, is there a better way to pre-process the images before fitting them, or to decrease the number of features? 
","One approach is to use an artificial neural network to extract features representing the images. This can be done either by using a pre-configured network with pre-trained weights and extracting the output of one of the hidden layers, or by constructing and training your own network for this purpose. 

To use a pre-configured, pre-trained module can be accomplished easily with Keras and TensorFlow, where you can import InceptionV3 or MobileNet with weights pre-trained on ImageNet, which would net you 2048 or 1024 features per image, respectively. 

An article discussing such an approach can be found here. This could hopefully give you better performance than using something like PCA to conduct dimensionality reduction.
",<machine-learning><python><scikit-learn><preprocessing><numpy>
